# Probability distributions {#distributions}

## Introduction

In Bayesian statistics, probability distributions are used for two fundamentally different purposes. First, they are used to describe distributions of data. These distributions are also called data distributions. Second, probability distributions are used to express information or knowledge about parameters. Such distributions are called prior or posterior distributions. The data distributions are part of descriptive statistics, whereas prior and posterior distributions are part of inferential statistics. The usage of probability distributions for describing data does not differ between frequentist and Bayesian statistics. Classically, the data distribution is known as "model assumption". Specifically to Bayesian statistics is the formal expression of statistical uncertainty (or "information" or "knowledge") by prior and posterior distributions. We here introduce some of the most often used probability distributions and present how they are used in statistics. 

Probability distributions are grouped into discrete and continuous distributions. Discrete distributions define for any discrete value the probability that exactly this value occurs. They are usually used as data distributions for discrete data such as counts. The function that describes a discrete distribution is called a probability function (their values are probabilities, i.e. a number between 0 and 1). Continuous distributions describe how continuous values are distributed. They are used as data distributions for continuous measurements such as body size and also as prior or posterior distributions for parameters such as the mean body size. Most parameters are measured on a continuous scale. The function that describes continuous distributions is called density function. Its values are non-negative and the area under the density function equals one. The area under a density function corresponds to probabilities. For example, the area under the density function above the value 2 corresponds to the proportion of data with values above 2 if the density function describes data, or it corresponds to the probability that the parameter takes on a value bigger than 2 if the density function is a posterior distribution.  


## Discrete distributions

### Bernoulli distribution

Bernoulli distributed data take on the exact values 0 or 1. The value 1 occurs with probability $p$.    

$ x \sim Bernoulli(p)$  

The probability function is $p(x) = p^x(1-p)^{1-x}$. 
The expected value is $E(x) = p$ and the variance is $Var(x) = p(1-p)$.

The flipping experiment of a fair coin produces Bernoulli distributed data with $p=0.5$ if head is taken as one and tail is taken as zero. The Bernoulli distribution is usually used as a data model for binary data such as whether a nest box is used or not, whether a seed germinated or not, whether a species occurs or not in a plot etc. 

### Binomial distribution

The binomial distribution describes the number of ones among a predefined number of Bernoulli trials. For example, the number of heads among 20 coin flips, the number of used nest boxes among the 50 nest boxes of the study area, or the number of seed that germinated among the 10 seeds in the pot. Binomially distributed data are counts with an upper limit ($n$).

$ x \sim binomial(p,n)$ 

The probability function is $p(x) = {n\choose x} p^x(1-p)^{(n-x)}$.
The expected value is $E(x) = np$ and the variance is $Var(x) = np(1-p)$.


### Poisson distribution

The Poisson distribution describes the distribution of counts without upper boundary, i.e., when we know how many times something happened but we do not know how many times it did not happen. A typical Poisson distributed variable is the number of raindrops in equally-sized grid cells on the floor, if we can assume that every rain drop falls down completely independent of the other raindrops and at a completely random point. 

$ p(x) \sim Poisson(\lambda)$  

The probability function is $p(x) = \frac{1}{x!}\lambda^xexp(-\lambda)$.
The expected values is $E(x) = \lambda$ and the variance is $Var(x) = \lambda$.

An important property of the Poisson distribution is that it has only one parameter $\lambda$. As a consequence, it does not allow for any combination of means and variances. In fact, they are assumed to be the same. In the real world, most count data do not behave like rain drops, that means variances of count data are in most real world examples not equal to the mean as assumed by the Poisson distribution. Therefore, when using the Poisson distribution as a data model, it is important to check for overdispersion.

Further, note that not all variables measured as an integer number are count data! For example, the number of days an animal spends in a specific area before moving away looks like a count. However, it is a continuous measurement. The duration an animal spends in a specific areas could also be measured in hours or minutes. The Poisson model assumes that the counts are all events that happened. However, an emigration of an animal is just one event, independent of how long it stayed.  


### Negative-binomial distribution
UNDER CONSTRUCTION

## Continuous distributions

### Normal distribution


normal distribution = Gaussian distribution  
 
 $p(\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(\theta -\mu)^2) = Normal(\mu, \sigma)$  
   
     
       
$E(\theta) = \mu$, $var(\theta) = \sigma^2$, $mode(\theta) = \mu$

The variance parameter can be specified to be a variance, a standard deviation or a precision. Depending on the software used (or author of the paper) different habits exist. 
  



### Gamma distribution
UNDER CONSTRUCTION

### Cauchy distribution {#cauchydistri}
We sometimes use the Cauchy distiribution to specify the prior distirbution of the standart deviation and similar parameters in a model. An example for this is the regression model that we used to introduce Stan (Chapter \@ref(firststanmod)).


### t-distribution
marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution  

  
$p(\theta|v,\mu,\sigma) = \frac{\Gamma((v+1)/2)}{\Gamma(v/2)\sqrt{v\pi}\sigma}(1+\frac{1}{v}(\frac{\theta-\mu}{\sigma})^2)^{-(v+1)/2}$  


$v$ degrees of freedom  
$\mu$ location  
$\sigma$ scale


### F-distribution

The F-distribution is not important in Bayesian statistics. 
Ratios of sample variances drawn from populations with equal variances follow an F-distribution. The density function of the F-distribution is even more complicated than the one of the t-distribution! We do not copy it here. Further, we have not yet met any Bayesian example where the F-distribution is used (that does not mean that there is no). It is used in frequentist analyses in order to compare variances, e.g. within ANOVAs. If two variances only differ because of natural variance in the data (nullhypothesis) then $\frac{Var(X_1)}{Var(X_2)}\sim F_{df_1,df_2}$.

```{r, echo=FALSE, fig.cap="Different density functions of the F statistics"}
par(mar=c(4.5,4.5,1,1))
x <- seq(0, 5, length=100)
y <- df(x, 5,5)
plot(x, y, type="l", lwd=2, las=1, xlab="F-value", ylab="Density", main="", ylim=c(0,1))
text(3, 0.3, "df1=5, df2=5", adj=c(0,0))
text(3, 0.2, "df1=2, df2=10", adj=c(0,0), col="blue")
text(3, 0.1, "df1=20, df2=20", adj=c(0,0), col="orange")

y <- df(x, 2,10)
lines(x,y, lwd=2, col="blue")
y <- df(x, 20, 20)
lines(x,y, lwd=2, col="orange")

```


