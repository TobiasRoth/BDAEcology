
# Generalized linear models {#glm}
<!-- Fraenzis draft 29.9.2024-->

## Introduction

Up to now, we have dealt with models that assume normally distributed residuals. Sometimes the nature of the outcome
variable makes it impossible to fulfill this assumption as might occur with binary variables (e.g., alive/dead, a specific behavior occurred/did not occur), proportions (which are confined to be between 0 and 1), or counts that cannot have negative values. For such cases, models for distributions other than the normal distribution are needed; such models are called generalized linear models (GLM). They consist of three elements:  

1. the linear predictor $\bf X \boldsymbol \beta$   
2. the link function $g()$  
3. the data distribution  

The linear predictor is exactly the same as in normal linear models. It is a linear function that defines the relationship between the dependent and the explanatory variables. The link function transforms the expected values of the outcome variable into the range of the linear predictor, which ranges from $-\infty$ to $+\infty$. Or, perhaps more intuitively, the inverse link function transforms the values of the linear predictor into the range of the outcome variable. Table \@ref(tab:linkfunD) gives a list of possible link functions that work with different data distributions. 
Then, a specific data distribution, for example, binomial or Poisson, is used to describe how the observations scatter around the expected values. A general model formula for a generalized linear model is:
$$\bf y \sim ExpDist(\bf\hat y, \boldsymbol\theta)$$
$$g(\bf\hat y) = \bf X\boldsymbol \beta $$
where *ExpDist* is a distribution of the exponential family and $g()$ is the link function. The vector $\bf y$ contains the observed values of the outcome variable, $\bf \beta$ contains the model parameters in the linear predictor (also called the model coefficients), and $\bf X$ is the model matrix containing the values of the predictor variables. $\boldsymbol \theta$ is an optional vector of additional parameters needed to define the data distribution (e.g., the number of trials in the binomial distribution or the variance in the normal distribution).
The normal linear model is a specific case of a generalized linear model, namely when *ExpDist* equals the normal distribution and $g()$ is the identity function ($g(x) = x$). Statistical distributions of the exponential family are normal, Bernoulli, binomial, Poisson, inverse-normal, gamma, negative binomial, among others. The normal, Bernoulli, binomial, Poisson or negative binomial distributions are by far the most often used distributions. Most, but not all, data we gather in the life sciences can be analyzed assuming one of these few distributions.


```{r linkfunD, echo=FALSE}
tab <- data.frame(link=c("logit", "probit", "cloglog", "identity", "inverse", "log", "1/mu^2", "sqrt", "cauchit", "exponent (mu^a)"),
                  Gaussian=c(NA, NA, NA, "D", NA, NA, NA, NA, NA, "x"),
                  Binomial=c("D", "x", "x", NA, NA, NA, NA, NA, "x", NA),
                  Gamma=c(NA, NA, NA, "x", "D", "x", NA, NA, NA, NA),
                  Inv_Gauss=c(NA, NA, NA, NA, NA, NA, "D", NA, NA, NA),
                  Poisson=c(NA, NA, NA, "x", NA, "D", NA, NA, "x", NA),
                  Negative_binomial=c(NA, NA, NA, "x", NA, "D", NA, "x",NA, NA))
kable(tab, caption="Frequently used distributions for the glm function with their default (D) link functions and other link functions that are possible.")

```

Paul Buerkner has implemented many different distributions and link function in the package `brms`, [see here ](https://paulbuerkner.com/brms/reference/brmsfamily.html).



## Bernoulli model

### Background

If the outcome variable can only take one of two values (e.g., a species is present or absent, or the individual survived or died; coded as 1 or 0) we use a Bernoulli model, also called logistic regression. The [Bernoulli distribution](#bernoulli-dist) only allows for the values zero and ones and it has only one parameter $p$, which defines the probability that the value is 1. 

When fitting a Bernoulli model to data, we have to estimate $p$. Often we are interested in correlations between $p$ and one or several explanatory variables. Therefore, we model $p$ as linearly dependent on the explanatory variables. Because the values of $p$ are squeezed between 0 and 1 (because it is a probability), $p$ is transformed by the link-function before the linear relationship is modeled.
$$g(p_i) = \bf X\boldsymbol \beta $$
Functions that can transform a probability into the scale of the linear
predictor ($-\infty$ to $+\infty$) are, for example, logit, probit, cloglog, or cauchit. These link functions differ slightly in the way they link the outcome variable to the explanatory variables (Figure \@ref(fig:illlink)). The logit link function is the most often
used link function in binomial models. However, sometimes another link
function might fit the data better. [Kevin S. Van Horn gives useful tipps when to use which link function](https://bayesium.com/wp-content/uploads/2015/08/logit-probit-cloglog.pdf).


```{r illlink, echo=FALSE, fig.cap="Left panel: Shape of different link functions commonly used for modelling probabilities. Right panel: The relationship between the predictor x (x-axis) and p on the scale of the link function (y-axis) is assumed to be linear."}
# Illustration of different link functions
# (Figure 8.3 in old book)
x <- seq(-10, 10, by=0.01)
y <- 0.5*x

par(mfrow=c(1,2),mar=c(4,5,0.5,1))

invlogit.y <- plogis(y) # logit
plot(x,invlogit.y, type="l", lwd=3, ylim=c(0, 1), las=1, col="blue", ylab=expression(paste("p = inverse link of X", beta)),
    cex.lab=1.2, lty=1)

invprobit.y <- pnorm(y) # probit
lines(x,invprobit.y, lwd=3, col="orange", lty=2)

#cloglog
invcloglog.y <- 1-exp(-exp(y))
lines(x, invcloglog.y, lwd=3, col="black", lty=3)

#cauchit
invcauchit.y <- pcauchy(y)
lines(x, invcauchit.y, lwd=3, col="brown", lty=4)
legend(-10, 1, lwd=2, col=c("blue", "orange", "black", "brown"),
  legend=c("logit", "probit", "cloglog", "cauchit"), lty=1:4, cex=0.8, bty="n")

plot(x,y, type="l", lwd=3, ylim=c(-5, 5), las=1, ylab=expression(paste("X", beta, " = link of p")), cex.lab=1.2)

```

### Fitting a Bernoulli model in R

Functions to fit a Bernoulli model are `glm`, `stan_glm`, `brm`, and there are many more that we do not know so well as the three we focus on in this book. 

We start by using the function `glm`. It uses the “iteratively reweighted least-squares method” which is an adaptation of the least-square (LS) method for fitting generalized linear models. The argument `family`allows to choose a data distribution. For fitting a Bernoulli model, we need to specify `binomial`. That is because the Bernoulli distribution is equal to the binomial distribution with only one trial (size parameter = 1). Note, if we forget the `family` argument, we fit a normal linear model, and there is no warning by R!

With the specification of the distribution, we also choose the link-function. The default link function for the binomial or Bernoulli model is the logit-function. To change the link-function, use e.g. `family=binomial(link=cloglog)`.

As an example, we use presence-absence data of little owls *Athene noctua* in nest boxes during the breeding season. The original data are published in @Gottschalk.2011; here we use only parts of these data. The variable `PA` contains the presence of a little owl: 1 indicates a nestbox used by little owls, whereas 0 stands for an empty nestbox. The variable `elevation` has the elevation in meters above sea level. We are interested in how the presence of the little owl is associated with elevation within the study area, that is, how the probability of presence changes with elevation. Our primary interest, therefore, is the slope $\beta_1$ of the regression line.

$$ y_i \sim Bernoulli(p_i) $$
$$ logit(p_i) = \beta_0 + \beta_1 elevation$$
where $logit(p_i) = log(p_i/(1-p_i))$.




```{r}
data(anoctua) # Athene noctua data in the blmeco package
mod <- glm(PA~elevation, data=anoctua, family=binomial)
mod
```


### Assessing model assumptions in a Bernoulli model

The residual plots normally look quite awful because the residual distribution very often has two peaks, a negative and a positive one resulting from the binary nature of the outcome variable. However, it is still good to have a look at these plots using `plot(mod)`. At least the average should roughly be around zero and not show a trend. 
An often more informative plot to judge model fit for a binary logistic regression is to compare the fitted values with the data. To better see the observations, we slightly jitter them in the vertical direction.
If the model would fit the data well, the data would be, on average, equal to the fitted values. Thus, we add the $y = x$-line to the plot using the `abline` function with intercept 0 and slope 1.
Of course, binary data cannot lie on this line because they can only take on the two discrete values 0 or 1. However, the mean of the 0 and 1 values should lie on the line if the model fits well. Therefore, we calculate the mean for suitably selected classes of fitted values. In our example, we choose a class width of 0.1. Then, we calculate means per class and add these to the plot, together with a classical standard error that tells us how reliable the means are. This can be an indication whether our arbitrarily chosen class width is reasonable.

```{r lrgof, fig.cap="Goodness of fit plot for the Bernoulli model fitted to little owl presence-absence data. Open circles = observed presence (1) or absence (0) jittered in the vertical direction; orange dots = mean (and 95% compatibility intervals given as vertical bards) of the observations within classes of width 0.1 along the x-axis. The dotted line indicates perfect coincidence between observation and fitted values. Orange larger points are from the model assuming a linear effect of elevation, wheras the smaller light blue points are from a model assuming a non-linear effect."}

plot(fitted(mod), jitter(anoctua$PA, amount=0.05), 
     xlab="Fitted values", ylab="Probability of presence", 
     las=1, cex.lab=1.2, cex=0.8)
abline(0,1, lty=3)

t.breaks <- cut(fitted(mod), seq(0,1, by=0.1))
means <- tapply(anoctua$PA, t.breaks, mean)
semean <- function(x) sd(x)/sqrt(length(x))
means.se <- tapply(anoctua$PA, t.breaks, semean)
points(seq(0.05, 0.95, by=0.1), means, pch=16, col="orange")
segments(seq(0.05, 0.95, by=0.1), means-2*means.se,
seq(0.05, 0.95,by=0.1), means+2*means.se,lwd=2, col="orange")

mod <- glm(PA ~ elevation + I(elevation^2) + I(elevation^3) +
             I(elevation^4), data=anoctua, family=binomial)
t.breaks <- cut(fitted(mod), seq(0,1, by=0.1))
means <- tapply(anoctua$PA, t.breaks, mean)
semean <- function(x) sd(x)/sqrt(length(x))
means.se <- tapply(anoctua$PA, t.breaks, semean)
points(seq(0.05, 0.95, by=0.1)+0.01, means, pch=16, col="lightblue", cex=0.7)
segments(seq(0.05, 0.95, by=0.1)+0.01, means-2*means.se,
seq(0.05, 0.95,by=0.1)+0.01, means+2*means.se,lwd=2, col="lightblue")


```

The means of the observed data (orange dots) do not fit well to the data (Figure \@ref(fig:lrgof)). For low presence probabilities, the model overestimates presence probabilities whereas, for medium presence probabilities, the model underestimates presence probability. This indicates that the relationship between little owl presence and elevation may not be linear. After including polynomials up to the fourth degree, we obtained a reasonable fit (light blue dots in Figure \@ref(fig:lrgof)). 

Further aspects of model fit that may be checked in Bernoulli models:  

Are all observations independent?  
May spatial or temporal correlation be an issue?  
Are all parameters well informed by the data? Some parameters may not be identifiable due to complete separation, i.e. when there is no overlap between the 0 and 1's regarding one of the predictor variables. In such cases `glm` may fail to fit the model. However, Bayesian methods (`stan_glm` or `brm`) do not fail but the result may be highly influenced by the prior distributions. A prior sensitivity analysis is recommended.

Note, we do not have to worry about overdispersion when the outcome variable is binary, even though the variance of the Bernoulli distribution is defined by p and no separate variance parameter exists. However, because the data can only take the values 0 and 1, there is no possibility that the data can show a higher variance than the one assumed by the Bernoulli distribution. 

### Visualising the results

When we are ready to report and visualise the results (i.e. after assessing the model fit, when we think the model reasonably well describes the data generating process). We can simulate the posterior distribution of $\beta_1$ and obtain the 95% compatibility interval.

```{r}
library(arm)
nsim <- 5000
bsim <- sim(mod, n.sim=nsim) # sim from package arm
apply(bsim@coef, 2, quantile, prob=c(0.5, 0.025, 0.975))
```

To interpret this polynomial function, an effect plot is helpful. To that end, and as we have done before, we calculate fitted values over the range of the covariate, together with compatibility intervals.

```{r}
newdat <- data.frame(elevation = seq(80,600,by=1))
Xmat <- model.matrix(~elevation+I(elevation^2)+I(elevation^3)+
                       I(elevation^4), data=newdat) # the model matrix
fitmat <- matrix(nrow=nrow(newdat), ncol=nsim)
for(i in 1:nsim) fitmat[,i] <- plogis(Xmat %*% bsim@coef[i,])
newdat$lwr <- apply(fitmat,1,quantile,probs=0.025)
newdat$fit <- plogis(Xmat %*% coef(mod))
newdat$upr <- apply(fitmat,1,quantile,probs=0.975)

```

We now can plot the data together with the estimate and its compatibility interval. We, again, use the function `jitter` to slightly scatter the points along the y-axis to make overlaying points visible.

```{r, fig.cap="Little owl presence data versus elevation with regression line and 95% compatibility interval (dotted lines). Open circles = observed presence (1) or abesnce (0) jittered in the vertical direction."}
plot(anoctua$elevation, jitter(anoctua$PA, amount=0.05), 
     las=1, cex.lab=1.4, cex.axis=1.2, xlab="Elevation",
     ylab="Probability of presence")
lines(newdat$elevation, newdat$fit, lwd=2)
lines(newdat$elevation, newdat$lwr, lty=3)
lines(newdat$elevation, newdat$upr, lty=3)
```


### Some remarks 

Binary data do not contain a lot of information. Therefore, large sample sizes are needed to obtain robust results.


Often presence/absence data are obtained by visiting plots several times during a distinct period, for example, a breeding period, and then it is reported whether a species has been seen or not. If it has been seen and if there is no misidentification in the data, it is present, however, if it has not been seen we
are usually not sure whether we have not detected it or whether it is absent. In the case of repeated visits to the same plot, it is possible to estimate the detection probability using occupancy models @MacKenzie.2002 or point count models @Royle.2004b. 
Finally, logistic regression can be used in the sense of a discriminant function analysis that aims to find predictors that discriminate members of two groups @Anderson.1974. However, if one wants to use the fitted value from such an analysis to assign group membership of a new subject, one has to take the prevalence of the two groups in the data into account. 


## Binomial model

### Background
The binomial model is usesd when the response variable is a count with an upper limit, e.g., the number of seeds that germinated among a total number of seeds in a pot, or the number of chicks hatching from the total number of eggs. Thus, we can use the binomial model always when the response is the sum of a predefined number of Bernoulli trials. Whether a seed germinates or not is a Bernoulli trial. If we have more than one seed, the number of germinated seeds follow a [binomial distribution](#binomial-dist).


As an example, we use data from a study on the effects of anthropogenic fire regimes traditionally applied to savanna habitat in Gabon, Central Africa [@Walters.2012]. Young trees survive fires better or worse depending, among other factors, on the fuel load, which, in turn, depends heavily on the time since the last fire happened. Thus, plots were burned after different lengths of time since the previous fire (4, 9, or 12 months ago). Trees that resprouted after the previous (first) fire were counted before and after the experimental (second) fire to estimate their survival of the experimental fire depending on the time since the previous fire.  
The outcome variable is the number of surviving trees among the total number of trees per plot $y_i$. The explanatory variable is the time since the previous fire, a factor with three levels: “4m”, “9m”, and “12m”. Assuming that the data follow a binomial distribution, the following model can be fitted to the data:

$$ y_i \sim binomial(p_i, n_i) $$

$$ logit(p_i) = \beta_0 + \beta_1 I(treatment_i=9m) + \beta_2 I(treatment_i=12m)$$
where $p_i$ being the survival probability and $n_i$ the total number of tree sprouts on plot $i$. Note that $n_i$ should not be confused with the sample size of the data set, i.e. the number of rows in the data table. 


### Fitting a binomial model in R

We normally use `glm`, `stan_glm` or `brm` for fitting a binomial model depending on the complexity of the predictors and correlation structure.

We here, again, start with using the `glm` function. 

A peculiarity with binomial models is that the outcome is not just one number, it is the number of trees still live $y_i$ out of $n_i$ trees that were alive before the experimental fire. Therefore, the outcome variable has to be given as a matrix with two columns.
The first column contains the number of successes (number of survivors $y_i$) and the second column contains the number of failures (number of trees killed by the fire, $n_i - y_i$). We build this matrix using `cbind` (“column bind”).

```{r}
data(resprouts) # example data from package blmeco
resprouts$succ <- resprouts$post
resprouts$fail <- resprouts$pre - resprouts$post

mod <- glm(cbind(succ, fail) ~ treatment, data=resprouts,
family=binomial)
mod
```

Experienced readers will be alarmed because the residual deviance is much larger than the residual degrees of freedom, which indicates overdispersion. We will soon discuss overdispersion, but, for now, we continue with the analysis for the sake of illustration.

The estimated model parameters are $\hat{b_0} =$ `r round(coef(mod)[1], 2)`,  $\hat{b_1} =$ `r round(coef(mod)[2], 2)`, and  $\hat{b_2} =$ `r round(coef(mod)[3], 2)`. These estimates tell us that tree survival was higher for the 9-month fire lag treatment compared to the 4-month treatment (which is the reference level), but lowest in the 12-month treatment. To obtain the mean survival probabilities per treatment, some math is needed because we have to back-transform the linear predictor to the scale of the outcome variable. The mean survival probability for the 4-month treatment is $logit^{-1}($`r round(coef(mod)[1], 2)`$) = \frac{e^{-1.24}}{1+e^{-1.24}}=$`r round(plogis(coef(mod)[1]), 2)`, for the 9-month treatment it is $logit^{-1}($`r round(coef(mod)[1], 2)`$ +$ `r round(coef(mod)[2], 2)`$) =$ `r round(plogis(coef(mod)[1]+coef(mod)[2]), 2)`, and for the 12-month treatment it is $logit^{-1}($`r round(coef(mod)[1], 2)`$ +$ `r round(coef(mod)[3], 2)`$) =$ `r round(plogis(coef(mod)[1]+coef(mod)[3]), 2)`. The function `plogis` gives the inverse of the logit function and can be used to estimate the survival probabilities, for example:


```{r}
plogis(coef(mod)[1]+ coef(mod)[2]) # for the 9month treatment
```


The direct interpretation of the model coefficients $\beta_1$ and $\beta_2$ is that they are the log of the ratio of the odds of two treatment levels (i.e., the log odds ratio). The odds for treatment “4 months” are 0.22/(1-0.22)=0.29 (calculated using non rounded values), which is the estimated ratio of survived to killed trees in this treatment. For treatment “9 months,” the odds are 0.48/(1-0.48) = 0.92, and the log odds ratio is log(0.92/0.29) = 1.16 = $beta_1$.

The model output includes the null deviance and the residual deviance. Deviance is a measure of the difference between the data and a model. It corresponds to the sum of squares in the normal linear model. The smaller the residual deviance the better the model fits to the data. Adding a predictor reduces the deviance, even if the predictor does not have any relation to the outcome variable. The Akaike information criterion (AIC) value in the model output (last line) is a deviance measure that is penalized for the number of model parameters. It can be used for [model comparison](#model_comparison).  
The residual deviance is defined as minus two times the difference of the log-likelihoods of the saturated model and our model. The saturated model is a model that uses the observed proportion of successes as the success probability for each observation $y_i \sim binomial(y_i/n_i, n_i)$. The saturated model has the highest possible likelihood (given the data set and the binomial model). This highest possible likelihood is compared to the likelihood of the model at hand,
$y_i \sim binomial(p_i, n_i)$ with $p_i$ dependent on some predictor variables. The null deviance is minus two times the difference of the log-likelihoods of the saturated model, and a model that contains only one overall mean success probability, the null model $y_i \sim binomial(p, n_i)$. The null deviance corresponds to the total sum of squares, that is, it is a measure of the total variance in the data.

### Assessing assumptions of a binomial model


Note that the variance is defined by n and p, that is, there is no
separate variance parameter. This is something we have to be careful about
when fitting a binomial model to data. Often, real data show a highervariance than assumed by the binomial distribution. This is called overdispersion
(see Section 8.2.3).

### Visualising results




## Poisson model
