# Probability distributions {#distributions}

## Introduction

In Bayesian statistics, probability distributions are used for two fundamentally different purposes. First, they are used to describe distributions of data. These distributions are also called data distributions. Second, probability distributions are used to express information or knowledge about parameters. Such distributions are called prior or posterior distributions. The data distributions are part of descriptive statistics, whereas prior and posterior distributions are part of inferential statistics. The usage of probability distributions for describing data does not differ between frequentist and Bayesian statistics. Classically, the data distribution is known as "model assumption". Specifically to Bayesian statistics is the formal expression of statistical uncertainty (or "information" or "knowledge") by prior and posterior distributions. We here introduce some of the most often used probability distributions and present how they are used in statistics. 

Probability distributions are grouped into discrete and continuous distributions. Discrete distributions define for any discrete value the probability that exactly this value occurs. They are usually used as data distributions for discrete data such as counts. The function that describes a discrete distribution is called a probability function (their values are probabilities, i.e. a number between 0 and 1). Continuous distributions describe how continuous values are distributed. They are used as data distributions for continuous measurements such as body size and also as prior or posterior distributions for parameters such as the mean body size. Most parameters are measured on a continuous scale. The function that describes continuous distributions is called density function. Its values are non-negative and the area under the density function equals one. The area under a density function corresponds to probabilities. For example, the area under the density function above the value 2 corresponds to the proportion of data with values above 2 if the density function describes data, or it corresponds to the probability that the parameter takes on a value bigger than 2 if the density function is a posterior distribution.  


## Discrete distributions

### Bernoulli distribution



### Poisson distribution
xxx

## Continuous distributions

### Normal distribution


normal distribution = Gaussian distribution  
 
 $p(\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(\theta -\mu)^2) = Normal(\mu, \sigma)$  
   
     
       
$E(\theta) = \mu$, $var(\theta) = \sigma^2$, $mode(\theta) = \mu$

The variance parameter can be specified to be a variance, a standard deviation or a precision. Depending on the software used (or author of the paper) different habits exist. 
  



### Gamma distribution
xxx

### Cauchy distribution {#cauchydistri}
We sometimes use the Cauchy distiribution to specify the prior distirbution of the standart deviation and similar parameters in a model. An example for this is the regression model that we used to introduce Stan (Chapter \@ref(firststanmod)).


### t-distribution
marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution  

  
$p(\theta|v,\mu,\sigma) = \frac{\Gamma((v+1)/2)}{\Gamma(v/2)\sqrt{v\pi}\sigma}(1+\frac{1}{v}(\frac{\theta-\mu}{\sigma})^2)^{-(v+1)/2}$  


$v$ degrees of freedom  
$\mu$ location  
$\sigma$ scale


### F-distribution

The F-distribution is not important in Bayesian statistics. 
Ratios of sample variances drawn from populations with equal variances follow an F-distribution. The density function of the F-distribution is even more complicated than the one of the t-distribution! We do not copy it here. Further, we have not yet met any Bayesian example where the F-distribution is used (that does not mean that there is no). It is used in frequentist analyses in order to compare variances, e.g. within ANOVAs. If two variances only differ because of natural variance in the data (nullhypothesis) then $\frac{Var(X_1)}{Var(X_2)}\sim F_{df_1,df_2}$.

```{r, echo=FALSE, fig.cap="Different density functions of the F statistics"}
par(mar=c(4.5,4.5,1,1))
x <- seq(0, 5, length=100)
y <- df(x, 5,5)
plot(x, y, type="l", lwd=2, las=1, xlab="F-value", ylab="Density", main="", ylim=c(0,1))
text(3, 0.3, "df1=5, df2=5", adj=c(0,0))
text(3, 0.2, "df1=2, df2=10", adj=c(0,0), col="blue")
text(3, 0.1, "df1=20, df2=20", adj=c(0,0), col="orange")

y <- df(x, 2,10)
lines(x,y, lwd=2, col="blue")
y <- df(x, 20, 20)
lines(x,y, lwd=2, col="orange")

```


