[["index.html", "Bayesian Data Analysis in Ecology with R and Stan Preface Why this book? About this book How to contribute? Acknowledgments", " Bayesian Data Analysis in Ecology with R and Stan Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Pius Korner-Nievergelt 2023-03-14 Preface Why this book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (Korner-Nievergelt et al. 2015). You can order it here. People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. About this book We do not copy text from the book into the e-book. Therefore, we refer to the book (Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. While we show the R-code behind most of the analyses, we sometimes choose not to show all the code in the html version of the book. This is particularly the case for some of the illustrations. An intrested reader can always consult the public GitHub repository with the rmarkdown-files that were used to generate the book. How to contribute? It is open so that everybody with a GitHub account can make comments and suggestions for improvement. Readers can contribute in two ways. One way is to add an issue. The second way is to contribute content directly through the edit button at the top of the page (i.e. a symbol showing a pencil in a square). That button is linked to the rmarkdown source file of each page. You can correct typos or add new text and then submit a GitHub pull request. We try to respond to you as quickly as possible. We are looking forward to your contribution! Acknowledgments We thank Yihui Xie for providing bookdown which makes it much fun to write open books such as ours. We thank many anonymous students and collaborators who searched information on new software, reported updates and gave feedback on earlier versions of the book. Specifically, we thank Carole Niffenegger for looking up the difference between the bulk and tail ESS in the brm output, Martin Küblbeck for using the conditional logistic regression in rstanarm,  "],["PART-I.html", "1 Introduction to PART I 1.1 Further reading", " 1 Introduction to PART I During our courses we are sometimes asked to give an introduction to some R-related stuff covering data analysis, presentation of results or rather specialist topics in ecology. In this part we present collected these introduction and try to keep them updated. This is also a commented collection of R-code that we documented for our own work. We hope this might be useful olso for other readers. 1.1 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],["basics.html", "2 Prerequisites: Basic statistical terms 2.1 Variables and observations 2.2 Displaying and summarizing variables 2.3 Inferential statistics 2.4 Bayes theorem and the common aim of frequentist and Bayesian methods 2.5 Classical frequentist tests and alternatives 2.6 Summary", " 2 Prerequisites: Basic statistical terms This chapter introduces some important terms useful for doing data analyses. It also introduces the essentials of the classical frequentist tests such as t- and Chisquare test. Even though we will not use nullhypotheses tests later (Amrhein, Greenland, and McShane 2019), we introduce them here because we need to understand 100 years of scientific literature. For each classical test, we provide a suggestion how to do it in a Bayesian way and we discuss some differences between the Bayesian and frequentist statistics. 2.1 Variables and observations Empirical research involves data collection. Data are collected by recording measurements of variables for observational units. An observational unit may be, for example, an individual, a plot, a time interval or a combination of those. The collection of all units ideally build a random sample of the entire population of units in that we are interested. The measurements (or observations) of the random sample are stored in a data table (sometimes also called data set, but a data set may include several data tables. A collection of data tables belonging to the same study or system is normally bundled and stored in a data base). A data table is a collection of variables (columns). Data tables normally are handled as objects of class data.frame in R. All measurements on a row in a data table belong to the same observational unit. The variables can be of different scales (Table 2.1). Table 2.1: Scales of measurements Scale Examples Properties Coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Elevational zones Identity and magnitude (values have an ordered relationship) ordered() Numeric Discrete: counts; continuous: body weight, wing length Identity, magnitude, and intervals or ratios intgeger() numeric() The aim of many studies is to describe how a variable of interest (y) is related to one or more predictor variables (x). How these variables are named differs between authors. The y-variable is called outcome variable, response or dependent variable. The x-variables are called predictors, explanatory variables or independent variables. The choose of the terms for x and y is a matter of taste. We avoid the terms dependent and independent variables because often we do not know whether the variable y is in fact depending on the x variables and also, often the x-variables are not independent of each other. In this book, we try to use outcome and predictor variables because these terms sound most neutral to us in that they refer to how the statistical model is constructed rather than to a real life relationship. 2.2 Displaying and summarizing variables While nominal and ordinal variables are summarized by giving the absolute number or the proportion of observations for each category, numeric variables normally are summarized by a location and a scatter statistics, such as the mean and the standard deviation or the median and some quantiles. The distribution of a numeric variable is graphically displayed in a histogram (Fig. 2.1). Figure 2.1: Histogram of the length of ell of statistics course participants. To draw a histogram, the variable is displayed on the x-axis and the \\(x_i\\)-values are assigned to classes. The edges of the classes are called breaks. They can be set with the argument breaks= within the function hist. The values given in the breaks= argument must at least span the values of the variable. If the argument breaks= is not specified, R searches for breaks-values that make the histogram look smooth. The number of observations falling in each class is given on the y-axis. The y-axis can be re-scaled so that the area of the histogram equals 1 by setting the argument density=TRUE. In that case, the values on the y-axis correspond to the density values of a probability distribution (Chapter 4). Location statistics are mean, median or mode. A common mean is the - arithmetic mean: \\(\\hat{\\mu} = \\bar{x} = \\frac{i=1}{n} x_i \\sum_{1}^{n}\\)(R function mean), where \\(n\\) is the sample size. The parameter \\(\\mu\\) is the (unknown) true mean of the entire population of which the \\(1,...,n\\) measurements are a random sample of. \\(\\bar{x}\\) is called the sample mean and used as an estimate for \\(\\mu\\). The \\(^\\) above any parameter indicates that the parameter value is obtained from a sample and, therefore, it may be different from the true value. The median is the 50% quantile. We find 50% of the measurements below and 50% above the median. If \\(x_1,..., x_n\\) are the ordered measurements of a variable, then the median is: - median \\(= x_{(n+1)/2}\\) for uneven \\(n\\), and median \\(= \\frac{1}{2}(x_{n/2} + x_{n/2+1})\\) for even \\(n\\) (R function median). The mode is the value that is occurring with highest frequency or that has the highest density. Scatter also is called spread, scale or variance. Variance parameters describe how far away from the location parameter single observations can be found, or how the measurements are scattered around their mean. The variance is defined as the average squared difference between the observations and the mean: variance \\(\\hat{\\sigma^2} = s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) The term \\((n-1)\\) is called the degrees of freedom. It is used in the denominator of the variance formula instead of \\(n\\) to prevent underestimating the variance. Because \\(\\bar{x}\\) is in average closer to \\(x_i\\) than the unknown true mean \\(\\mu\\) would be, the variance would be underestimated if \\(n\\) is used in the denominator. The maximum likelihood estimate (Chapter xxx.xx) of the variance corresponds to the variance formula using \\(n\\) instead of \\(n-1\\) in the denominator, see, e.g., Royle and Dorazio (2008). The variance is used to compare the degree of scatter among different groups. However, its values are difficult to interpret because of the squared unit. Therefore, the square root of the variance, the standard deviation is normally reported: standard deviation \\(\\hat{\\sigma} = s = \\sqrt{s^2}\\) (R Function sd) The standard deviation is approximately the average deviation of an observation from the sample mean. In the case of a normal distribution, about two thirds (68%) of the data are expected within one standard deviation around the mean. The variance and standard deviation each describe the scatter with a single value. Thus, we have to assume that the observations are scattered symmetrically around their mean in order to get a picture of the distribution of the measurements. When the measurements are spread asymmetrically (skewed distribution), then it may be more precise to describe the scatter with more than one value. Such statistics could be quantiles from the lower and upper tail of the data. Quantiles inform us about both location and spread of a distribution. The \\(p\\)th-quantile is the value with the property that a proportion \\(p\\) of all values are less than or equal to the value of the quantile. The median is the 50% quantile.The 25% quantile and the 75% quantile are also called the lower and upper quartiles, respectively. The range between the 25% and the 75% quartile is called the interquartile range. This range includes 50% of the distribution and is also used as a measure of scatter. The R function quantile extracts sample quantiles. The median, the quartiles, and the interquartile range can be graphically displayed using box and-whisker plots (boxplots in short, R function boxplot). The horizontal fat bars are the medians (Fig. 2.2). The boxes mark the interquartile range. The whiskers reach out to the last observation within 1.5 times the interquartile range from the quartile. Circles mark observations beyond 1.5 times the interquartile range from the quartile. par(mar=c(5,4,1,1)) boxplot(ell~car, data=dat, las=1, ylab=&quot;Lenght of ell [cm]&quot;, col=&quot;tomato&quot;, xaxt=&quot;n&quot;) axis(1, at=c(1,2), labels=c(&quot;Not owing a car&quot;, &quot;Car owner&quot;)) n &lt;- table(dat$car) axis(1, at=c(1,2), labels=paste(&quot;n=&quot;, n, sep=&quot;&quot;), mgp=c(3,2, 0)) Figure 2.2: Boxplot of the length of ell of statistics course participants who are or ar not owner of a car. The boxplot is an appealing tool for comparing location, variance and distribution of measurements among groups. 2.2.1 Correlations A correlation measures the strength with which characteristics of two variables are associated with each other (co-occur). If both variables are numeric, we can visualize the correlation using a scatterplot. par(mar=c(5,4,1,1)) plot(temp~ell, data=dat, las=1, xlab=&quot;Lenght of ell [cm]&quot;, ylab=&quot;Comfort temperature [°C]&quot;, pch=16) Figure 2.3: Scatterplot of the length of ell and the comfort temperature of statistics course participants. The covariance between variable \\(x\\) and \\(y\\) is defined as: covariance \\(q = \\frac{1}{n-1}\\sum_{i=1}^{n}((x_i-\\bar{x})*(y_i-\\bar{y}))\\) (R function cov) As for the variance, also the units of the covariance are sqared and therefore covariance values are difficult to interpret. A standardized covariance is the Pearson correlation coefficient: Pearson correlation coefficient: \\(r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\) (R function cor) Means, variances, standard deviations, covariances and correlations are sensible for outliers. Single observations containing extreme values normally have a overproportional influence on these statistics. When outliers are present in the data, we may prefer a more robust correlation measure such as the Spearman correlation or Kendalls tau. Both are based on the ranks of the measurements instead of the measurements themselves. Spearman correlation coefficient: correlation between rank(x) and rank(y) (R function cor(x,y, method=\"spearman\")) Kendalls tau: \\(\\tau = 1-\\frac{4I}{(n(n-1))}\\), where \\(I\\) = number of pairs \\((i,k)\\) for which \\((x_i &lt; x_k)\\) &amp; \\((y_i &gt; y_k)\\) or viceversa. (R function cor(x,y, method=\"kendall\")) 2.2.2 Principal components analyses PCA The principal components analysis (PCA) is a multivariate correlation analysis. A multidimensional data set with \\(k\\) variables can be seen as a cloud of points (observations) in a \\(k\\)-dimensional space. Imagine, we could move around in the space and look at the cloud from different locations. From some locations, the data looks highly correlated, whereas from others, we cannot see the correlation. That is what PCA is doing. It is rotating the coordinate system (defined by the original variables) of the data cloud so that the correlations are no longer visible. The axes of the new coordinates system are linear combinations of the original variables. They are called principal components. There are as many principal coordinates as there are original variables, i.e. \\(k\\), \\(p_1, ..., p_k\\). The principal components meet further requirements: the first component explains most variance the second component explains most of the remaining variance and is perpendicular (= uncorrelated) to the first one third component explains most of the remaining variance and is perpendicular to the first two  For example, in a two-dimensional data set \\((x_1, x_2)\\) the principal components become \\(pc_{1i} = b_{11}x_{1i} + b_{12}x_{2i}\\) \\(pc_{2i} = b_{21}x_{1i} + b_{22}x_{2i}\\) with \\(b_{jk}\\) being loadings of principal component \\(j\\) and original variable \\(k\\). Fig. 2.4 shows the two principal components for a two-dimensional data set. They can be calculated using matrix algebra: principal components are eigenvectors of the covariance or correlation matrix. Figure 2.4: Principal components of a two dimensional data set based on the covariance matrix (green) and the correlation matrix (brown). The choice between correlation or covariance matrix is essential and important. The covariance matrix is an unstandardized correlation matrix. Therefore, the units, i.e., whether cm or m are used, influence the results of the PCA if it is based on the covariance matrix. When the PCA is based on the covariance matrix, the results will change, when we change the units of one variable, e.g., from cm to m. Basing the PCA on the covariance matrix only makes sense, when the variances are comparable among the variables, i.e., if all variables are measured in the same unit and we would like to weight each variable according to its variance. If this is not the case, the PCA must be based on the correlation matrix. pca &lt;- princomp(cbind(x1,x2)) # PCA based on covariance matrix pca &lt;- princomp(cbind(x1,x2), cor=TRUE) # PCA based on correlation matrix loadings(pca) ## ## Loadings: ## Comp.1 Comp.2 ## x1 0.707 0.707 ## x2 0.707 -0.707 ## ## Comp.1 Comp.2 ## SS loadings 1.0 1.0 ## Proportion Var 0.5 0.5 ## Cumulative Var 0.5 1.0 The loadings measure the correlation of each variable with the principal components. They inform about what aspects of the data each component is measuring. The signs of the loadings are arbitrary, thus we can multiplied them by -1 without changing the PCA. Sometimes this can be handy for describing the meaning of the principal component in a paper. For example, Zbinden et al. (2018) combined the number of hunting licenses, the duration of the hunting period and the number of black grouse cocks that were allowed to be hunted per hunter in a principal component in order to measure hunting pressure. All three variables had a negative loading in the first component, so that high values of the component meant low hunting pressure. Before the subsequent analyses, for which a measure of hunting pressure was of interest, the authors changed the signs of the loadings so that this component measured hunting pressure. The proportion of variance explained by each component is, beside the loadings, an important information. If the first few components explain the main part of the variance, it means that maybe not all \\(k\\) variables are necessary to describe the data, or, in other words, the original \\(k\\) variables contain a lot of redundant information. # extract the variance captured by each component summary(pca) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.2679406 0.6263598 ## Proportion of Variance 0.8038367 0.1961633 ## Cumulative Proportion 0.8038367 1.0000000 Ridge regression is similar to doing a PCA within a linear model while components with low variance are shrinked to a higher degree than components with a high variance. 2.3 Inferential statistics 2.3.1 Uncertainty there is never a yes-or-no answer there will always be uncertainty Amrhein (2017)[https://peerj.com/preprints/26857] The decision whether an effect is important or not cannot not be done based on data alone. For a decision we should carefully consider the consequences of each decision, the aims we would like to achieve and the data. Consequences, needs and wishes of different stakeholders can be formally combined with the information in data by using decision theoretical methods. In most data analyses, particularly in basic research and when working on case studies, we normally do not consider consequences of decisions. In these cases, our job is extracting the information of data so that this information later can be used by other scientists, stakeholders and politicians to make decisions. Therefore, statistics is describing pattern in data and quantifying the uncertainty of the described patterns that is due to the fact that the data is just a (small) random sample from the population we would like to know. Quantification of uncertainty is only possible if: 1. the mechanisms that generated the data are known 2. the observations are a random sample from the population of interest Most studies aim at understanding the mechanisms that generated the data, thus they are most likely not known. To overcome that problem, we construct models, e.g. statistical models, that are (strong) abstractions of the data generating process. And we report the model assumptions. All uncertainty measures are conditional on the model we used to analyze the data, i.e., they are only reliable, if the model we used somehow realistically describes the data generating process. Because most statistical models do not describe the data generating process well, the true uncertainty almost always is much higher than the one we report. In order to obtain a random sample from the population under study, a good study design is a prerequisite. To illustrate how inference about a big population is drawn from a small sample, we here use simulated data. The advantage of using simulated data is that the mechanism that generated the data is known. However, in the example, we use different models for simulation and analysis. Imagine there are 300000 PhD students on the world and we would like to know how many statistics courses they have taken before they started their PhD (Fig. 2.5). # simulate the virtual true population set.seed(235325) # set seed for random number generator # simulate fake data of the whole population # using an overdispersed Poisson distribution # There is no need to understand more of this model # at this moment of the course than that this model # produces integer numbers (counts). Poisson models # will be introduced later. statscourses &lt;- rpois(300000, rgamma(300000, 2, 3)) # draw a random sample from the population n &lt;- 12 # sample size y &lt;- sample(statscourses, 12, replace=FALSE) Figure 2.5: Histogram of the number of statistics courses 300000 virtual PhD students have taken before their PhD started. The rugs on the x-axis indicate a random sample of the 300000 students. The black vertical line indicates the mean of the 300000 students (true mean) and the blue line indicates the mean of the sample. We observe the sample mean, what do we know about the population mean? There are two different approaches to answer this question. 1) We could ask us, how much the sample mean would scatter, if we repeat the study many times? This approach is called the frequentist statistics. 2) We could ask us for any possible value, what is the probability that it is the true population mean? To do so, we use probability theory and that is called the Bayesian statistics. Both approaches use (essentially similar) models. Only the mathematical techniques to calculate uncertainty measures differ between the two approaches. In cases when beside the data no other information is used to construct the model, then the results are approximately identical (at least for large enough sample sizes). We illustrate what uncertainty intervals mean in Fig. 2.6. A frequentist 95% confidence interval (blue horizontal semgent in Fig. 2.6) is constructed such that, if you were to (hypothetically) repeat the experiment or sampling many many times, 95% of the intervals constructed would contain the true value of the parameter. From the Bayesian posterior distribution we could construct a 95% interval (e.g., by using the 2.5% and 97.5% quantiles). This interval has traditionally been called credible interval. It can be interpreted that we are 95% sure that the true mean is inside that interval. Both interpretations only are reliable if the model is a realistic abstraction of the data generating process (or if the model assumptions are realistic). Because both terms, confidence and credible interval, suggest that the interval indicates confidence or credibility but the intervals actually show uncertainty, it has been suggested to rename the interval into compatibility or uncertainty interval Andrew Gelman and Greenland (2019). Figure 2.6: Histogram of means of repeated samples from the true populations. The scatter of these means visualize the true uncertainty of the mean in this example. The blue vertical line indicates the mean of the original sample. The blue segment shows the 95% confidence interval (obtained by fequensist methods) and the violet line shows the posterior distribution of the mean (obtained by Bayesian methods). For both solutions, we assumed a Normal distribution for the data that is different from the true mechanism that generated the data (which was an overdispersed Poisson model. The star indicates the true mean. 2.3.2 Standard error The standard error SE is, beside the uncertainty interval, an alternative possibility to measure uncertainty. It measures an average deviation of the sample mean from the (unknown) true population mean. The frequentist method for obtaining the SE is based on the central limit theorem. According to the central limit theorem the sum of independent, not necessarily normally distributed random numbers are normally distributed when sample size is large enough (Chapter 4). Because the mean is a sum (divided by a constant, the sample size) it can be assumed that the distribution of many means of samples is normal. It can be mathematically shown that the standard error SE equals the standard deviation SD of the sample divided by the square root of the sample size: frequentist SE = SD/sqrt(n) = \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\) Bayesian SE: Using Bayesian methods, the SE is the SD of the posterior distribution. It is very important to keep the difference between SE and SD in mind! SD measures the scatter of the data, whereas SE measures the uncertainty of the mean (or of another estimated parameter, Fig. 2.7). SD is a descriptive statistics describing a characteristics of the data, whereas SE is an inferential statistics showing us how far away we possibly are from the true parameter value. When sample size increases, SE becomes smaller, whereas SD does not change (given the added observations are drawn at random from the same big population as the ones already in the sample). Figure 2.7: Illustration of the difference between SD and SE. The SD measures the scatter in the data (for the sample (green tickmarks) in green and for the true population (light grey histogram) in a thin black line. The SE measures the uncertainty of the sample mean (in blue for the sample mean and in bold black for the means of the repeated samples (dark grey histogram)). 2.4 Bayes theorem and the common aim of frequentist and Bayesian methods 2.4.1 Bayes theorem for discrete events The Bayes theorem describes the probability of event A conditional on event B (the probability of A after B has already occurred) from the probability of B conditional on A and the two probabilities of the events A and B: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) Imagine, event A is The person likes wine as a birthday present. and event B The person has no car.. The conditional probability of A given B is the probability that a person not owing a car likes wine. Answers from students whether they have a car and what they like as a birthday present are summarized in Table 2.2. Table 2.2: Cross table of the students birthday preference and car ownership. car/birthday flowers wine sum no car 6 9 15 car 1 6 7 sum 7 15 22 We can apply the Bayes theorem to obtain the probability that the student likes wine given it has no car, \\(P(A|B)\\). Lets assume that only the ones who prefer wine above flowers among the students go together for having a glass of wine at the bar after the statistics course. While they drink wine, the tell each other about their cars and they obtain the probability that a student has no car given it likes wine, \\(P(B|A) = 0.6\\). During the statistics class the teacher asked the students about their car ownership and birthday preference. Therefore, they know that \\(P(A) =\\) likes wine \\(= 0.68\\) and \\(P(B) =\\) no car \\(= 0.68\\). With these information, they can obtain the probability that a student likes wine given it has no car, even if not all students without cars went to the bar: \\(P(A|B) = \\frac{0.6*0.68}{0.68} = 0.6\\). 2.4.2 Bayes theorem for continuous parameters When we use the Bayes theorem for analyzing data, then the aim is to make probability statements for parameters. Such a probability statement could be the description of what we know about the population mean after having looked at the data. Because the mean of the population is measured at a continuous scale we use a probability density function to describe what we know about the mean. The Bayes theorem can be formulated for probability density functions denoted with \\(p(\\theta)\\), e.g. for a parameter \\(\\theta\\) (for example probability density functions see Chapter 4). What we are interested in is the probability of the parameter \\(\\theta\\) given the data, i.e., \\(p(\\theta|y)\\). This probability density function is called the posterior distribution of the parameter \\(\\theta\\). Here is the Bayes theorem formulated for obtaining the posterior distribution of a parameter from the data \\(y\\), the prior distribution of the parameter \\(p(\\theta)\\) and assuming a model for the data generating process. The data model is defined by the likelihood that specifies how the data \\(y\\) is distributed given the parameters \\(p(y|\\theta)\\): \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} = \\frac{p(y|\\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta) d\\theta}\\) The probability of the data \\(p(y)\\) is also called the scaling constant, because it is a constant. It is the integral of the likelihood over all possible values of the parameter(s) of the model. 2.4.3 Estimating a mean assuming that the variance is known For illustration, we first describe a simple (unrealistic) example for which it is almost possible to follow the mathematical steps for solving the Bayes theorem even for non-mathematicians. Even if we cannot follow all steps, this example will illustrate the principle how the Bayesian theorem works for continuous parameters. The example is unrealistic because we assume that the variance \\(\\sigma^2\\) in the data \\(y\\) is known. We construct a data model by assuming that \\(y\\) is normally distributed: \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known. The function \\(Norm\\) defines the probability density function of the normal distribution (Chapter 4). The parameter, for which we would like to get the posterior distribution is \\(\\theta\\), the mean. We specify a prior distribution for \\(\\theta\\). Because the normal distribution is a conjugate prior for a normal data model with known variance, we use the normal distribution. Conjugate priors have nice mathematical properties (see Chapter 10) and are therefore preferred when the posterior distribution is obtained algebraically. That is the prior: \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) With the above data, data model and prior, the posterior distribution of the mean \\(\\theta\\) is defined by: \\(p(\\theta|y) = Norm(\\mu_n, \\tau_n)\\), where \\(\\mu_n= \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}}\\) and \\(\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\) \\(\\bar{y}\\) is the arithmetic mean of the data. Because only this value is needed in order to obtain the posterior distribution, this value is called the sufficient statistics. From the mathematical formulas above and also from Fig. 2.8 we see that the mean of the posterior distribution is a weighted average between the prior mean and \\(\\bar{y}\\) with weights equal to the precisions (\\(\\frac{1}{\\tau_0^2}\\) and \\(\\frac{n}{\\sigma^2}\\)). Figure 2.8: Hypothetical example showing the result of applying the Bayes theorem for obtaining a posterior distribution of a continuous parameter. The likelhood is defined by the data and the model, the prior is expressing the knowledge about the parameter before looking at the data. Combining the two distributions using the Bayes theorem results in the posterior distribution. 2.4.4 Estimating the mean and the variance We now move to a more realistic example, which is estimating the mean and the variance of a sample of weights of Snowfinches Montifringilla nivalis (Fig. 2.9). To analyze those data, a model with two parameters (the mean and the variance or standard deviation) is needed. The data model (or likelihood) is specified as \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\). Figure 2.9: Snowfinches stay above the treeline for winter. They come to feeders. # weight (g) y &lt;- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5) n &lt;- length(y) Because there are two parameters, we need to specify a two-dimensional prior distribution. We looked up in A. Gelman et al. (2014b) that the conjugate prior distribution in our case is an Normal-Inverse-Chisquare distribution: \\(p(\\theta, \\sigma) = N-Inv-\\chi^2(\\mu_0, \\sigma_0^2/\\kappa_0; v_0, \\sigma_0^2)\\) From the same reference we looked up how the posterior distribution looks like in our case: \\(p(\\theta,\\sigma|y) = \\frac{p(y|\\theta, \\sigma)p(\\theta, \\sigma)}{p(y)} = N-Inv-\\chi^2(\\mu_n, \\sigma_n^2/\\kappa_n; v_n, \\sigma_n^2)\\), with \\(\\mu_n= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0 + \\frac{n}{\\kappa_0+n}\\bar{y}\\), \\(\\kappa_n = \\kappa_0+n\\), \\(v_n = v_0 +n\\), \\(v_n\\sigma_n^2=v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\) For this example, we need the arithmetic mean \\(\\bar{y}\\) and standard deviation \\(s^2\\) from the sample for obtaining the posterior distribution. Therefore, these two statistics are the sufficient statistics. The above formula look intimidating, but we never really do that calculations. We let Rdoing that for us in most cases by simulating many numbers from the posterior distribution. In the end, we can visualize the distribution of these many numbers to have a look at the posterior distribution. In Fig. 2.10 the two-dimensional \\((\\theta, \\sigma)\\) posterior distribution is visualized by using simulated values. The two dimensional distribution is called the joint posterior distribution. The mountain of dots in Fig. 2.10 visualize the Normal-Inverse-Chisquare that we calculated above. When all values of one parameter is displayed in a histogram ignoring the values of the other parameter, it is called the marginal posterior distribution. Algebraically, the marginal distribution is obtained by integrating one of the two parameters out over joint posterior distribution. This step is definitively way easier when simulated values from the posterior distribution are available! For the Snowfinch weights example, with a normal data model and a conjugate prior the marginal posterior distribution of the mean \\(\\theta\\) becomes a t-distribution and the one of \\(\\sigma\\) a inverse-Chisquare distribution. Another nice property of the posterior distribution in our example is that a horizontal section through the joint posterior distribution (at a fixed value of \\(\\sigma\\), i.e. a fixed variance) leads to a normal distribution. In other words, the density of the dots in the plot at a fixed value of \\(\\sigma\\) follows a normal density function. Figure 2.10: Visualization of the joint posterior distribution for the mean and standard deviation of Snowfinch weights. The lower left panel shows the two-dimensional joint posterior distribution, whereas the upper and right panel show the marginal posterior distributions of each parameter separately. The marginal posterior distributions of every parameter is what we normally report in a paper. They inform us about what we know about these parameters (of the big unmeasured population) after having looked at the data. In our example the the marginal distribution of the mean is a t-distribution (Chapter 4). Frequentist statistical methods also use a t-distribution to describe the uncertainty of a mean for the case when the variance is not known. Thus, frequentist methods came to the same solution using a completely different approach and different techniques. Doesnt that increase dramatically our trust in statistical methods? 2.5 Classical frequentist tests and alternatives 2.5.1 Nullhypothesis testing Null hypothesis testing is constructing a model that describes how the data would look like in case of what we expect to be would not be. Then, the data is compared to how the model thinks the data should look like. If the data does not look like the model thinks they should, we reject that model and accept that our expectation may be true. To decide whether the data looks like the model constructed from the null hypothesis thinks the data should look like the p-value is used. The p-value is the probability of observing the data or more extreme data given the null hypothesis is true. Small p-values indicate that it is rather unlikely to observe the data or more extreme data given the null hypothesis \\(H_0\\) is true. Null hypothesis testing is problematic. We discuss some of the problems after having introduces the most commonly used classical tests. 2.5.2 Comparison of a sample with a fixed values (one-sample t-test) A null hypothesis is used to construct a model that can generate (hypothetical) data. For example, a null hypothesis could be \\(H_0:\\)The mean of Snowfinch weights is exactly 40g. A normal distribution with a mean of \\(\\mu_0=40\\) and a variance equal to the estimated variance in the data \\(s^2\\) is then assumed to describe how we would expect the data to look like given the null hypothesis was true. From that model it is possible to calculate the distribution of hypothetical means of many different hypothetical samples of sample size \\(n\\). The result is a t-distribution (Fig. 2.11). In classical tests, the distribution is standardized so that its variance was one. Then the sample mean, or in classical tests a standardized difference between the mean and the hypothetical mean of the null hypothesis (here 40g), called test statistics \\(t = \\frac{\\bar{y}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\), is compared to that (standardized) t-distribution. If the test statistics falls well within the expected distribution the null hypothesis is accepted. Then, the data is well compatible with the null hypothesis. However, if the test statistics falls in the tails or outside the distribution, then the null hypothesis is rejected and we could write that the mean weight of Snowfinches is statistically significantly different from 40g. Unfortunately, we cannot infer about the probability of the null hypothesis and how relevant the result is. Figure 2.11: Illustration of a one-sample t-test. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the t-distribution that shows how hypothetical sample means are expected to be distributed if the big population of Snowfinches has a mean weight of 40g (i.e., if the null hypothesis was true). Orange area shows the area of the t-distribution that lays equal or farther away from 40g than the sample mean. The orange area is the p-value. We can use the r-function t.test to calculate the p-value of a one sample t-test. t.test(y, mu=40) ## ## One Sample t-test ## ## data: y ## t = 3.0951, df = 7, p-value = 0.01744 ## alternative hypothesis: true mean is not equal to 40 ## 95 percent confidence interval: ## 40.89979 46.72521 ## sample estimates: ## mean of x ## 43.8125 The output of the r-function t.test also includes the mean and the 95% confidence interval (or compatibility or uncertainty interval) of the mean. The CI could, alternatively, be obtained as the 2.5% and 97.5% quantiles of a t-distribution with a mean equal to the sample mean, degrees of freedom equal to the sample size minus one and a standard deviation equal to the standard error of the mean. # lower limit of 95% CI mean(y) + qt(0.025, df=length(y)-1)*sd(y)/sqrt(n) ## [1] 40.89979 # upper limit of 95% CI mean(y) + qt(0.975, df=length(y)-1)*sd(y)/sqrt(n) ## [1] 46.72521 When applying the Bayes theorem for obtaining the posterior distribution of the mean we end up with the same t-distribution as described above, in case we use flat prior distributions for the mean and the standard deviation. Thus, the two different approaches end up with the same result! par(mar=c(4.5, 5, 2, 2)) hist(y, col=&quot;blue&quot;, xlim=c(30,52), las=1, freq=FALSE, main=NA, ylim=c(0, 0.3)) abline(v=mean(y), lwd=2, col=&quot;lightblue&quot;) abline(v=40, lwd=2) lines(density(bsim@coef)) text(45, 0.3, &quot;posterior distribution\\nof the mean of y&quot;, cex=0.8, adj=c(0,1), xpd=NA) Figure 2.12: Illustration of the posterior distribution of the mean. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the posterior distribution that shows what we know about the mean after having looked at the data. The area under the posterior density function that is larger than 40 is the posterior probability of the hypothesis that the true mean Snwofinch weight is larger than 40g. The posterior probability of the hypothesis that the true mean Snowfinch weight is larger than 40g, \\(P(H:\\mu&gt;40) =\\), is equal to the proportion of simulated random values from the posterior distribution, saved in the vector bsim@coef, that are larger than 40. # Two ways of calculating the proportion of values # larger than a specific value within a vector of values round(sum(bsim@coef[,1]&gt;40)/nrow(bsim@coef),2) ## [1] 0.99 round(mean(bsim@coef[,1]&gt;40),2) ## [1] 0.99 # Note: logical values TRUE and FALSE become # the numeric values 1 and 0 within the functions sum() and mean() We, thus, can be pretty sure that the mean Snowfinch weight (in the big world population) is larger than 40g. Such a conclusion is not very informative, because it does not tell us how much larger we can expect the mean Snowfinch weight to be. Therefore, we prefer reporting a credible interval (or compatibility interval or uncertainty interval) that tells us what values for the mean Snowfinch weight are compatible with the data (given the data model we used realistically reflects the data generating process). Based on such an interval, we can conclude that we are pretty sure that the mean Snowfinch weight is between 40 and 48g. # 80% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.1, 0.9)) ## 10% 90% ## 42.07725 45.54080 # 95% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.025, 0.975)) ## 2.5% 97.5% ## 40.90717 46.69152 # 99% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.005, 0.995)) ## 0.5% 99.5% ## 39.66181 48.10269 2.5.3 Comparison of the locations between two groups (two-sample t-test) Many research questions aim at measuring differences between groups. For example, we could be curious to know how different in size car owner are from people not owning a car. A boxplot is a nice possibility to visualize the ell length measurements of two (or more) groups (Fig. 2.13). From the boxplot, we do not see how many observations are in the two samples. We can add that information to the plot. The boxplot visualizes the samples but it does not show what we know about the big (unmeasured) population mean. To show that, we need to add a compatibility interval (or uncertainty interval, credible interval, confidence interval, in brown in Fig. 2.13). Figure 2.13: Ell length of car owners (Y) and people not owning a car (N). Horizontal bar = median, box = interquartile range, whiskers = extremest observation within 1.5 times the interquartile range from the quartile, circles=observations farther than 1.5 times the interquartile range from the quartile. Filled brown circles = means, vertical brown bars = 95% compatibility interval. When we added the two means with a compatibility interval, we see what we know about the two means, but we do still not see what we know about the difference between the two means. The uncertainties of the means do not show the uncertainty of the difference between the means. To do so, we need to extract the difference between the two means from a model that describes (abstractly) how the data has been generated. Such a model is a linear model that we will introduce in Chapter 11. The second parameter measures the differences in the means of the two groups. And from the simulated posterior distribution we can extract a 95% compatibility interval. Thus, we can conclude that the average ell length of car owners is with high probability between 0.5 cm smaller and 2.5 cm larger than the averag ell of people not having a car. mod &lt;- lm(ell~car, data=dat) mod ## ## Call: ## lm(formula = ell ~ car, data = dat) ## ## Coefficients: ## (Intercept) carY ## 43.267 1.019 bsim &lt;- sim(mod, n.sim=nsim) quantile(bsim@coef[,&quot;carY&quot;], prob=c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -0.501348 1.014478 2.494324 The corresponding two-sample t-test gives a p-value for the null hypothesis: The difference between the two means equals zero., a confidence interval for the difference and the two means. While the function lmgives the difference Y minus N, the function t.testgives the difference N minus Y. Luckily the two means are also given in the output, so we know which group mean is the larger one. t.test(ell~car, data=dat, var.equal=TRUE) ## ## Two Sample t-test ## ## data: ell by car ## t = -1.4317, df = 20, p-value = 0.1677 ## alternative hypothesis: true difference in means between group N and group Y is not equal to 0 ## 95 percent confidence interval: ## -2.5038207 0.4657255 ## sample estimates: ## mean in group N mean in group Y ## 43.26667 44.28571 In both possibilities, we used to compare the to means, the Bayesian posterior distribution of the difference and the t-test or the confidence interval of the difference, we used a data model. We thus assumed that the observations are normally distributed. In some cases, such an assumption is not a reasonable assumption. Then the result is not reliable. In such cases, we can either search for a more realistic model or use non-parametric (also called distribution free) methods. Nowadays, we have almost infinite possibilities to construct data models (e.g. generalized linear models and beyond). Therefore, we normally start looking for a model that fits the data better. However, in former days, all these possiblities did not exist (or were not easily available for non-mathematicians). Therefore, we here introduce two of such non-parametric methods, the Wilcoxon-test (or Mann-Whitney-U-test) and the randomisation test. Some of the distribution free statistical methods are based on the rank instead of the value of the observations. The principle of the Wilcoxon-test is to rank the observations and sum the ranks per group. It is not completely true that the non-parametric methods do not have a model. The model of the Wilcoxon-test knows how the difference in the sum of the ranks between two groups is distributed given the mean of the two groups do not differ (null hypothesis). Therefore, it is possible to get a p-value, e.g. by the function wilcox.test. wilcox.test(ell~car, data=dat) ## ## Wilcoxon rank sum test with continuity correction ## ## data: ell by car ## W = 34.5, p-value = 0.2075 ## alternative hypothesis: true location shift is not equal to 0 The note in the output tells us that ranking is ambiguous, when some values are equal. Equal values are called ties when they should be ranked. The result of the Wilcoxon-test tells us how probable it is to observe the difference in the rank sum between the two sample or a more extreme difference given the means of the two groups are equal. That is at least something. A similar result is obtained by using a randomisation test. This test is not based on ranks but on the original values. The aim of the randomisation is to simulate a distribution of the difference in the arithmetic mean between the two groups assuming this difference would be zero. To do so, the observed values are randomly distributed among the two groups. Because of the random distribution among the two groups, we expect that, if we repeat that virtual experiment many times, the average difference between the group means would be zero (both virtual samples are drawn from the same big population). We can use a loop in R for repeating the random re-assignement to the two groups and, each time, extracting the difference between the group means. As a result, we have a vector of many (nsim) values that all are possible differences between group means given the two samples were drawn from the same population. The proportion of these values that have an equal or larger absolute value give the probability that the observed or a larger difference between the group means is observed given the null hypothesis would be true, thus that is a p-value. diffH0 &lt;- numeric(nsim) for(i in 1:nsim){ randomcars &lt;- sample(dat$car) rmod &lt;- lm(ell~randomcars, data=dat) diffH0[i] &lt;- coef(rmod)[&quot;randomcarsY&quot;] } mean(abs(diffH0)&gt;abs(coef(mod)[&quot;carY&quot;])) # p-value ## [1] 0.1858 Visualizing the possible differences between the group means given the null hypothesis was true shows that the observed difference is well within what is expected if the two groups would not differ in their means (Fig. 2.14). Figure 2.14: Histogram if differences between the means of randomly assigned groups (grey) and the difference between the means of the two observed groups (red) The randomization test results in a p-value and, we could also report the observed difference between the group means. However, it does not tell us, what values of the difference all would be compatible with the data. We do not get an uncertainty measurement for the difference. In order to get a compatibility interval without assuming a distribution for the data (thus non-parametric) we could bootstrap the samples. Bootstrapping is sampling observations from the data with replacement. For example, if we have a sample of 8 observations, we draw 8 times a random observation from the 8 observation. Each time, we assume that all 8 observations are available. Thus a bootstrapped sample could include some observations several times, whereas others are missing. In this way, we simulate the variance in the data that is due to the fact that our data do not contain the whole big population. Also bootstrapping can be programmed in R using a loop. diffboot &lt;- numeric(1000) for(i in 1:nsim){ ngroups &lt;- 1 while(ngroups==1){ bootrows &lt;- sample(1:nrow(dat), replace=TRUE) ngroups &lt;- length(unique(dat$car[bootrows])) } rmod &lt;- lm(ell~car, data=dat[bootrows,]) diffboot[i] &lt;- coef(rmod)[2] } quantile(diffboot, prob=c(0.025, 0.975)) ## 2.5% 97.5% ## -0.3395643 2.4273810 The resulting values for the difference between the two group means can be interpreted as the distribution of those differences, if we had repeated the study many times (Fig. 2.15). A 95% interval of the distribution corresponds to a 95% compatibility interval (or confidence interval or uncertainty interval). hist(diffboot); abline(v=coef(mod)[2], lwd=2, col=&quot;red&quot;) Figure 2.15: Histogram of the boostrapped differences between the group means (grey) and the observed difference. For both methods, randomisation test and bootstrapping, we have to assume that all observations are independent. Randomization and bootstrapping becomes complicated or even unfeasible when data are structured. 2.6 Summary Bayesian data analysis is applying the Bayes theorem for summarizing knowledge based on data, priors and the model assumptions. Frequentist statistics is quantifying uncertainty by hypothetical repetitions. "],["analyses_steps.html", "3 Data analysis step by step 3.1 Plausibility of Data 3.2 Relationships 3.3 Data Distribution 3.4 Preparation of Explanatory Variables 3.5 Data Structure 3.6 Define Prior Distributions 3.7 Fit the Model 3.8 Check Model 3.9 Model Uncertainty 3.10 Draw Conclusions Further reading", " 3 Data analysis step by step In this chapter we provide a checklist with some guidance for data analysis. However, do not expect the list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps 3.2 to 3.8 until we find a model that fit the data well and that is realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful model. There is a chance and danger at the same time: we may find interesting results that answer different questions than we asked originally. They may be very exciting and important, however they may be biased. We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we have to be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process. 3.1 Plausibility of Data Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries. See chapter 5 for useful R-code that can be used for data preparation and to make plausibility controls. 3.2 Relationships Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful. We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible. 3.3 Data Distribution What is the nature of the variable of interest (outcome, dependent variable)? At this stage, there is no use of formally comparing the distribution of the outcome variable to a statistical distribution, because the rawdata is not required to follow a specific distribution. The models assume that conditional on the explanatory variables and the model structure, the outcome variable follows a specific distribution. Therefore, checking how well the chosen distribution fits to the data is done after the model fit 3.8. This first choice is solely done based on the nature of the data. Normally, our first choice is one of the classical distributions for which robust software for model fitting is available. Here is a rough guideline for this first choice: continuous measurements \\(\\Longrightarrow\\) normal distribution &gt; exceptions: time-to-event data \\(\\Longrightarrow\\) see survival analysis count \\(\\Longrightarrow\\) Poisson or negative-binomial distribution count with upper bound (proportion) \\(\\Longrightarrow\\) binomial distribution binary \\(\\Longrightarrow\\) Bernoully distribution rate (count by a reference) \\(\\Longrightarrow\\) Poisson including an offset nominal \\(\\Longrightarrow\\) multinomial distribution Chapter 4 gives an overview of the distributions that are most relevant for ecologists. 3.4 Preparation of Explanatory Variables Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step: Is the variance (of the explanatory variable) big enough so that an effect of the variable can be measured? Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable (e.g., log, square-root). Does it show a bimodal distribution? Consider making the variable binary. Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a trans- formation (e.g., log) could linearize the relationship between x and y. Centering: Centering (\\(x.c = x-mean(x)\\)) is a transformation that produces a variable with a mean of 0. Centering is optional. We have two reasons to center a predictor variable. First, it helps the model fitting algorithm to better converge because it reduces correlations among model parameters. Second, with centered predictors, the intercept and main effects in the linear model are better interpretable (they are measured at the center of the data instead of at the covariate value of 0 which may be far off). Scaling: Scaling (\\(x.s = x/c\\), where \\(c\\) is a constant) is a transformation that changes the unit of the variable. Also scaling is optional. We have three reasons to scale an predictor variable. First, to make the effect sizes better understandable. For example, a population change from one year to the next may be very small and hard to interpret. When we give the change for a 10-year period, its ecological meaning is better understandable. Second, to make the estimate of the effect sizes comparable between variables, we may use \\(x.s = x/sd(x)\\). The resulting variable has a unit of one standard deviation. A standard deviation may be comparable between variables that oritinally are measured in different units (meters, seconds etc). A. Gelman and Hill (2007) (p. 55 f) propose to scale the variables by two times the standard deviation (\\(x.s = x/(2*sd(x))\\)) to make effect sizes comparable between numeric and binary variables. Third, scaling can be important for model convergence, especially when polynomials are included. Also, consider the use of orthogonal polynomials, see Chapter 4.2.9 in Korner-Nievergelt et al. (2015). Collinearity: Look at the correlation among the explanatory variables (pairs plot or correlation matrix). If the explanatory variables are correlated, go back to step 2. Also, Chapter 4.2.7 in Korner-Nievergelt et al. (2015) discusses collinearity. Are interactions and polynomial terms needed in the model? If not already done in step 2, think about the relationship between each explanatory variable and the dependent variable. Is it linear or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM). May the effect of one explanatory variable depend on the value of another explanatory variable (interaction)? 3.5 Data Structure After having taken into account all of the (fixed effect) terms from step 4: are the observations independent or grouped/structured? What random factors are needed in the model? Are the data obviously temporally or spatially correlated? Or, are other correlation structures present, such as phylogenetic relationships? Our strategy is to start with a rather simple model that may not account for all correlation structures that in fact are present in the data. We first, only include those that are known to be important a priory. Only when residual analyses reveals important additional correlation structures, we include them in the model. 3.6 Define Prior Distributions Decide whether we would like to use informative prior distributions or whether we would like use priors that only have a negligible effect on the results. When the results are later used for informing authorities or for making a decision (as usual in applied sciences), then we would like to base the results on all information available. Information from the literature is then used to construct informative prior distributions. In contrast to applied sciences, in basic research we often would like to show only the information in the data that should not be influenced by earlier results. Therefore, in basic research we look for priors that do not influence the results. 3.7 Fit the Model Fit the model. 3.8 Check Model We assess model fit by graphical analyses of the residuals (Chapter 6 in Korner-Nievergelt et al. (2015)), by predictive model checking (Section 10.1 in Korner-Nievergelt et al. (2015)), or by sensitivity analysis (Chapter 15 in Korner-Nievergelt et al. (2015)). For non-Gaussian models it is often easier to assess model fit using pos- terior predictive checks (Chapter 10 in Korner-Nievergelt et al. (2015)) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, we list in Chapter 16 of Korner-Nievergelt et al. (2015) some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases. 3.9 Model Uncertainty If, while working through steps 1 to 8, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we then turn to the methods presented in Chapter 11 (Korner-Nievergelt et al. (2015)) to draw inference from more than one model. If we have only one model, we proceed to 3.10. 3.10 Draw Conclusions Simulate values from the joint posterior distribution of the model parameters (sim or Stan). Use these samples to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities. Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],["distributions.html", "4 Probability distributions 4.1 Introduction 4.2 Discrete distributions 4.3 Continuous distributions", " 4 Probability distributions 4.1 Introduction In Bayesian statistics, probability distributions are used for two fundamentally different purposes. First, they are used to describe distributions of data. These distributions are also called data distributions. Second, probability distributions are used to express information or knowledge about parameters. Such distributions are called prior or posterior distributions. The data distributions are part of descriptive statistics, whereas prior and posterior distributions are part of inferential statistics. The usage of probability distributions for describing data does not differ between frequentist and Bayesian statistics. Classically, the data distribution is known as model assumption. Specifically to Bayesian statistics is the formal expression of statistical uncertainty (or information or knowledge) by prior and posterior distributions. We here introduce some of the most often used probability distributions and present how they are used in statistics. Probability distributions are grouped into discrete and continuous distributions. Discrete distributions define for any discrete value the probability that exactly this value occurs. They are usually used as data distributions for discrete data such as counts. The function that describes a discrete distribution is called a probability function (their values are probabilities, i.e. a number between 0 and 1). Continuous distributions describe how continuous values are distributed. They are used as data distributions for continuous measurements such as body size and also as prior or posterior distributions for parameters such as the mean body size. Most parameters are measured on a continuous scale. The function that describes continuous distributions is called density function. Its values are non-negative and the area under the density function equals one. The area under a density function corresponds to probabilities. For example, the area under the density function above the value 2 corresponds to the proportion of data with values above 2 if the density function describes data, or it corresponds to the probability that the parameter takes on a value bigger than 2 if the density function is a posterior distribution. 4.2 Discrete distributions 4.2.1 Bernoulli distribution Bernoulli distributed data take on the exact values 0 or 1. The value 1 occurs with probability \\(p\\). \\(x \\sim Bernoulli(p)\\) The probability function is \\(p(x) = p^x(1-p)^{1-x}\\). The expected value is \\(E(x) = p\\) and the variance is \\(Var(x) = p(1-p)\\). The flipping experiment of a fair coin produces Bernoulli distributed data with \\(p=0.5\\) if head is taken as one and tail is taken as zero. The Bernoulli distribution is usually used as a data model for binary data such as whether a nest box is used or not, whether a seed germinated or not, whether a species occurs or not in a plot etc. 4.2.2 Binomial distribution The binomial distribution describes the number of ones among a predefined number of Bernoulli trials. For example, the number of heads among 20 coin flips, the number of used nest boxes among the 50 nest boxes of the study area, or the number of seed that germinated among the 10 seeds in the pot. Binomially distributed data are counts with an upper limit (\\(n\\)). \\(x \\sim binomial(p,n)\\) The probability function is \\(p(x) = {n\\choose x} p^x(1-p)^{(n-x)}\\). The expected value is \\(E(x) = np\\) and the variance is \\(Var(x) = np(1-p)\\). 4.2.3 Poisson distribution The Poisson distribution describes the distribution of counts without upper boundary, i.e., when we know how many times something happened but we do not know how many times it did not happen. A typical Poisson distributed variable is the number of raindrops in equally-sized grid cells on the floor, if we can assume that every rain drop falls down completely independent of the other raindrops and at a completely random point. \\(x \\sim Poisson(\\lambda)\\) The probability function is \\(p(x) = \\frac{1}{x!}\\lambda^xexp(-\\lambda)\\). It is implemented in the R-function dpois. The expected values is \\(E(x) = \\lambda\\) and the variance is \\(Var(x) = \\lambda\\). An important property of the Poisson distribution is that it has only one parameter \\(\\lambda\\). As a consequence, it does not allow for any combination of means and variances. In fact, they are assumed to be the same. In the real world, most count data do not behave like rain drops, that means variances of count data are in most real world examples not equal to the mean as assumed by the Poisson distribution. Therefore, when using the Poisson distribution as a data model, it is important to check for overdispersion. Further, note that not all variables measured as an integer number are count data! For example, the number of days an animal spends in a specific area before moving away looks like a count. However, it is a continuous measurement. The duration an animal spends in a specific areas could also be measured in hours or minutes. The Poisson model assumes that the counts are all events that happened. However, an emigration of an animal is just one event, independent of how long it stayed. 4.2.4 Negative-binomial distribution The negative-binomial distribution represents the number of zeros which occur in a sequence of Bernoulli trials before a target number of ones is reached. It is hard to see this situation in, e.g., the number of individuals counted on plots. Therefore, we were reluctant to introduce this distribution in our old book (Korner-Nievergelt et al. 2015). However, the negative-binomial distribution often fits much better to count data than the Poisson model because it has two parameters and therefore allows for fitting both the mean and the variance to the data. Therefore, we started using the negative-binomial distribution as a data model more often. \\(x \\sim negative-binomial(p,n)\\) Its probability function is rather complex: \\(p(x) = \\frac{\\Gamma(x+n)}{\\Gamma(n) x!} p^n (1-p)^x\\) with \\(\\Gamma\\) being the Gamma-function. Luckily, the negative-binomial probability function is implemented in the R-function dnegbin. The expected value of the negative-binomial distribution is \\(E(x) = n\\frac{(1-p)}{p}\\) and the variance is \\(Var(x) = n\\frac{(1-p)}{p^2}\\). We like to specify the distribution using the mean and the scale parameter \\(x \\sim negativ-binomial(\\mu,\\theta)\\), because in practice we often specify a linear predictor for the logarithm of the mean \\(\\mu\\). 4.3 Continuous distributions 4.3.1 Beta distribution The beta distribution is restricted to the range [0,1]. It describes the knowledge about a probability parameter. Therefore, it is usually used as a prior or posterior distribution for probabilities. The beta distribution sometimes is used as a data model for continuous probabilities, However, it is difficult to get a good fit of such models, because measured proportions often take on values of zero and ones which is not allowed in most (but not all) beta distributions, thus this distribution does not describe the variance of measured proportions correctly. However, for describing knowledge of a proportion parameter, it is a very convenient distribution with two parameters. \\(x \\sim beta(a,b)\\) Its density function is \\(p(x) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1}\\). The R-function dbetadoes the rather complicated calculations for us. The expected value of a beta distribution is \\(E(x) = \\frac{a}{(a+b)}\\) and the variance is \\(Var(x) = \\frac{ab}{(a+b)^2(a+b+1)}\\). The \\(beta(1,1)\\) distribution is equal to the \\(uniform(0,1)\\) distribution. The higher the sum of \\(a\\) and \\(b\\), the more narrow is the distribution (Figure 4.1). Figure 4.1: Beta distributions with different parameter values. 4.3.2 Normal distribution The normal, or Gaussian distribution is widely used since a long time in statistics. It describes the distribution of measurements that vary because of a sum of random errors. Based on the central limit theorem, sample averages are approximately normally distributed (2). \\(x \\sim normal(\\mu, \\sigma^2)\\) The density function is \\(p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x -\\mu)^2)\\) and it is implemented in the R-function dnorm. The expected value is \\(E(x) = \\mu\\) and the variance is \\(Var(x) = \\sigma^2\\). The variance parameter can be specified to be a variance, a standard deviation or a precision. Different software (or authors) have different habits, e.g., R and Stan use the standard deviation sigma \\(\\sigma\\), whereas BUGS (WinBugs, OpenBUGS or jags) use the precision, which is the inverse of the variance $= $. The normal distribution is used as a data model for measurements that scatter symmetrically around a mean, such as body size (in m), food consumption (in g), or body temperature (°C). The normal distribution also serves as prior distribution for parameters that can take on negative or positive values. The larger the variance, the flatter (less informative) is the distribution. The standard normal distribution is a normal distribution with a mean of zero and a variance of one, \\(z \\sim normal(0, 1)\\). The standard normal distribution is also called the z-distribution. Or, a z-variable is a variable with a mean of zero and a standard deviation of one. x &lt;- seq(-3, 3, length=100) y &lt;- dnorm(x) # density function of a standard normal distribution dat &lt;- tibble(x=x,y=y) plot(x,y, type=&quot;l&quot;, lwd=2, col=&quot;#d95f0e&quot;, las=1, ylab=&quot;normal density of x&quot;) segments(0, dnorm(1), 1, dnorm(1), lwd=2) segments(0, dnorm(0), 0, 0) text(0.5, 0.23, expression(sigma)) Figure 4.2: Standard normal distribution Plus minus one times the standard deviation (\\(\\sigma\\)) from the mean includes around 68% of the area under the curve (corresponding to around 68% of the data points in case the normal distribution is used as a data model, or 68% of the prior or posterior mass if the normal distribution is used to describe the knowledge about a parameter). Plus minus two times the standard deviation includes around 95% of the area under the curve. 4.3.3 Gamma distribution The gamma distribution is a continuous probability distribution for strictly positive values (zero is not included). The shape of the gamma distribution is right skewed with a long upper tail, whereas most of the mass is centered around a usually small value. It has two parameters, the shape \\(\\alpha\\) and the inverse scale \\(\\beta\\). \\(x \\sim gamma(\\alpha,\\beta)\\) Its density function is \\(p(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{(\\alpha-1)} exp(-\\beta x)\\), or dgamma in R. The expected value is \\(E(x) = \\frac{\\alpha}{\\beta}\\) and the variance is \\(Var(x) = \\frac{\\alpha}{\\beta^2}\\). The gamma distribution is becoming more and more popular as a data model for durations (time to event) or other highly right skewed continuous measurements that do not have values of zero. The gamma distribution is a conjugate prior distribution for the mean of a Poisson distribution and for the precision parameter of a normal distribution. However, in hierarchical models with normally distributed random effects, it is not recommended to use the gamma distribution as a prior distribution for the among-group variance (A. Gelman 2006). The Cauchy or folded t-distribution seem to have less influence on the posterior distributions of the variance parameters. 4.3.4 Cauchy distribution The Cauchy distribution is a symmetric distribution with much heavier tails compared to the normal distribution. $ x Cauchy(a,b)$ Its probability density function is \\(p(x) = \\frac{1}{\\pi b[1+(\\frac{x-a}{b})^2]}\\). The mean and the variance of the Cauchy distribution are not defined. The median is \\(a\\). The part of the Cauchy distribution for positive values, i.e., half of the Cauchy distribution, is often used as a prior distribution for variance parameters. 4.3.5 t-distribution The t-distribution is the marginal posterior distribution of a the mean of a sample with unknown variance when conjugate prior distributions are used to obtain the posterior distribution. The t-distribution has three parameters, the degrees of freedom \\(v\\), the location \\(\\mu\\) and the scale \\(\\sigma\\). \\(x \\sim t(v, \\mu, \\sigma)\\) Its density function is \\(p(x) = \\frac{\\Gamma((v+1)/2)}{\\Gamma(v/2)\\sqrt{v\\pi}\\sigma}(1+\\frac{1}{v}(\\frac{x-\\mu}{\\sigma})^2)^{-(v+1)/2}\\). Its expected value is \\(E(x) = \\mu\\) for \\(v&gt;1\\) and the variance is \\(Var(x) = \\frac{v}{v-2}\\sigma ^2\\) for \\(v&gt;2\\). The t-distribution is sometimes used as data model. Because of its heavier tails compared to the normal model, the model parameters are less influenced by measurement errors when a t-distribution is used instead of a normal distribution. This is called robust statistics. Similar to the Cauchy distribution, the folded t-distribution, i.e., the positive part of the t-distribution, can serve as a prior distribution for variance parameters. 4.3.6 F-distribution The F-distribution is not important in Bayesian statistics. Ratios of sample variances drawn from populations with equal variances follow an F-distribution. The density function of the F-distribution is even more complicated than the one of the t-distribution! We do not copy it here. Further, we have not yet met any Bayesian example where the F-distribution is used (that does not mean that there is no). It is used in frequentist analyses in order to compare variances, e.g. within ANOVAs. If two variances only differ because of natural variance in the data (nullhypothesis) then \\(\\frac{Var(X_1)}{Var(X_2)}\\sim F_{df_1,df_2}\\). Figure 4.3: Different density functions of the F statistics "],["rfunctions.html", "5 Important R-functions 5.1 Data preparation 5.2 Figures 5.3 Summary", " 5 Important R-functions THIS CHAPTER IS UNDER CONSTRUCTION!!! 5.1 Data preparation 5.2 Figures 5.3 Summary "],["reproducibleresearch.html", "6 Reproducible research 6.1 Summary 6.2 Further reading", " 6 Reproducible research THIS CHAPTER IS UNDER CONSTRUCTION!!! 6.1 Summary 6.2 Further reading Rmarkdown: The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages. Bookdown by Yihui Xie: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. The book can be exported to HTML, PDF, and e-books (e.g. EPUB). The book style is customizable. You can easily write and preview the book in RStudio IDE or other editors, and host the book wherever you want (e.g. bookdown.org). Our book is written using bookdown. "],["furthertopics.html", "7 Further topics 7.1 Bioacoustic analyse 7.2 Python", " 7 Further topics This is a collection of short introductions or links with commented R code that cover other topics that might be useful for ecologists. 7.1 Bioacoustic analyse Bioacoustic analyses are nicely covered in a blog by Marcelo Araya-Salas. 7.2 Python Like R, python is a is a high-level programming language that is used by many ecologists. The reticulate package provides a comprehensive set of tools for interoperability between Python and R. "],["PART-II.html", "8 Introduction to PART II Further reading", " 8 Introduction to PART II Further reading A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. A. Gelman et al. 2014b) and Trevor Hastie (e.g. Efron and Hastie (2016)) because both explain complicated things in a concise and understandable way. "],["bayesian_paradigm.html", "9 The Bayesian paradigm 9.1 Introduction 9.2 Summary", " 9 The Bayesian paradigm THIS CHAPTER IS UNDER CONSTRUCTION!!! 9.1 Introduction 9.2 Summary xxx "],["priors.html", "10 Prior distributions 10.1 Introduction 10.2 How to choose a prior 10.3 Prior sensitivity", " 10 Prior distributions THIS CHAPTER IS UNDER CONSTRUCTION!!! 10.1 Introduction The prior is an integral part of a Bayesian model. We must specify one. When to use informative priors: In practise (management, politics etc.) we would like to base our decisions on all information available. Therefore, we consider it to be responsible including informative priors in applied research whenever possible. Priors allow combining information from the literature with information in data or combining information from different data sets. When using non-informative priors: in basic research when results should only report the information in the current data set. Results from a case study may later be used in a meta-analyses that assumes independence across the different studies included. 10.2 How to choose a prior important reference: Lemoine (2019) TODO 10.2.1 Priors for variance parameters A. Gelman (2006) discusses advantages of using folded t-distributions or cauchy distributions as prior distributions for variance parameters in hierarchical models. When specifying t-distributions, we find it hard to imagine how the distributions looks like with what parameter values. Therefore, we simulate values from different distributions and look at the histograms. Because the parameterisation of the t-distribution differst among software language, it is important to use the software the model is finally fitted in Figure 10.1 we give some examples of folded t-distributions specified in jags using different values for the precision (second parameter) and degrees of freedom (third parameter). Figure 10.1: Folded t-distributions with different precisions and degrees of freedom. The panel titles give the jags code of the distribution. Dark blue vertical lines indicate 90% quantiles, light-blue lines indicate 98% quantiles. GIVE EXAMPLE FOR STAN TOO 10.3 Prior sensitivity xxx "],["lm.html", "11 Normal Linear Models 11.1 Linear regression 11.2 Linear model with one categorical predictor (one-way ANOVA) 11.3 Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression 11.4 Partial coefficients and some comments on collinearity 11.5 Ordered Factors and Contrasts 11.6 Quadratic and Higher Polynomial Terms", " 11 Normal Linear Models 11.1 Linear regression 11.1.1 Background Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression. Typical questions that can be answered using linear regression are: How does \\(y\\) change with changes in \\(x\\)? How is y predicted from \\(x\\)? An ordinary linear regression (i.e., one numeric \\(x\\) and one numeric \\(y\\) variable) can be represented by a scatterplot of \\(y\\) against \\(x\\). We search for the line that ts best and describe how the observations scatter around this regression line (see Fig. 11.2 for an example). The model formula of a simple linear regression with one continuous predictor variable \\(x_i\\) (the subscript \\(i\\) denotes the \\(i=1,\\dots,n\\) data points) is: \\[\\begin{align} \\mu_i &amp;=\\beta_0 + \\beta_1 x_i \\\\ y_i &amp;\\sim normal(\\mu_i, \\sigma^2) \\tag{11.1} \\end{align}\\] While the first part of Equation (11.1) describes the regression line, the second part describes how the data points, also called observations, are distributed around the regression line (Figure 11.1). In other words: the observation \\(y_i\\) stems from a normal distribution with mean \\(\\mu_i\\) and variance \\(\\sigma^2\\). The mean of the normal distribution, \\(\\mu_i\\) , equals the sum of the intercept (\\(b_0\\) ) and the product of the slope (\\(b_1\\)) and the continuous predictor value, \\(x_i\\). Equation (11.1) is called the data model, because it describes mathematically the process that has (or, better, that we think has) produced the data. This nomenclature also helps to distinguish data models from models for parameters such as prior or posterior distributions. The differences between observation \\(y_i\\) and the predicted values \\(\\mu_i\\) are the residuals (i.e., \\(\\epsilon_i=y_i-\\mu_i\\)). Equivalently to Equation (11.1), the regression could thus be written as: \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\ \\epsilon_i &amp;\\sim normal(0, \\sigma^2) \\tag{11.2} \\end{align}\\] We prefer the notation in Equation (11.1) because, in this formula, the stochastic part (second row) is nicely separated from the deterministic part (first row) of the model, whereas, in the second notation (11.2) the rst row contains both stochastic and deterministic parts. For illustration, we here simulate a data set and below t a linear regression to these simulated data. The advantage of simulating data is that the following analyses can be reproduced without having to read data into R. Further, for simulating data, we need to translate the algebraic model formula into R language which helps us understanding the model structure. set.seed(34) # set a seed for the random number generator # define the data structure n &lt;- 50 # sample size x &lt;- runif(n, 10, 30) # sample values of the predictor variable # define values for each model parameter sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope # simulate y-values from the model mu &lt;- b0 + b1 * x # define the regression line (deterministic part) y &lt;- rnorm(n, mu, sd = sigma) # simulate y-values # save data in a data.frame dat &lt;- tibble(x = x, y = y) Figure 11.1: Illustration of a linear regression. The blue line represents the deterministic part of the model, i.e., here regression line. The stochastic part is represented by a probability distribution, here the normal distribution. The normal distribution changes its mean but not the variance along the x-axis, and it describes how the data are distributed. The blue line and the orange distribution together are a statistical model, i.e., an abstract representation of the data which is given in black. Using matrix notation equation (11.1) can also be written in one row: \\[\\boldsymbol{y} \\sim Norm(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2\\boldsymbol{I})\\] where \\(\\boldsymbol{ I}\\) is the \\(n \\times n\\) identity matrix (it transforms the variance parameter to a \\(n \\times n\\) matrix with its diagonal elements equal \\(\\sigma^2\\) ; \\(n\\) is the sample size). The multiplication by \\(\\boldsymbol{ I}\\) is necessary because we use vector notation, \\(\\boldsymbol{y}\\) instead of \\(y_{i}\\) . Here, \\(\\boldsymbol{y}\\) is the vector of all observations, whereas \\(y_{i}\\) is a single observation, \\(i\\). When using vector notation, we can write the linear predictor of the model, \\(\\beta_0 + \\beta_1 x_i\\) , as a multiplication of the vector of the model coefcients \\[\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\] times the model matrix \\[\\boldsymbol{X} = \\begin{pmatrix} 1 &amp; x_1 \\\\ \\dots &amp; \\dots \\\\ 1 &amp; x_n \\end{pmatrix}\\] where \\(x_1 , \\dots, x_n\\) are the observed values for the predictor variable, \\(x\\). The rst column of \\(\\boldsymbol{X}\\) contains only ones because the values in this column are multiplied with the intercept, \\(\\beta_0\\) . To the intercept, the product of the second element of \\(\\boldsymbol{\\beta}\\), \\(\\beta_1\\) , with each element in the second column of \\(\\boldsymbol{X}\\) is added to obtain the predicted value for each observation, \\(\\boldsymbol{\\mu}\\): \\[\\begin{align} \\boldsymbol{X \\beta}= \\begin{pmatrix} 1 &amp; x_1 \\\\ \\dots &amp; \\dots \\\\ 1 &amp; x_n \\end{pmatrix} \\times \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} = \\begin{pmatrix} \\beta_0 + \\beta_1x_1 \\\\ \\dots \\\\ \\beta_0 + \\beta_1x_n \\end{pmatrix}= \\begin{pmatrix} \\hat{y}_1 \\\\ \\dots \\\\ \\hat{y}_n \\end{pmatrix} = \\boldsymbol{\\mu} \\tag{11.3} \\end{align}\\] 11.1.2 Fitting a Linear Regression in R In Equation (11.1), the fitted values \\(\\mu_i\\) are directly dened by the model coefcients, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) . Therefore, when we can estimate \\(\\beta_{0}\\), \\(\\beta_{1}\\) , and \\(\\sigma^2\\), the model is fully dened. The last parameter \\(\\sigma^2\\) describes how the observations scatter around the regression line and relies on the assumption that the residuals are normally distributed. The estimates for the model parameters of a linear regression are obtained by searching for the best tting regression line. To do so, we search for the regression line that minimizes the sum of the squared residuals. This model tting method is called the least-squares method, abbreviated as LS. It has a very simple solution using matrix algebra (see e.g., Aitkin et al. 2009). The least-squares estimates for the model parameters of a linear regression are obtained in R using the function lm. mod &lt;- lm(y ~ x, data = dat) coef(mod) ## (Intercept) x ## 2.0049517 0.6880415 summary(mod)$sigma ## [1] 5.04918 The object mod produced by lm contains the estimates for the intercept, \\(\\beta_0\\) , and the slope, \\(\\beta_1\\). The residual standard deviation \\(\\sigma^2\\) is extracted using the function summary. We can show the result of the linear regression as a line in a scatter plot with the covariate (x) on the x-axis and the observations (y) on the y-axis (Fig. 11.2). Figure 11.2: Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The tted values lie where the orange dotted lines touch the blue regression line. Conclusions drawn from a model depend on the model assumptions. When model assumptions are violated, estimates usually are biased and inappropriate conclusions can be drawn. We devote Chapter 12 to the assessment of model assumptions, given its importance. 11.1.3 Drawing Conclusions To answer the question about how strongly \\(y\\) is related to \\(x\\) taking into account statistical uncertainty we look at the joint posterior distribution of \\(\\boldsymbol{\\beta}\\) (vector that contains \\(\\beta_{0}\\) and \\(\\beta_{1}\\) ) and \\(\\sigma^2\\) , the residual variance. The function sim calculates the joint posterior distribution and renders a simulated values from this distribution. What does sim do? It simulates parameter values from the joint posterior distribution of a model assuming flat prior distributions. For a normal linear regression, it rst draws a random value, \\(\\sigma^*\\) from the marginal posterior distribution of \\(\\sigma\\), and then draws random values from the conditional posterior distribution for \\(\\boldsymbol{\\beta}\\) given \\(\\sigma^*\\) (A. Gelman et al. 2014a). The conditional posterior distribution of the parameter vector \\(\\boldsymbol{\\beta}\\), \\(p(\\boldsymbol{\\beta}|\\sigma^*,\\boldsymbol{y,X})\\) can be analytically derived. With at prior distributions, it is a uni- or multivariate normal distribution \\(p(\\boldsymbol{\\beta}|\\sigma^*,\\boldsymbol{y,X})=normal(\\boldsymbol{\\hat{\\beta}},V_\\beta,(\\sigma^*)^2)\\) with: \\[\\begin{align} \\boldsymbol{\\hat{\\beta}=(\\boldsymbol{X^TX})^{-1}X^Ty} \\tag{11.4} \\end{align}\\] and \\(V_\\beta = (\\boldsymbol{X^T X})^{-1}\\). The marginal posterior distribution of \\(\\sigma^2\\) is independent of specic values of \\(\\boldsymbol{\\beta}\\). It is, for at prior distributions, an inverse chi-square distribution \\(p(\\sigma^2|\\boldsymbol{y,X})=Inv-\\chi^2(n-k,\\sigma^2)\\), where \\(\\sigma^2 = \\frac{1}{n-k}(\\boldsymbol{y}-\\boldsymbol{X,\\hat{\\beta}})^T(\\boldsymbol{y}-\\boldsymbol{X,\\hat{\\beta}})\\), and \\(k\\) is the number of parameters. The marginal posterior distribution of \\(\\boldsymbol{\\beta}\\) can be obtained by integrating the conditional posterior distribution \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})=normal(\\boldsymbol{\\hat{\\beta}},V_\\beta\\sigma^2)\\) over the distribution of \\(\\sigma^2\\) . This results in a uni- or multivariate \\(t\\)-distribution. Because sim simulates values \\(\\beta_0^*\\) and \\(\\beta_1^*\\) always conditional on \\(\\sigma^*\\), a triplet of values (\\(\\beta_0^*\\), \\(\\beta_1^*\\), \\(\\sigma^*\\)) is one draw of the joint posterior distribution. When we visualize the distribution of the simulated values for one parameter only, ignoring the values for the other, we display the marginal posterior distribution of that parameter. Thus, the distribution of all simulated values for the parameter \\(\\beta_0\\) is a \\(t\\)-distribution even if a normal distribution has been used for simulating the values. The \\(t\\)-distribution is a consequence of using a different \\(\\sigma^2\\)-value for every draw of \\(\\beta_0\\). Using the function sim from the package, we can draw values from the joint posterior distribution of the model parameters and describe the marginal posterior distribution of each model parameter using these simulated values. library(arm) nsim &lt;- 1000 bsim &lt;- sim(mod, n.sim = nsim) The function sim simulates (in our example) 1000 values from the joint posterior distribution of the three model parameters \\(\\beta_0\\) , \\(\\beta_1\\), and \\(\\sigma\\). These simulated values are shown in Figure 11.3. Figure 11.3: Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters. The posterior distribution describes, given the data and the model, which values relative to each other are more likely to correspond to the parameter value we aim at measuring. It expresses the uncertainty of the parameter estimate. It shows what we know about the model parameter after having looked at the data and given the model is realistic. The 2.5% and 97.5% quantiles of the marginal posterior distributions can be used as 95% uncertainty intervals of the model parameters. The function coef extracts the simulated values for the beta coefcients, returning a matrix with nsim rows and the number of columns corresponding to the number of parameters. In our example, the rst column contains the simulated values from the posterior distribution of the intercept and the second column contains values from the posterior distribution of the slope. The 2 in the second argument of the apply-function (see Chapter ??) indicates that the quantile function is applied columnwise. apply(X = coef(bsim), MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975)) %&gt;% round(2) ## (Intercept) x ## 2.5% -2.95 0.44 ## 97.5% 7.17 0.92 We also can calculate an uncertainty interval of the estimated residual standard deviation, \\(\\hat{\\sigma}\\). quantile(bsim@sigma, probs = c(0.025, 0.975)) %&gt;% round(1) ## 2.5% 97.5% ## 4.2 6.3 We can further get a posterior probability for specic hypotheses, such as The slope parameter is larger than 1 or The slope parameter is larger than 0.5. These probabilities are the proportion of simulated values from the posterior distribution that are larger than 1 and 0.5, respectively. sum(coef(bsim)[,2] &gt; 1) / nsim # alternatively: mean(coef(bsim)[,2]&gt;1) ## [1] 0.008 sum(coef(bsim)[,2] &gt; 0.5) / nsim ## [1] 0.936 From this, there is very little evidence in the data that the slope is larger than 1, but we are quite condent that the slope is larger than 0.5 (assuming that our model is realistic). We often want to show the effect of \\(x\\) on \\(y\\) graphically, with information about the uncertainty of the parameter estimates included in the graph. To draw such effect plots, we use the simulated values from the posterior distribution of the model parameters. From the deterministic part of the model, we know the regression line \\(\\mu = \\beta_0 + \\beta_1 x_i\\). The simulation from the joint posterior distribution of \\(\\beta_0\\) and \\(\\beta_1\\) gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We can draw these regression lines in an x-y plot (scatter plot) to show the uncertainty in the regression line estimation (Fig. 11.4, left). Note, that in this case it is not advisable to use ggplot because we draw many lines in one plot, which makes ggplot rather slow. par(mar = c(4, 4, 0, 0)) plot(x, y, pch = 16, las = 1, xlab = &quot;Outcome (y)&quot;) for(i in 1:nsim) { abline(coef(bsim)[i,1], coef(bsim)[i,2], col = rgb(0, 0, 0, 0.05)) } Figure 11.4: Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line. A more convenient way to show uncertainty is to draw the 95% uncertainty interval, CrI, of the regression line. To this end, we rst dene new x-values for which we would like to have the tted values (about 100 points across the range of x will produce smooth-looking lines when connected by line segments). We save these new x-values within the new tibble newdat. Then, we create a new model matrix that contains these new x-values (newmodmat) using the function model.matrix. We then calculate the 1000 tted values for each element of the new x (one value for each of the 1000 simulated regressions, Fig. 11.4), using matrix multiplication (%*%). We save these values in the matrix tmat. Finally, we extract the 2.5% and 97.5% quantiles for each x-value from tmat, and draw the lines for the lower and upper limits of the credible interval (Fig. 11.5). # Calculate 95% credible interval newdat &lt;- tibble(x = seq(10, 30, by = 0.1)) newmodmat &lt;- model.matrix( ~ x, data = newdat) fitmat &lt;- matrix(ncol = nsim, nrow = nrow(newdat)) for(i in 1:nsim) {fitmat[,i] &lt;- newmodmat %*% coef(bsim)[i,]} newdat$CrI_lo &lt;- apply(fitmat, 1, quantile, probs = 0.025) newdat$CrI_up &lt;- apply(fitmat, 1, quantile, probs = 0.975) # Make plot regplot &lt;- ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_smooth(method = lm, se = FALSE) + geom_line(data = newdat, aes(x = x, y = CrI_lo), lty = 3) + geom_line(data = newdat, aes(x = x, y = CrI_up), lty = 3) + labs(x = &quot;Predictor (x)&quot;, y = &quot;Outcome (y)&quot;) regplot Figure 11.5: Regression with 95% credible interval of the posterior distribution of the tted values. The interpretation of the 95% uncertainty interval is straightforward: We are 95% sure that the true regression line is within the credible interval (given the data and the model). As with all statistical results, this interpretation is only valid in the model world (if the world would look like the model). The larger the sample size, the narrower the interval, because each additional data point increases information about the true regression line. The uncertainty interval measures statistical uncertainty of the regression line, but it does not describe how new observations would scatter around the regression line. If we want to describe where future observations will be, we have to report the posterior predictive distribution. We can get a sample of random draws from the posterior predictive distribution \\(\\hat{y}|\\boldsymbol{\\beta},\\sigma^2,\\boldsymbol{X}\\sim normal( \\boldsymbol{X \\beta, \\sigma^2})\\) using the simulated joint posterior distributions of the model parameters, thus taking the uncertainty of the parameter estimates into account. We draw a new \\(\\hat{y}\\)-value from \\(normal( \\boldsymbol{X \\beta, \\sigma^2})\\) for each simulated set of model parameters. Then, we can visualize the 2.5% and 97.5% quantiles of this distribution for each new x-value. # increase number of simulation to produce smooth lines of the posterior # predictive distribution set.seed(34) nsim &lt;- 50000 bsim &lt;- sim(mod, n.sim=nsim) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- newmodmat%*%coef(bsim)[i,] # prepare matrix for simulated new data newy &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) # for each simulated tted value, simulate one new y-value for(i in 1:nsim) { newy[,i] &lt;- rnorm(nrow(newdat), mean = fitmat[,i], sd = bsim@sigma[i]) } # Calculate 2.5% and 97.5% quantiles newdat$pred_lo &lt;- apply(newy, 1, quantile, probs = 0.025) newdat$pred_up &lt;- apply(newy, 1, quantile, probs = 0.975) # Add the posterior predictive distribution to plot regplot + geom_line(data = newdat, aes(x = x, y = pred_lo), lty = 2) + geom_line(data = newdat, aes(x = x, y = pred_up), lty = 2) Figure 11.6: Regression line with 95% uncertainty interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines. Of future observations, 95% are expected to be within the interval dened by the broken lines in Fig. 11.6. Increasing sample size will not give a narrower predictive distribution because the predictive distribution primarily depends on the residual variance \\(\\sigma^2\\) which is a property of the data that is independent of sample size. The way we produced Fig. 11.6 is somewhat tedious compared to how easy we could have obtained the same gure using frequentist methods: predict(mod, newdata = newdat, interval = \"prediction\") would have produced the y-values for the lower and upper lines in Fig. 11.6 in one R-code line. However, once we have a simulated sample of the posterior predictive distribution, we have much more information than is contained in the frequentist prediction interval. For example, we could give an estimate for the proportion of observations greater than 20, given \\(x = 25\\). sum(newy[newdat$x == 25, ] &gt; 20) / nsim ## [1] 0.44504 Thus, we expect 44% of future observations with \\(x = 25\\) to be higher than 20. We can extract similar information for any relevant threshold value. Another reason to learn the more complicated R code we presented here, compared to the frequentist methods, is that, for more complicated models such as mixed models, the frequentist methods to obtain condence intervals of tted values are much more complicated than the Bayesian method just presented. The latter can be used with only slight adaptations for mixed models and also for generalized linear mixed models. 11.1.4 Interpretation of the R summary output The solution for \\(\\boldsymbol{\\beta}\\) is the Equation (11.3). Most statistical software, including R, return an estimated frequentist standard error for each \\(\\beta_k\\). We extract these standard errors together with the estimates for the model parameters using the summary function. summary(mod) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5777 -3.6280 -0.0532 3.9873 12.1374 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0050 2.5349 0.791 0.433 ## x 0.6880 0.1186 5.800 0.000000507 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.049 on 48 degrees of freedom ## Multiple R-squared: 0.412, Adjusted R-squared: 0.3998 ## F-statistic: 33.63 on 1 and 48 DF, p-value: 0.0000005067 The summary output rst gives a rough summary of the residual distribution. However, we will do more rigorous residual analyses in Chapter 12. The estimates of the model coefcients follow. The column Estimate contains the estimates for the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\) . The column Std. Error contains the estimated (frequentist) standard errors of the estimates. The last two columns contain the t-value and the p-value of the classical t-test for the null hypothesis that the coefcient equals zero. The last part of the summary output gives the parameter \\(\\sigma\\) of the model, named residual standard error and the residual degrees of freedom. We think the name residual standard error for sigma is confusing, because \\(\\sigma\\) is not a measurement of uncertainty of a parameter estimate like the standard errors of the model coefcients are. \\(\\sigma\\) is a model parameter that describes how the observations scatter around the tted values, that is, it is a standard deviation. It is independent of sample size, whereas the standard errors of the estimates for the model parameters will decrease with increasing sample size. Such a standard error of the estimate of \\(\\sigma\\), however, is not given in the summary output. Note that, by using Bayesian methods, we could easily obtain the standard error of the estimated \\(\\sigma\\) by calculating the standard deviation of the posterior distribution of \\(\\sigma\\). The \\(R^2\\) and the adjusted \\(R^2\\) measure the proportion of variance in the outcome variable \\(y\\) that is explained by the predictors in the model. \\(R^2\\) is calculated from the sum of squared residuals, \\(SSR = \\sum_{i=1}^{n}(y_i - \\hat{y})\\), and the total sum of squares, \\(SST = \\sum_{i=1}^{n}(y_i - \\bar{y})\\), where \\(\\bar{y})\\) is the mean of \\(y\\). \\(SST\\) is a measure of total variance in \\(y\\) and \\(SSR\\) is a measure of variance that cannot be explained by the model, thus \\(R^2 = 1- \\frac{SSR}{SST}\\) is a measure of variance that can be explained by the model. If \\(SSR\\) is close to \\(SST\\), \\(R^2\\) is close to zero and the model cannot explain a lot of variance. The smaller \\(SSR\\), the closer \\(R^2\\) is to 1. This version of \\(R2\\) approximates 1 if the number of model parameters approximates sample size even if none of the predictor variables correlates with the outcome. It is exactly 1 when the number of model parameters equals sample size, because \\(n\\) measurements can be exactly described by \\(n\\) parameters. The adjusted \\(R^2\\), \\(R^2 = \\frac{var(y)-\\hat\\sigma^2}{var(y)}\\) takes sample size \\(n\\) and the number of model parameters \\(k\\) into account (see explanation to variance in chapter 2). Therefore, the adjusted \\(R^2\\) is recommended as a measurement of the proportion of explained variance. 11.2 Linear model with one categorical predictor (one-way ANOVA) The aim of analysis of variance (ANOVA) is to compare means of an outcome variable \\(y\\) between different groups. To do so in the frequentists framework, variances between and within the groups are compared using F-tests (hence the name analysis of variance). When doing an ANOVA in a Bayesian way, inference is based on the posterior distributions of the group means and the differences between the group means. One-way ANOVA means that we only have one predictor variable, specifically a categorical predictor variable (in R defined as a factor). We illustrate the one-way ANOVA based on an example of simulated data (Fig. 11.7). We have measured weights of 30 virtual individuals for each of 3 groups. Possible research questions could be: How big are the differences between the group means? Are individuals from group 2 heavier than the ones from group 1? Which group mean is higher than 7.5 g? # settings for the simulation set.seed(626436) b0 &lt;- 12 # mean of group 1 (reference group) sigma &lt;- 2 # residual standard deviation b1 &lt;- 3 # difference between group 1 and group 2 b2 &lt;- -5 # difference between group 1 and group 3 n &lt;- 90 # sample size # generate data group &lt;- factor(rep(c(&quot;group 1&quot;,&quot;group 2&quot;, &quot;group 3&quot;), each=30)) simresid &lt;- rnorm(n, mean=0, sd=sigma) # simulate residuals y &lt;- b0 + as.numeric(group==&quot;group 2&quot;) * b1 + as.numeric(group==&quot;group 3&quot;) * b2 + simresid dat &lt;- tibble(y, group) # make figure dat %&gt;% ggplot(aes(x = group, y = y)) + geom_boxplot(fill = &quot;orange&quot;) + labs(y = &quot;Weight (g)&quot;, x = &quot;&quot;) + ylim(0, NA) Figure 11.7: Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box. An ANOVA is a linear regression with a categorical predictor variable instead of a continuous one. The categorical predictor variable with \\(k\\) levels is (as a default in R) transformed to \\(k-1\\) indicator variables. An indicator variable is a binary variable containing 0 and 1 where 1 indicates a specic level (a category of the predictor variable). Often, one indicator variable is constructed for every level except for the reference level. In our example, the categorical variable is group with the three levels group 1, group 2, and group 3 (\\(k = 3\\)). Group 1 is taken as the reference level (default in R is the first in the alphabeth), and for each of the other two groups an indicator variable is constructed, \\(I(group_i = 2)\\) and \\(I(group_i = 3)\\). The function \\(I()\\) gives out 1, if the expression is true and 0 otherwise. We can write the model as a formula: \\[\\begin{align} \\mu_i &amp;=\\beta_0 + \\beta_1 I(group_i=2) + \\beta_1 I(group_i=3) \\\\ y_i &amp;\\sim normal(\\mu_i, \\sigma^2) \\tag{11.5} \\end{align}\\] where \\(y_i\\) is the \\(i\\)-th observation (weight measurement for individual \\(i\\) in our example), and \\(\\beta_{0,1,2}\\) are the model coefcients. The residual variance is \\(\\sigma^2\\). The model coefcients \\(\\beta_{0,1,2}\\) constitute the deterministic part of the model. From the model formula it follows that the group means, \\(m_g\\), are: \\[\\begin{align} m_1 &amp;=\\beta_0 \\\\ m_2 &amp;=\\beta_0 + \\beta_1 \\\\ m_3 &amp;=\\beta_0 + \\beta_2 \\\\ \\tag{11.6} \\end{align}\\] There are other possibilities to describe three group means with three parameters, for example: \\[\\begin{align} m_1 &amp;=\\beta_1 \\\\ m_2 &amp;=\\beta_2 \\\\ m_3 &amp;=\\beta_3 \\\\ \\tag{11.7} \\end{align}\\] In this case, the model formula would be: \\[\\begin{align} \\mu_i &amp;= \\beta_1 I(group_i=1) + \\beta_2 I(group_i=2) + \\beta_3 I(group_i=3) \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{11.8} \\end{align}\\] The way the group means are calculated within a model is called the parameterization of the model. Different statistical software use different parameterizations. The parameterization used by R by default is the one shown in Equation (11.5). R automatically takes the rst level as the reference (the rst level is the rst one alphabetically unless the user denes a different order for the levels). The mean of the rst group (i.e., of the rst factor level) is the intercept, \\(b_0\\) , of the model. The mean of another factor level is obtained by adding, to the intercept, the estimate of the corresponding parameter (which is the difference from the reference group mean). The parameterization of the model is dened by the model matrix. In the case of a one-way ANOVA, there are as many columns in the model matrix as there are factor levels (i.e., groups); thus there are k factor levels and k model coefcients. Recall from Equation (11.3) that for each observation, the entry in the \\(j\\)-th column of the model matrix is multiplied by the \\(j\\)-th element of the model coefcients and the \\(k\\) products are summed to obtain the tted values. For a data set with \\(n = 5\\) observations of which the rst two are from group 1, the third from group 2, and the last two from group 3, the model matrix used for the parameterization described in Equation (11.6) and defined in R by the formula ~ group is \\[\\begin{align} \\boldsymbol{X}= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{align}\\] If parameterization of Equation (11.7) (corresponding R formula: ~ group - 1) were used, \\[\\begin{align} \\boldsymbol{X}= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{align}\\] To obtain the parameter estimates for model parameterized according to Equation (11.6) we t the model in R: # fit the model mod &lt;- lm(y~group, data=dat) # parameter estimates mod ## ## Call: ## lm(formula = y ~ group, data = dat) ## ## Coefficients: ## (Intercept) groupgroup 2 groupgroup 3 ## 12.367 2.215 -5.430 summary(mod)$sigma ## [1] 1.684949 The Intercept is \\(\\beta_0\\). The other coefcients are named with the factor name (group) and the factor level (either group 2 or group 3). These are \\(\\beta_1\\) and \\(\\beta_2\\) , respectively. Before drawing conclusions from an R output we need to examine whether the model assumptions are met, that is, we need to do a residual analysis as described in Chapter 12. Different questions can be answered using the above ANOVA: What are the group means? What is the difference in the means between group 1 and group 2? What is the difference between the means of the heaviest and lightest group? In a Bayesian framework we can directly assess how strongly the data support the hypothesis that the mean of the group 2 is larger than the mean of group 1. We rst simulate from the posterior distribution of the model parameters. library(arm) nsim &lt;- 1000 bsim &lt;- sim(mod, n.sim=nsim) Then we obtain the posterior distributions for the group means according to the parameterization of the model formula (Equation (11.6)). m.g1 &lt;- coef(bsim)[,1] m.g2 &lt;- coef(bsim)[,1] + coef(bsim)[,2] m.g3 &lt;- coef(bsim)[,1] + coef(bsim)[,3] The histograms of the simulated values from the posterior distributions of the three means are given in Fig. 11.8. The three means are well separated and, based on our data, we are condent that the group means differ. From these simulated posterior distributions we obtain the means and use the 2.5% and 97.5% quantiles as limits of the 95% uncertainty intervals (Fig. 11.8, right). # save simulated values from posterior distribution in tibble post &lt;- tibble(`group 1` = m.g1, `group 2` = m.g2, `group 3` = m.g3) %&gt;% gather(&quot;groups&quot;, &quot;Group means&quot;) # histograms per group leftplot &lt;- ggplot(post, aes(x = `Group means`, fill = groups)) + geom_histogram(aes(y=..density..), binwidth = 0.5, col = &quot;black&quot;) + labs(y = &quot;Density&quot;) + theme(legend.position = &quot;top&quot;, legend.title = element_blank()) # plot mean and 95%-CrI rightplot &lt;- post %&gt;% group_by(groups) %&gt;% dplyr::summarise( mean = mean(`Group means`), CrI_lo = quantile(`Group means`, probs = 0.025), CrI_up = quantile(`Group means`, probs = 0.975)) %&gt;% ggplot(aes(x = groups, y = mean)) + geom_point() + geom_errorbar(aes(ymin = CrI_lo, ymax = CrI_up), width = 0.1) + ylim(0, NA) + labs(y = &quot;Weight (g)&quot;, x =&quot;&quot;) multiplot(leftplot, rightplot, cols = 2) Figure 11.8: Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% uncertainty intervals obtained from the simulated distributions (right). To obtain the posterior distribution of the difference between the means of group 1 and group 2, we simply calculate this difference for each draw from the joint posterior distribution of the group means. d.g1.2 &lt;- m.g1 - m.g2 mean(d.g1.2) ## [1] -2.209551 quantile(d.g1.2, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## -3.128721 -1.342693 The estimated difference is -2.2095511. In the small model world, we are 95% sure that the difference between the means of group 1 and 2 is between -3.1287208 and -1.3426929. How strongly do the data support the hypothesis that the mean of group 2 is larger than the mean of group 1? To answer this question we calculate the proportion of the draws from the joint posterior distribution for which the mean of group 2 is larger than the mean of group 1. sum(m.g2 &gt; m.g1) / nsim ## [1] 1 This means that in all of the 1000 simulations from the joint posterior distribution, the mean of group 2 was larger than the mean of group 1. Therefore, there is a very high probability (i.e., it is close to 1; because probabilities are never exactly 1, we write &gt;0.999) that the mean of group 2 is larger than the mean of group 1. 11.3 Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression Up to now, we introduced normal linear models with one predictor only. We can add more predictors to the model and these can be numerical or categorical ones. Traditionally, models with 2 or 3 categorical predictors are called two-way or three-way ANOVA, respectively. Models with a mixture of categorical and numerical predictors are called ANCOVA. And, models containing only numerical predictors are called multiple regressions. Nowadays, we only use the term normal linear model as an umbrella term for all these types of models. While it is easy to add additional predictors in the R formula of the model, it becomes more difficult to interpret the coefficients of such multi-dimensional models. Two important topics arise with multi-dimensional models, interactions and partial effects. We dedicate partial effects the full next chapter and introduce interactions in this chapter using two examples. The first, is a model including two categorical predictors and the second is a model with one categorical and one numeric predictor. 11.3.1 Linear model with two categorical predictors (two-way ANOVA) In the first example, we ask how large are the differences in wing length between age and sex classes of the Coal tit Periparus ater. Wing lengths were measured on 19 coal tit museum skins with known sex and age class. data(periparusater) dat &lt;- tibble(periparusater) # give the data a short handy name dat$age &lt;- recode_factor(dat$age, &quot;4&quot;=&quot;adult&quot;, &quot;3&quot;=&quot;juvenile&quot;) # replace EURING code dat$sex &lt;- recode_factor(dat$sex, &quot;2&quot;=&quot;female&quot;, &quot;1&quot;=&quot;male&quot;) # replace EURING code To describe differences in wing length between the age classes or between the sexes a normal linear model with two categorical predictors is fitted to the data. The two predictors are specified on the right side of the model formula separated by the + sign, which means that the model is an additive combination of the two effects (as opposed to an interaction, see following). mod &lt;- lm(wing ~ sex + age, data=dat) After having seen that the residual distribution does not appear to violate the model assumptions (as assessed with diagnostic residual plots, see Chapter 12), we can draw inferences. We first have a look at the model parameter estimates: mod ## ## Call: ## lm(formula = wing ~ sex + age, data = dat) ## ## Coefficients: ## (Intercept) sexmale agejuvenile ## 61.3784 3.3423 -0.8829 summary(mod)$sigma ## [1] 2.134682 R has taken the first level of the factors age and sex (as defined in the data.frame dat) as the reference levels. The intercept is the expected wing length for individuals having the reference level in age and sex, thus adult female. The other two parameters provide estimates of what is to be added to the intercept to get the expected wing length for the other levels. The parameter sexmale is the average difference between females and males. We can conclude that in males have in average a 3.3 mm longer wing than females. Similarly, the parameter agejuvenile measures the differences between the age classes and we can conclude that, in average, juveniles have a 0.9 shorter wing than adults. When we insert the parameter estimates into the model formula, we get the receipt to calculate expected values for each age and sex combination: \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}I(sex=male) + \\hat{\\beta_2}I(age=juvenile)\\) which yields \\(\\hat{y_i}\\) = 61.4 \\(+\\) 3.3 \\(I(sex=male) +\\) -0.9 \\(I(age=juvenile)\\). Alternatively, we could use matrix notation. We construct a new data set that contains one virtual individual for each age and sex class. newdat &lt;- tibble(expand.grid(sex=factor(levels(dat$sex)), age=factor(levels(dat$age)))) # expand.grid creates a data frame with all combination of values given newdat ## # A tibble: 4 × 2 ## sex age ## &lt;fct&gt; &lt;fct&gt; ## 1 female adult ## 2 male adult ## 3 female juvenile ## 4 male juvenile newdat$fit &lt;- predict(mod, newdata=newdat) # fast way of getting fitted values # or Xmat &lt;- model.matrix(~sex+age, data=newdat) # creates a model matrix newdat$fit &lt;- Xmat %*% coef(mod) For this new data set the model matrix contains four rows (one for each combination of age class and sex) and three columns. The first column contains only ones because the values of this column are multiplied by the intercept (\\(\\beta_0\\)) in the matrix multiplication. The second column contains an indicator variable for males (so only the rows corresponding to males contain a one) and the third column has ones for juveniles. \\[\\begin{align} \\hat{y} = \\boldsymbol{X \\hat{\\beta}} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ \\end{pmatrix} \\times \\begin{pmatrix} 61.4 \\\\ 3.3 \\\\ -0.9 \\end{pmatrix} = \\begin{pmatrix} 61.4 \\\\ 64.7 \\\\ 60.5 \\\\ 63.8 \\end{pmatrix} = \\boldsymbol{\\mu} \\tag{11.3} \\end{align}\\] The result of the matrix multiplication is a vector containing the expected wing length for adult and juvenile females and adult and juvenile males. When creating the model matrix with model.matrix care has to be taken that the columns in the model matrix match the parameters in the vector of model coefficients. To achieve that, it is required that the model formula is identical to the model formula of the model (same order of terms!), and that the factors in newdat are identical in their levels and their order as in the data the model was fitted to. To describe the uncertainty of the fitted values, we use 2000 sets of parameter values of the joint posterior distribution to obtain 2000 values for each of the four fitted values. These are stored in the object fitmat. In the end, we extract for every fitted value, i.e., for every row in fitmat, the 2.5% and 97.5% quantiles as the lower and upper limits of the 95% uncertainty interval. nsim &lt;- 2000 bsim &lt;- sim(mod, n.sim=nsim) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- Xmat %*% coef(bsim)[i,] newdat$lwr &lt;- apply(fitmat, 1, quantile, probs=0.025) newdat$upr &lt;- apply(fitmat, 1, quantile, probs=0.975) dat$sexage &lt;- factor(paste(dat$sex, dat$age)) newdat$sexage &lt;- factor(paste(newdat$sex, newdat$age)) dat$pch &lt;- 21 dat$pch[dat$sex==&quot;male&quot;] &lt;- 22 dat$col=&quot;blue&quot; dat$col[dat$age==&quot;adult&quot;] &lt;- &quot;orange&quot; par(mar=c(4,4,0.5,0.5)) plot(wing~jitter(as.numeric(sexage), amount=0.05), data=dat, las=1, ylab=&quot;Wing length (mm)&quot;, xlab=&quot;Sex and age&quot;, xaxt=&quot;n&quot;, pch=dat$pch, bg=dat$col, cex.lab=1.2, cex=1, cex.axis=1, xlim=c(0.5, 4.5)) axis(1, at=c(1:4), labels=levels(dat$sexage), cex.axis=1) segments(as.numeric(newdat$sexage), newdat$lwr, as.numeric(newdat$sexage), newdat$upr, lwd=2, lend=&quot;butt&quot;) points(as.numeric(newdat$sexage), newdat$fit, pch=17) Figure 11.9: Wing length measurements on 19 museumm skins of coal tits per age class and sex. Fitted values are from the additive model (black triangles) and from the model including an interaction (black dots). Vertical bars = 95% uncertainty intervals. We can see that the fitted values are not equal to the arithmetic means of the groups; this is especially clear for juvenile males. The fitted values are constrained because only three parameters were used to estimate four means. In other words, this model assumes that the age difference is equal in both sexes and, vice versa, that the difference between the sexes does not change with age. If the effect of sex changes with age, we would include an interaction between sex and age in the model. Including an interaction adds a fourth parameter enabling us to estimate the group means exactly. In R, an interaction is indicated with the : sign. mod2 &lt;- lm(wing ~ sex + age + sex:age, data=dat) # alternative formulations of the same model: # mod2 &lt;- lm(wing ~ sex * age, data=dat) # mod2 &lt;- lm(wing ~ (sex + age)^2, data=dat) The formula for this model is \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}I(sex=male) + \\hat{\\beta_2}I(age=juvenile) + \\hat{\\beta_3}I(age=juvenile)I(sex=male)\\). From this formula we get the following expected values for the sexes and age classes: for adult females: \\(\\hat{y} = \\beta_0\\) for adult males: \\(\\hat{y} = \\beta_0 + \\beta_1\\) for juveniles females: \\(\\hat{y} = \\beta_0 + \\beta_2\\) for juveniles males: \\(\\hat{y} = \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\) The interaction parameter measures how much different between age classes is the difference between the sexes. To obtain the fitted values the R-code above can be recycled with two adaptations. First, the model name needs to be changed to mod2. Second, importantly, the model matrix needs to be adapted to the new model formula. newdat$fit2 &lt;- predict(mod2, newdata=newdat) bsim &lt;- sim(mod2, n.sim=nsim) Xmat &lt;- model.matrix(~ sex + age + sex:age, data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- Xmat %*% coef(bsim)[i,] newdat$lwr2 &lt;- apply(fitmat, 1, quantile, probs=0.025) newdat$upr2 &lt;- apply(fitmat, 1, quantile, probs=0.975) print(newdat[,c(1:5,7:9)], digits=3) ## # A tibble: 4 × 8 ## sex age fit[,1] lwr upr fit2 lwr2 upr2 ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female adult 61.4 59.3 63.3 61.1 58.8 63.5 ## 2 male adult 64.7 63.3 66.2 64.8 63.3 66.4 ## 3 female juvenile 60.5 58.4 62.6 60.8 58.2 63.4 ## 4 male juvenile 63.8 61.7 66.0 63.5 60.7 66.2 These fitted values are now exactly equal to the arithmetic means of each groups. tapply(dat$wing, list(dat$age, dat$sex), mean) # arithmetic mean per group ## female male ## adult 61.12500 64.83333 ## juvenile 60.83333 63.50000 We can also see that the uncertainty of the fitted values is larger for the model with an interaction than for the additive model. This is because, in the model including the interaction, an additional parameter has to be estimated based on the same amount of data. Therefore, the information available per parameter is smaller than in the additive model. In the additive model, some information is pooled between the groups by making the assumption that the difference between the sexes does not depend on age. The degree to which a difference in wing length is important depends on the context of the study. Here, for example, we could consider effects of wing length on flight energetics and maneuverability or methodological aspects like measurement error. Mean between-observer difference in wing length measurement is around 0.3 mm (Jenni and Winkler 1989). Therefore, we may consider that the interaction is important because its point estimate is larger than 0.3 mm. mod2 ## ## Call: ## lm(formula = wing ~ sex + age + sex:age, data = dat) ## ## Coefficients: ## (Intercept) sexmale agejuvenile ## 61.1250 3.7083 -0.2917 ## sexmale:agejuvenile ## -1.0417 summary(mod2)$sigma ## [1] 2.18867 Further, we think a difference of 1 mm in wing length may be relevant compared to the among-individual variation of which the standard deviation is around 2 mm. Therefore, we report the parameter estimates of the model including the interaction together with their uncertainty intervals. Table 11.1: Parameter estimates of the model for wing length of Coal tits with 95% uncertainty interval. Parameter Estimate lwr upr (Intercept) 61.12 58.85 63.53 sexmale 3.71 0.93 6.59 agejuvenile -0.29 -3.93 3.36 sexmale:agejuvenile -1.04 -5.96 3.90 From these parameters we obtain the estimated differences in wing length between the sexes for adults of 3.7mm and the posterior probability of the hypotheses that males have an average wing length that is at least 1mm larger compared to females is mean(bsim@coef[,2]&gt;1) which is 0.97. Thus, there is some evidence that adult Coal tit males have substantially larger wings than adult females in these data. However, we do not draw further conclusions on other differences from these data because statistical uncertainty is large due to the low sample size. 11.3.2 A linear model with a categorical and a numeric predictor (ANCOVA) An analysis of covariance, ANCOVA, is a normal linear model that contains at least one factor and one continuous variable as predictor variables. The continuous variable is also called a covariate, hence the name analysis of covariance. An ANCOVA can be used, for example, when we are interested in how the biomass of grass depends on the distance from the surface of the soil to the ground water in two different species (Alopecurus pratensis, Dactylis glomerata). The two species were grown by Ellenberg (1953) in tanks that showed a gradient in distance from the soil surface to the ground water. The distance from the soil surface to the ground water is used as a covariate (water). We further assume that the species react differently to the water conditions. Therefore, we include an interaction between species and water. The model formula is then \\(\\hat{y_i} = \\beta_0 + \\beta_1I(species=Dg) + \\beta_2water_i + \\beta_3I(species=Dg)water_i\\) \\(y_i \\sim normal(\\hat{y_i}, \\sigma^2)\\) To fit the model, it is important to first check whether the factor is indeed defined as a factor and the continuous variable contains numbers (i.e., numeric or integer values) in the data frame. data(ellenberg) index &lt;- is.element(ellenberg$Species, c(&quot;Ap&quot;, &quot;Dg&quot;)) &amp; complete.cases(ellenberg$Yi.g) dat &lt;- ellenberg[index,c(&quot;Water&quot;, &quot;Species&quot;, &quot;Yi.g&quot;)] # select two species dat &lt;- droplevels(dat) str(dat) ## &#39;data.frame&#39;: 84 obs. of 3 variables: ## $ Water : int 5 20 35 50 65 80 95 110 125 140 ... ## $ Species: Factor w/ 2 levels &quot;Ap&quot;,&quot;Dg&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Yi.g : num 34.8 28 44.5 24.8 37.5 ... Species is a factor with two levels and Water is an integer variable, so we are fine and we can fit the model mod &lt;- lm(log(Yi.g) ~ Species + Water + Species:Water, data=dat) # plot(mod) # 4 standard residual plots We log-transform the biomass to make the residuals closer to normally distributed. So, the normal distribution assumption is met well. However, a slight banana shaped relationship exists between the residuals and the fitted values indicating a slight non-linear relationship between biomass and water. Further, residuals showed substantial autocorrelation because the grass biomass was measured in different tanks. Measurements from the same tank were more similar than measurements from different tanks after correcting for the distance to water. Thus, the analysis we have done here suffers from pseudoreplication. We will re-analyze the example data in a more appropriate way in Chapter 13. Lets have a look at the model matrix (first and last six rows only). head(model.matrix(mod)) # print the first 6 rows of the matrix ## (Intercept) SpeciesDg Water SpeciesDg:Water ## 24 1 0 5 0 ## 25 1 0 20 0 ## 26 1 0 35 0 ## 27 1 0 50 0 ## 28 1 0 65 0 ## 29 1 0 80 0 tail(model.matrix(mod)) # print the last 6 rows of the matrix ## (Intercept) SpeciesDg Water SpeciesDg:Water ## 193 1 1 65 65 ## 194 1 1 80 80 ## 195 1 1 95 95 ## 196 1 1 110 110 ## 197 1 1 125 125 ## 198 1 1 140 140 The first column of the model matrix contains only 1s. These are multiplied by the intercept in the matrix multiplication that yields the fitted values. The second column contains the indicator variable for species Dactylis glomerata (Dg). Species Alopecurus pratensis (Ap) is the reference level. The third column contains the values for the covariate. The last column contains the product of the indicator for species Dg and water. This column specifies the interaction between species and water. The parameters are the intercept, the difference between the species, a slope for water and the interaction parameter. mod ## ## Call: ## lm(formula = log(Yi.g) ~ Species + Water + Species:Water, data = dat) ## ## Coefficients: ## (Intercept) SpeciesDg Water SpeciesDg:Water ## 4.33041 -0.23700 -0.01791 0.01894 summary(mod)$sigma ## [1] 0.9001547 These four parameters define two regression lines, one for each species (Figure 11.10 Left). For Ap, it is \\(\\hat{y_i} = \\beta_0 + \\beta_2water_i\\), and for Dg it is \\(\\hat{y_i} = (\\beta_0 + \\beta_1) + (\\beta_2 + \\beta_3)water_i\\). Thus, \\(\\beta_1\\) is the difference in the intercept between the species and \\(\\beta_3\\) is the difference in the slope. Figure 11.10: Aboveground biomass (g, log-transformed) in relation to distance to ground water and species (two grass species). Fitted values from a model including an interaction species x water (left) and a model without interaction (right) are added. The dotted line indicates water=0. As a consequence of including an interaction in the model, the interpretation of the main effects become difficult. From the above model output, we read that the intercept of the species Dg is lower than the intercept of the species Ap. However, from a graphical inspection of the data, we would expect that the average biomass of species Dg is higher than the one of species Ap. The estimated main effect of species is counter-intuitive because it is measured where water is zero (i.e, it is the difference in the intercepts and not between the mean biomasses of the species). Therefore, the main effect of species in the above model does not have a biologically meaningful interpretation. We have two possibilities to get a meaningful species effect. First, we could delete the interaction from the model (Figure 11.10 Right). Then the difference in the intercept reflects an average difference between the species. However, the fit for such an additive model is much worth compared to the model with interaction, and an average difference between the species may not make much sense because this difference so much depends on water. Therefore, we prefer to use a model including the interaction and may opt for th second possibility. Second, we could move the location where water equals 0 to the center of the data by transforming, specifically centering, the variable water: \\(water.c = water - mean(water)\\). When the predictor variable (water) is centered, then the intercept corresponds to the difference in fitted values measured in the center of the data. For drawing biological conclusions from these data, we refer to Chapter 13, where we use a more appropriate model. 11.4 Partial coefficients and some comments on collinearity Many biologists think that it is forbidden to include correlated predictor variables in a model. They use variance inflating factors (VIF) to omit some of the variables. However, omitting important variables from the model just because a correlation coefficient exceeds a threshold value can have undesirable effects. Here, we explain why and we present the usefulness and limits of partial coefficients (also called partial correlation or partial effects). We start with an example illustrating the usefulness of partial coefficients and then, give some guidelines on how to deal with collinearity. As an example, we look at hatching dates of Snowfinches and how these dates relate to the date when snow melt started (first date in the season when a minimum of 5% ground is snow free). A thorough analyses of the data is presented by Schano et al. (2021). An important question is how well can Snowfinches adjust their hatching dates to the snow conditions. For Snowfinches, it is important to raise their nestlings during snow melt. Their nestlings grow faster when they are reared during the snow melt compared to after snow has completely melted, because their parents find nutrient rich insect larvae in the edges of melting snow patches. load(&quot;RData/snowfinch_hatching_date.rda&quot;) # Pearson&#39;s correlation coefficient cor(datsf$elevation, datsf$meltstart, use = &quot;pairwise.complete&quot;) ## [1] 0.3274635 mod &lt;- lm(meltstart~elevation, data=datsf) 100*coef(mod)[2] # change in meltstart with 100m change in elevation ## elevation ## 2.97768 Hatching dates of Snowfinch broods were inferred from citizen science data from the Alps, where snow melt starts later at higher elevations compared to lower elevations. Thus, the start of snow melt is correlated with elevation (Pearsons correlation coefficient 0.33). In average, snow starts melting 3 days later with every 100m increase in elevation. mod1 &lt;- lm(hatchday.mean~meltstart, data=datsf) mod1 ## ## Call: ## lm(formula = hatchday.mean ~ meltstart, data = datsf) ## ## Coefficients: ## (Intercept) meltstart ## 167.99457 0.06325 From a a normal linear regression of hatching date on the snow melt date, we obtain an estimate of 0.06 days delay in hatching date with one day later snow melt. This effect sizes describes the relationship in the data that were collected along an elevational gradient. Along the elevational gradient there are many factors that change such as average temperature, air pressure or sun radiation. All these factors may have an influence on the birds decision to start breeding. Consequentily, from the raw correlation between hatching dates and start of snow melt we cannot conclude how Snowfinches react to changes in the start of snow melt because the correlation seen in the data may be caused by other factors changing with elevation (such a correlation is called pseudocorrelation). However, we are interested in the correlation between hatching date and date of snow melt independent of other factors changing with elevation. In other words, we would like to measure how much in average hatching date delays when snow melt starts one day later while all other factors are kept constant. This is called the partial effect of snow melt date. Therefore, we include elevation as a covariate in the model. library(arm) mod &lt;- lm(hatchday.mean~elevation + meltstart, data=datsf) mod ## ## Call: ## lm(formula = hatchday.mean ~ elevation + meltstart, data = datsf) ## ## Coefficients: ## (Intercept) elevation meltstart ## 154.383936 0.007079 0.037757 From this model, we obtain an estimate of 0.04 days delay in hatching date with one day later snow melt at a given elevation. That gives a difference in hatching date between early and late years (around one month difference in snow melt date) at a given elevation of 1.13 days (Figure 11.11). We further get an estimate of 0.71 days later hatching date for each 100m shift in elevation. Thus, a 18.75 days later snow melt corresponds to a similar delay in average hatching date when elevation increases by 100m. When we estimate the coefficient within a constant elevation (coloured regression lines in Figure 11.11), it is lower than the raw correlation and closer to a causal relationship, because it is corrected for elevation. However, in observational studies, we never can be sure whether the partial coefficients can be interpreted as a causal relationship unless we include all factors that influence hatching date. Nevertheless, partial effects give much more insight into a system compared to univariate analyses because we can separated effects of simultaneously acting variables (that we have measured). The result indicates that Snowfinches may not react very sensibly to varying timing of snow melt, whereas at higher elevations they clearly breed later compared to lower elevations. Figure 11.11: Illustration of the partial coefficient of snow melt date in a model of hatching date. Panel A shows the entire raw data together with the regression lines drawn for three different elevations. The regression lines span the range of snow melt dates occurring at the respective elevation (shown in panel C). Panel B is the same as panel A, but zoomed in to the better see the regression lines and with an additional regression line (in black) from the model that does not take elevation into account. We have seen that it can be very useful to include more than one predictor variable in a model even if they are correlated with each other. In fact, there is nothing wrong with that. However, correlated predictors (collinearity) make things more complicated. For example, partial regression lines should not be drawn across the whole range of values of a variable, to avoid extrapolating out of data. At 2800 m asl snow melt never starts in the beginning of March. Therefore, the blue regression line would not make sense for snow melt dates in March. Further, sometimes correlations among predictors indicate that these predictors measure the same underlying aspect and we are actually interested in the effect of this underlying aspect on our response. For example, we could include also the date of the end of snow melt. Both variables, the start and the end of the snow melt measure the timing of snow melt. Including both as predictor in the model would result in partial coefficients that measure how much hatching date changes when the snow melt starts one day later, while the end date is constant. That interpretation is a mixture of the effect of timing and duration rather than of snow melt timing alone. Similarly, the coefficient of the end of snow melt measures a mixture of duration and timing. Thus, if we include two variables that are correlated because they measure the same aspect (just a little bit differently), we get coefficients that are hard to interpret and may not measure what we actually are interested in. In such a cases, we get easier to interpret model coefficients, if we include just one variable of each aspect that we are interested in, e.g. we could include one timing variable (e.g. start of snow melt) and the duration of snow melt that may or may not be correlated with the start of snow melt. To summarize, the decision of what to do with correlated predictors primarily relies on the question we are interested in, i.e., what exactly should the partial coefficients be an estimate for. A further drawback of collinearity is that model fitting can become difficult. When strong correlations are present, model fitting algorithms may fail. If they do not fail, the statistical uncertainty of the estimates often becomes large. This is because the partial coefficient of one variable needs to be estimated for constant values of the other predictors in the model which means that a reduced range of values is available as illustrated in Figure 11.11 C. However, if uncertainty intervals (confidence, credible or compatibility intervals) are reported alongside the estimates, then using correlated predictors in the same model is absolutely fine, if the fitting algorithm was successful. The correlations per se can be interesting. Further readings on how to visualize and analyse data with complex correlation structures: principal component analysis (Manly 1994) path analyses, e.g. Shipley (2009) structural equation models (Hoyle 2012) 11.5 Ordered Factors and Contrasts In this chapter, we have seen that the model matrix is an \\(n \\times k\\) matrix (with \\(n\\) = sample size and \\(k\\) = number of model coefficients) that is multiplied by the vector of the \\(k\\) model coefficients to obtain the fitted values of a normal linear model. The first column of the model matrix normally contains only ones. This column is multiplied by the intercept. The other columns contain the observed values of the predictor variables if these are numeric variables, or indicator variables (= dummy variables) for factor levels if the predictors are categorical variables (= factors). For categorical variables the model matrix can be constructed in a number of ways. How it is constructed determines how the model coefficients can be interpreted. For example, coefficients could represent differences between means of specific factor levels to the mean of the reference level. That is what we have introduced above. However, they could also represent a linear, quadratic or cubic effect of an ordered factor. Here, we show how this works. An ordered factor is a categorical variable with levels that have a natural order, for example, low, medium and high. How do we tell R that a factor is ordered? The swallow data contain a factor nesting_aid that contains the type aid provided in a barn for the nesting swallows. The natural order of the levels is none &lt; support (e.g., a wooden stick in the wall that helps support a nest built by the swallow) &lt; artificial_nest &lt; both (support and artificial nest). However, when we read in the data R orders these levels alphabetically rather than according to the logical order. data(swallows) levels(swallows$nesting_aid) ## [1] &quot;artif_nest&quot; &quot;both&quot; &quot;none&quot; &quot;support&quot; And with the function contrasts we see how R will construct the model matrix. contrasts(swallows$nesting_aid) ## both none support ## artif_nest 0 0 0 ## both 1 0 0 ## none 0 1 0 ## support 0 0 1 R will construct three dummy variables and call them both, none, and support. The variable both will have an entry of one when the observation is both and zero otherwise. Similarly, the other two dummy variables are indicator variables of the other two levels and artif_nest is the reference level. The model coefficients can then be interpreted as the difference between artif_nest and each of the other levels. The instruction how to transform a factor into columns of a model matrix is called the contrasts. Now, lets bring the levels into their natural order and define the factor as an ordered factor. swallows$nesting_aid &lt;- factor(swallows$nesting_aid, levels=c(&quot;none&quot;, &quot;support&quot;, &quot;artif_nest&quot;, &quot;both&quot;), ordered=TRUE) levels(swallows$nesting_aid) ## [1] &quot;none&quot; &quot;support&quot; &quot;artif_nest&quot; &quot;both&quot; The levels are now in the natural order. R will, from now on, use this order for analyses, tables, and plots, and because we defined the factor to be an ordered factor, R will use polynomial contrasts: contrasts(swallows$nesting_aid) ## .L .Q .C ## [1,] -0.6708204 0.5 -0.2236068 ## [2,] -0.2236068 -0.5 0.6708204 ## [3,] 0.2236068 -0.5 -0.6708204 ## [4,] 0.6708204 0.5 0.2236068 When using polynomial contrasts, R will construct three (= number of levels minus one) variables that are called .L, .Q, and .C for linear, quadratic and cubic effects. The contrast matrix defines which numeric value will be inserted in each of the three corresponding columns in the model matrix for each observation, for example, an observation with support in the factor nesting_aid will get the values -0.224, -0.5 and 0.671 in the columns L, Q and C of the model matrix. These contrasts define yet another way to get 4 different group means: \\(m1 = \\beta_0  0.671* \\beta_1 + 0.5*\\beta_2 - 0.224* \\beta_3\\) \\(m2 = \\beta_0  0.224* \\beta_1 - 0.5*\\beta_2 + 0.671* \\beta_3\\) \\(m3 = \\beta_0 + 0.224* \\beta_1 - 0.5*\\beta_2 - 0.671* \\beta_3\\) \\(m4 = \\beta_0 + 0.671* \\beta_1 + 0.5*\\beta_2 + 0.224* \\beta_3\\) The group means are the same, independent of whether a factor is defined as ordered or not. The ordering also has no effect on the variance that is explained by the factor nesting_aid or the overall model fit. Only the model coefficients and their interpretation depend on whether a factor is defined as ordered or not. When we define a factor as ordered, the coefficients can be interpreted as linear, quadratic, cubic, or higher order polynomial effects. The number of the polynomials will always be the number of factor levels minus one (unless the intercept is omitted from the model in which case it is the number of factor levels). Linear, quadratic, and further polynomial effects normally are more interesting for ordered factors than single differences from a reference level because linear and polynomial trends tell us something about consistent changes in the outcome along the ordered factor levels. Therefore, an ordered factor with k levels is treated like a covariate consisting of the centered level numbers (-1.5, -0.5, 0.5, 1.5 in our case with four levels) and k-1 orthogonal polynomials of this covariate are included in the model. Thus, if we have an ordered factor A with three levels, y~A is equivalent to y~x+I(x^2), with x=-1 for the lowest, x=0 for the middle and x=1 for the highest level. Note that it is also possible to define own contrasts if we are interested in specific differences or trends. However, it is not trivial to find meaningful and orthogonal (= uncorrelated) contrasts. 11.6 Quadratic and Higher Polynomial Terms The straight regression line for the biomass of grass species Ap Alopecurus pratensis dependent on the distance to the ground water does not fit well (Figure 11.10). The residuals at low and high values of water tend to be positive and intermediate water levels are associated with negative residuals. This points out a possible violation of the model assumptions. The problem is that the relationship between distance to water and biomass of species Ap is not linear. In real life, we often find non-linear relationships, but if the shape of the relationship is quadratic (plus, potentially, a few more polynomials) we can still use linear modeling (the term linear refers to the linear function used to describe the relationship between the outcome and the predictor variables: \\(f(x) = \\beta_0 + \\beta_1x + \\beta_2x^2\\) is a linear function compared to, e.g., \\(f(x) = \\beta^x\\), which is not a linear function). We simply add the quadratic term of the predictor variable, that is, water in our example, as a further predictor in the linear predictor: \\(\\hat{y_i} = \\beta_0+\\beta_1water_i+\\beta_2water_i^2\\). A quadratic term can be fitted in R using the function I() which tells R that we want the squared values of distance to water. If we do not use I() the ^2 indicates a two-way interaction. The model specification is then lm(log(Yi.g) ~ Water + I(Water^2), data=...). The cubic term would be added by +I(Water^3). As with interactions, a polynomial term changes the interpretation of lower level polynomials. Therefore, we normally include all polynomials up to a specific degree. Furthermore, polynomials are normally correlated (if no special transformation is used, see below) which could cause problems when fitting the model such as non-convergence. To avoid collinearity among polynomials, so called orthogonal polynomials can be used. These are polynomials that are uncorrelated. To that end, we can use the function poly which creates as many orthogonal polynomials of the variable as we want: poly(dat$Water, 2) creates two columns, the first one can be used to model the linear effect of water, the second one to model the quadratic term of water: t.poly &lt;- poly(dat$Water, 2) dat$Water.l &lt;- t.poly[,1] # linear term for water dat$Water.q &lt;- t.poly[,2] # quadratic term for water mod &lt;- lm(log(Yi.g) ~ Water.l + Water.q, data=dat) When orthogonal polynomials are used, the estimated linear and quadratic effects can be interpreted as purely linear and purely quadratic influences of the predictor on the outcome. The function poly applies a specific transformation to the original variables. To reproduce the transformation (e.g. for getting the corresponding orthogonal polynomials for new data used to draw an effect plot), the function predict can be used with the poly-object created based on the original data. newdat &lt;- data.frame(Water = seq(0,130)) # transformation analogous to the one used to fit the model: newdat$Water.l &lt;- predict(t.poly, newdat$Water)[,1] newdat$Water.q &lt;- predict(t.poly, newdat$Water)[,2] These transformed variables can then be used to calculate fitted values that correspond to the water values specified in the new data. "],["residualanalysis.html", "12 Assessing Model Assumptions 12.1 Model Assumptions 12.2 Independent and Identically Distributed 12.3 The QQ-Plot 12.4 Temporal Autocorrelation 12.5 Spatial Autocorrelation 12.6 Heteroscedasticity", " 12 Assessing Model Assumptions 12.1 Model Assumptions Every statistical model makes assumptions. We try to build models that reect the data-generating process as realistically as possible. However, a model never is the truth. Yet, all inferences drawn from a model, such as estimates of effect size or derived quantities with credible intervals, are based on the assumption that the model is true. However, if a model captures the datagenerating process poorly, for example, because it misses important structures (predictors, interactions, polynomials), inferences drawn from the model are probably biased and results become unreliable. In a (hypothetical) model that captures all important structures of the data generating process, the stochastic part, the difference between the observation and the tted value (the residuals), should only show random variation. Analyzing residuals is a very important part of the data analysis process. Residual analysis can be very exciting, because the residuals show what remains unexplained by the present model. Residuals can sometimes show surprising patterns and, thereby, provide deeper insight into the system. However, at this step of the analysis it is important not to forget the original research questions that motivated the study. Because these questions have been asked without knowledge of the data, they protect against data dredging. Of course, residual analysis may raise interesting new questions. Nonetheless, these new questions have emerged from patterns in the data, which might just be random, not systematic, patterns. The search for a model with good t should be guided by thinking about the process that generated the data, not by trial and error (i.e., do not try all possible variable combinations until the residuals look good; that is data dredging). All changes done to the model should be scientically justied. Usually, model complexity increases, rather than decreases, during the analysis. 12.2 Independent and Identically Distributed Usually, we model an outcome variable as independent and identically distributed (iid) given the model parameters. This means that all observations with the same predictor values behave like independent random numbers from the identical distribution. As a consequence, residuals should look iid. Independent means that: The residuals do not correlate with other variables (those that are included in the model as well as any other variable not included in the model). The residuals are not grouped (i.e., the means of any set of residuals should all be equal). The residuals are not autocorrelated (i.e., no temporal or spatial autocorrelation exist; Sections 12.4 and 12.5). Identically distributed means that: All residuals come from the same distribution. In the case of a linear model with normal error distribution (Chapter 11) the residuals are assumed to come from the same normal distribution. Particularly: The residual variance is homogeneous (homoscedasticity), that is, it does not depend on any predictor variable, and it does not change with the tted value. The mean of the residuals is zero over the whole range of predictor values. When numeric predictors (covariates) are present, this implies that the relationship between x and y can be adequately described by a straight line. Residual analysis is mainly done graphically. R makes it very easy to plot residuals to look at the different aspects just listed. As a rst example, we use the coal tit example from Chapter 11: Hier fehlt noch ein Teil aus dem BUCH. 12.3 The QQ-Plot xxx 12.4 Temporal Autocorrelation 12.5 Spatial Autocorrelation 12.6 Heteroscedasticity "],["lmer.html", "13 Linear Mixed Effect Models 13.1 Background 13.2 Fitting a normal linear mixed model in R 13.3 Restricted maximum likelihood estimation (REML)", " 13 Linear Mixed Effect Models 13.1 Background 13.1.1 Why Mixed Effects Models? Mixed effects models (or hierarchical models A. Gelman and Hill (2007) for a discussion on the terminology) are used to analyze nonindependent, grouped, or hierarchical data. For example, when we measure growth rates of nestlings in different nests by taking mass measurements of each nestling several times during the nestling phase, the measurements are grouped within nestlings (because there are repeated measurements of each) and the nestlings are grouped within nests. Measurements from the same individual are likely to be more similar than measurements from different individuals, and individuals from the same nest are likely to be more similar than nestlings from different nests. Measurements of the same group (here, the groups are individuals or nests) are not independent. If the grouping structure of the data is ignored in the model, the residuals do not fulfill the independence assumption. Further, predictor variables can be measured on different hierarchical levels. For example, in each nest some nestlings were treated with a hormone implant whereas others received a placebo. Thus, the treatment is measured at the level of the individual, while clutch size is measured at the level of the nest. Clutch size was measured only once per nest but entered in the data file more than once (namely for each individual from the same nest). Repeated measure results in pseudoreplication if we do not account for the hierarchical data structure in the model. Mixed models allow modeling of the hierarchical structure of the data and, therefore, account for pseudoreplication. Mixed models are further used to analyze variance components. For example, when the nestlings were cross-fostered so that they were not raised by their genetic parents, we would like to estimate the proportions of the variance (in a measurement, e.g., wing length) that can be assigned to genetic versus to environmental differences. The three problems, grouped data, repeated measure and interest in variances are solved by adding further variance parameters to the model. As a result, the linear predictor of such models contain parameters that are fixed and parameters that vary among levels of a grouping variable. The latter are called random effects. Thus, a mixed model contains fixed and random effects. Often the grouping variable, which is a categorical variable, i.e., a factor, is called the random effect, even though it is not the factor that is random. The levels of the factor are seen as a random sample from a bigger population of levels, and a distribution, usually the normal distribution, is fitted to the level-specific parameter values. Thus, a random effect in a model can be seen as a model (for a parameter) that is nested within the model for the data. Predictors that are defined as fixed effects are either numeric or, if they are categorical, they have a finite (fixed) number of levels. For example, the factor treatment in the Barn owl study below has exactly two levels placebo and corticosterone and nothing more. In contrast, random effects have a theoretically infinite number of levels of which we have measured a random sample. For example, we have measured 10 nests, but there are many more nests in the world that we have not measured. Normally, fixed effects have a low number of levels whereas random effects have a large number of levels (at least 3!). For fixed effects we are interested in the specific differences between levels (e.g., between males and females), whereas for random effects we are only interested in the between-level (between-group, e.g., between-nest) variance rather than in differences between specific levels (e.g., nest A versus nest B). Typical fixed effects are: treatment, sex, age classes, or season. Typical random effects are: nest, individual, field, school, or study plot. It depends sometimes on the aim of the study whether a factor should be treated as fixed or random. When we would like to compare the average size of a corn cob between specific regions, then we include region as a fixed factor. However, when we would like to know how the size of a corn cob is related to the irrigation system and we have several measurements within each of a sample of regions, then we treat region as a random factor. 13.1.2 Random Factors and Partial Pooling In a model with fixed factors, the differences of the group means to the mean of the reference group are separately estimated as model parameters. This produces \\(k-1\\) (independent) model parameters, where \\(k\\) is the number of groups (or number of factor levels). In contrast, for a random factor, the between-group variance is estimated and the \\(k\\) group-specific means are assumed to be normally distributed around the population mean. These \\(k\\) means are thus not independent. We usually call the differences between the specific mean of group \\(g\\) and the mean of all groups \\(b_g\\). They are assumed to be realizations of the same (in most cases normal) distribution with a mean of zero. They are like residuals. The variance of the \\(b_g\\) values is the among-group variance. Treating a factor as a random factor is equivalent to partial pooling of the data. There are three different ways to obtain means for grouped data. First, the grouping structure of the data can be ignored. This is called complete pooling (left panel in Figure 13.1). Second, group means may be estimated separately for each group. In this case, the data from all other groups are ignored when estimating a group mean. No pooling occurs in this case (right panel in Figure 13.1). Third, the data of the different groups can be partially pooled (i.e., treated as a random effect). Thereby, the group means are weighted averages of the population mean and the unpooled group means. The weights are proportional to sample size and the inverse of the variance (see A. Gelman and Hill (2007), p. 252). Further, the estimated mean of all group equals the mean of the group specific means, thus, every group is weighed similarly for calculating the overall mean. In contrast, in the complete pooling case, the groups get weights proportional to their sample sizes. Complete pooling Partial pooling No pooling \\(\\hat{y_i} = \\beta_0\\)  \\(y_i \\sim normal(\\hat{y_i}, \\sigma^2)\\) \\(\\hat{y_i} = \\beta_0 + b_{g[i]}\\)  \\(b_g \\sim normal(0, \\sigma_b^2)\\)  \\(y_i \\sim normal(\\hat{y_i}, \\sigma^2)\\) \\(\\hat{y_i} = \\beta_{0[g[i]]}\\)  \\(y_i \\sim normal(\\hat{y_i}, \\sigma_g^2)\\) Figure 13.1: Three possibilities to obtain group means for grouped data: complete pooling, partial pooling, and no pooling. Open symbols = data, orange dots with vertical bars = group means with 95% uncertainty intervals, horizontal black line with shaded interval = population mean with 95% uncertainty interval. What is the advantage of analyses using partial pooling (i.e., mixed, hierarchical, or multilevel modelling) compared to the complete or no pooling analyses? Complete pooling ignores the grouping structure of the data. As a result, the uncertainty interval of the population mean may be too narrow. We are too confident in the result because we assume that all observations are independent when they are not. This is a typical case of pseudoreplication. On the other hand, the no pooling method (which is equivalent to treating the factor as fixed) has the danger of overestimation of the among-group variance because the group means are estimated independently of each other. The danger of overestimating the among-group variance is particularly large when sample sizes per group are low and within-group variance large. In contrast, the partial pooling method assumes that the group means are a random sample from a common distribution. Therefore, information is exchanged between groups. Estimated means for groups with low sample sizes, large variances, and means far away from the population mean are shrunk towards the population mean. Thus, group means that are estimated with a lot of imprecision (because of low sample size and high variance) are shrunk towards the population mean. How strongly they are shrunk depends on the precision of the estimates for the group specific means and the population mean. An example will help make this clear. Imagine that we measured 60 nestling birds from 10 nests (6 nestlings per nest) and found that the average nestling mass at day 10 was around 20 g with a among-nest standard deviation of 2 g. Then, we measure only one nestling from one additional nest (from the same population) whose mass was 12 g. What do we know about the average mass of this new nest? The mean of the measurements for this nest is 12 g, but with n = 1 uncertainty is high. Because we know that the average mass of the other nests was 20 g, and because the new nest belonged to the same population, a value higher than 12 g is a better estimate for an average nestling mass of the new nest than the 12 g measurement of one single nestling, which could, by chance, have been an exceptionally light individual. This is the shrinkage that partial pooling allows in a mixed model. Because of this shrinkage, the estimates for group means from a mixed model are sometimes called shrinkage estimators. A consequence of the shrinkage is that the residuals are positively correlated with the fitted values. To summarize, mixed models are used to appropriately estimate among-group variance, and to account for non-independency among data points. 13.2 Fitting a normal linear mixed model in R To introduce the linear mixed model, we use repeated hormone measures at nestling Barn Owls Tyto alba. The cortbowl data set contains stress hormone data (corticosterone, variable totCort) of nestling Barn owls which were either treated with a corticosterone-implant, or with a placebo-implant as the control group. The aim of the study was to quantify the corticosterone increase due to the corticosterone implants (Almasi et al. 2009). In each brood, one or two nestlings were implanted with a corticosterone-implant and one or two nestlings with a placebo-implant (variable Implant). Blood samples were taken just before implantation, and at days 2 and 20 after implantation. data(cortbowl) dat &lt;- cortbowl dat$days &lt;- factor(dat$days, levels=c(&quot;before&quot;, &quot;2&quot;, &quot;20&quot;)) str(dat) # the data was sampled in 2004,2005, and 2005 by the Swiss Ornithologicla Institute ## &#39;data.frame&#39;: 287 obs. of 6 variables: ## $ Brood : Factor w/ 54 levels &quot;231&quot;,&quot;232&quot;,&quot;233&quot;,..: 7 7 7 7 8 8 9 9 10 10 ... ## $ Ring : Factor w/ 151 levels &quot;898054&quot;,&quot;898055&quot;,..: 44 45 45 46 31 32 9 9 18 19 ... ## $ Implant: Factor w/ 2 levels &quot;C&quot;,&quot;P&quot;: 2 2 2 1 2 1 1 1 2 1 ... ## $ Age : int 49 29 47 25 57 28 35 53 35 31 ... ## $ days : Factor w/ 3 levels &quot;before&quot;,&quot;2&quot;,&quot;20&quot;: 3 2 3 2 3 1 2 3 2 2 ... ## $ totCort: num 5.76 8.42 8.05 25.74 8.04 ... In total, there are 287 measurements of 151 individuals (variable Ring) of 54 broods. Because the measurements from the same individual are non-independent, we use a mixed model to analyze these data: Two additional arguments for a mixed model are: a) the mixed model allows prediction of corticosterone levels for an average individual, whereas the fixed effect model allows prediction of corticosterone levels only for the 151 individuals that were sampled; and b) fewer parameters are needed. If we include individual as a fixed factor, we would use 150 parameters, while the random factor needs a much lower number of parameters. We first create a graphic to show the development for each individual, separately for owls receiving corticosterone versus owls receiving a placebo (Figure 13.2). Figure 13.2: Total corticosterone before and at day 2 and 20 after implantation of a corticosterone or a placebo implant. Lines connect measurements of the same individual. We fit a normal linear model with Ring as a random factor, and Implant, days and the interaction of Implant \\(\\times\\) days as fixed effects. Note that both Implant and days are defined as factors, thus R creates indicator variables for all levels except the reference level. Later, we will also include Brood as a grouping level; for now, we ignore this level and start with a simpler (less perfect) model for illustrative purposes. \\(\\hat{y_i} = \\beta_0 + b_{Ring[i]} + \\beta_1I(days=2) + \\beta_2I(days=20) + \\beta_3I(Implant=P) + \\beta_4I(days=2)I(Implant=P) + \\beta_5I(days=20)I(Implant=P)\\) \\(b_{Ring} \\sim normal(0, \\sigma_b)\\) \\(y_i \\sim normal(\\hat{y_i}, \\sigma)\\) Several different functions to fit a mixed model have been written in R: lme, gls, gee have been the first ones. Then lmer followed, and now, stan_lmer and brm allow to fit a large variety of hierarchical models. We here start w ith using lmer from the package lme4 (which is automatically loaded to the R-console when loading arm), because it is a kind of basis function also for stan_lmerand brm. Further, sim can treat lmer-objects but none of the earlier ones. The function lmer is used similarly to the function lm. The only difference is that the random factors are added in the model formula within parentheses. The 1 stands for the intercept and the | means grouped by. (1|Ring), therefore, adds the random deviations for each individual to the average intercept. These deviations are the b_{Ring} in the model formula above. Corticosterone data are log transformed to achieve normally distributed residuals. After having fitted the model, in real life, we always first inspect the residuals, before we look at the model output. However, that is a dilemma for this text book. Here, we would like to explain how the model is constructed just after having shown the model code. Therefore, we do the residual analyses later, but in real life, we would do it now. mod &lt;- lmer(log(totCort) ~ Implant + days + Implant:days + (1|Ring), data=dat, REML=TRUE) mod ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: log(totCort) ~ Implant + days + Implant:days + (1 | Ring) ## Data: dat ## REML criterion at convergence: 611.9053 ## Random effects: ## Groups Name Std.Dev. ## Ring (Intercept) 0.3384 ## Residual 0.6134 ## Number of obs: 287, groups: Ring, 151 ## Fixed Effects: ## (Intercept) ImplantP days2 days20 ## 1.91446 -0.08523 1.65307 0.26278 ## ImplantP:days2 ImplantP:days20 ## -1.71999 -0.09514 The output of the lmer-object tells us that the model was fitted using the REML-method, which is the restricted maximum likelihood method. The REML criterion is the statistic describing the model fit for a model fitted by REML. The model output further contains the parameter estimates. These are grouped into a random effects and fixed effects section. The random effects section gives the estimates for the among-individual standard deviation of the intercept (\\(\\sigma_{Ring} =\\) 0.34) and the residual standard deviation (\\(\\sigma =\\) 0.61). The fixed effects section gives the estimates for the intercept (\\(\\beta_0 =\\) 1.91), which is the mean logarithm of corticosterone for an average individual that received a corticosterone implant at the day of implantation. The other model coefficients are defined as follows: the difference in the logarithm of corticosterone between placebo- and corticosterone-treated individuals before implantation (\\(\\beta_1 =\\) -0.09), the difference between day 2 and before implantation for the corticosterone-treated individuals (\\(\\beta_2 =\\) 1.65), the difference between day 20 and before implantation for the corticosterone-treated individuals (\\(\\beta_3 =\\) 0.26), and the interaction parameters which tell us how the differences between day 2 and before implantation (\\(\\beta_4 =\\) -1.72), and day 20 and before implantation (\\(\\beta_5 =\\) -0.1), differ for the placebo-treated individuals compared to the corticosterone treated individuals. Neither the model output shown above nor the summary function (not shown) give any information about the proportion of variance explained by the model such as an \\(R^2\\). The reason is that it is not straightforward to obtain a measure of model fit in a mixed model, and different definitions of \\(R^2\\) exist (Nakagawa and Schielzeth 2013). The function fixef extracts the estimates for the fixed effects, the function ranef extracts the estimates for the random deviations from the population intercept for each individual. The ranef-object is a list with one element for each random factor in the model. We can extract the random effects for each ring using the $Ring notation. round(fixef(mod), 3) ## (Intercept) ImplantP days2 days20 ImplantP:days2 ## 1.914 -0.085 1.653 0.263 -1.720 ## ImplantP:days20 ## -0.095 head(ranef(mod)$Ring) # print the first 6 Ring effects ## (Intercept) ## 898054 0.24884979 ## 898055 0.11845863 ## 898057 -0.10788277 ## 898058 0.06998959 ## 898059 -0.08086498 ## 898061 -0.08396839 13.3 Restricted maximum likelihood estimation (REML) For a mixed model the restricted maximum likelihood method is used by default instead of the maximum likelihood (ML) method. The reason is that the ML-method underestimates the variance parameters because this method assumes that the fixed parameters are known without uncertainty when estimating the variance parameters. However, the estimates of the fixed effects have uncertainty. The REML method uses a mathematical trick to make the estimates for the variance parameters independent of the estimates for the fixed effects. We recommend reading the very understandable description of the REML method in Zuur et al. (2009). For our purposes, the relevant difference between the two methods is that the ML-estimates are unbiased for the fixed effects but biased for the random effects, whereas the REML-estimates are biased for the fixed effects and unbiased for the random effects. However, when sample size is large compared to the number of model parameters, the differences between the ML- and REML-estimates become negligible. As a guideline, use REML if the interest is in the random effects (variance parameters), and ML if the interest is in the fixed effects. The estimation method can be chosen by setting the argument REML to FALSE (default is TRUE). mod &lt;- lmer(log(totCort) ~ Implant + days + Implant:days + (1|Ring), data=dat, REML=FALSE) # using ML When we fit the model by stan_lmer from the rstanarm-package or brm from the brms-package, i.e., using the Bayes theorem instead of ML or REML, we do not have to care about this choice (of course!). The result from a Bayesian analyses is unbiased for all parameters (at least from a mathematical point of view - also parameters from a Bayesian model can be biased if the model violates assumptions or is confounded). "],["glm.html", "14 Generalized linear models 14.1 Introduction 14.2 Summary", " 14 Generalized linear models THIS CHAPTER IS UNDER CONSTRUCTION!!! 14.1 Introduction 14.2 Summary xxx "],["glmm.html", "15 Generalized linear mixed models 15.1 Introduction 15.2 Summary", " 15 Generalized linear mixed models 15.1 Introduction THIS CHAPTER IS UNDER CONSTRUCTION!!! In chapter 13 on linear mixed effect models we have introduced how to analyze metric outcome variables for which a normal error distribution can be assumed (potentially after transformation), when the data have a hierarchical structure and, as a consequence, observations are not independent. In chapter 14 on generalized linear models we have introduced how to analyze outcome variables for which a normal error distribution can not be assumed, as for example binary outcomes or count data. More precisely, we have extended modelling outcomes with normal error to modelling outcomes with error distributions from the exponential family (e.g., binomial or Poisson). Generalized linear mixed models (GLMM) combine the two complexities and are used to analyze outcomes with a non-normal error distribution when the data have a hierarchical structure. In this chapter, we will show how to analyze such data. Remember, a hierarchical structure of the data means that the data are collected at different levels, for example smaller and larger spatial units, or include repeated measurements in time on a specific subject. Typically, the outcome variable is measured/observed at the lowest level but other variables may be measured at different levels. A first example is introduced in the next section. 15.1.1 Binomial Mixed Model 15.1.1.1 Background To illustrate the binomial mixed model we use a subset of a data set used by Grüebler, Korner-Nievergelt, and Von Hirschheydt (2010) on barn swallow Hirundo rustica nestling survival (we selected a nonrandom sample to be able to fit a simple model; hence, the results do not add unbiased knowledge about the swallow biology!). For 63 swallow broods, we know the clutch size and the number of the nestlings that fledged. The broods came from 51 farms (larger unit), thus some of the farms had more than one brood. Note that each farm can harbor one or several broods, and the broods are nested within farms (as opposed to crossed, see chapter 13), i.e., each brood belongs to only one farm. There are three predictors measured at the level of the farm: colony size (the number of swallow broods on that farm), cow (whether there are cows on the farm or not), and dung heap (the number of dung heaps, piles of cow dung, within 500 m of the farm). The aim was to assess how swallows profit from insects that are attracted by livestock on the farm and by dung heaps. Broods from the same farm are not independent of each other because they belong to the same larger unit (farm), and thus share the characteristics of the farm (measured or unmeasured). Predictor variables were measured at the level of the farm, and are thus the same for all broods from a farm. In the model described and fitted below, we account for the non-independence of these clutches when building the model by including a random intercept per farm to model random variation between farms. The outcome variable is a proportion (proportion fledged from clutch) and thus consists of two values for each observation, as seen with the binomial model without random factors (Section 14.2.2): the number of chicks that fledged (successes) and the number of chicks that died (failures), i.e., the clutch size minus number that fledged. The random factor farm adds a farm-specific deviation \\(b_g\\) to the intercept in the linear predictor. These deviations are modeled as normally distributed with mean \\(0\\) and standard deviation \\(\\sigma_g\\). \\[ y_i \\sim binomial\\left(p_i, n_i\\right)\\\\ logit\\left(p_i\\right) = \\beta_0 + b_{g[i]} + \\beta_1\\;colonysize_i + \\beta_2\\;I\\left(cow_i = 1\\right) + \\beta_3\\;dungheap_i\\\\ b_g \\sim normal\\left(0, \\sigma_g\\right) \\] # Data on Barn Swallow (Hirundo rustica) nestling survival on farms # (a part of the data published in Grüebler et al. 2010, J Appl Ecol 47:1340-1347) library(blmeco) data(swallowfarms) #?swallowfarms # to see the documentation of the data set dat &lt;- swallowfarms str(dat) ## &#39;data.frame&#39;: 63 obs. of 6 variables: ## $ farm : int 1001 1002 1002 1002 1004 1008 1008 1008 1010 1016 ... ## $ colsize: int 1 4 4 4 1 11 11 11 3 3 ... ## $ cow : int 1 1 1 1 1 1 1 1 0 1 ... ## $ dung : int 0 0 0 0 1 1 1 1 2 2 ... ## $ clutch : int 8 9 8 7 13 7 9 16 10 8 ... ## $ fledge : int 8 0 6 5 9 3 7 4 9 8 ... # check number of farms in the data set length(unique(dat$farm)) ## [1] 51 15.1.1.2 Fitting a Binomial Mixed Model in R 15.1.1.2.1 Using the glmer function dat$colsize.z &lt;- scale(dat$colsize) # z-transform values for better model convergence dat$dung.z &lt;- scale(dat$dung) dat$die &lt;- dat$clutch - dat$fledge dat$farm.f &lt;- factor(dat$farm) # for clarity we define farm as a factor The glmer function uses the standard way to formulate a statistical model in R, with the outcome on the left, followed by the ~ symbol, meaning explained by, followed by the predictors, which are separated by +. The notation for the random factor with only a random intercept was introduced in chapter 13 and is (1|farm.f) here. Remember that for fitting a binomial model we have to provide the number of successful events (number of fledglings that survived) and the number of failures (those that died) within a two-column matrix that we create using the function cbind. # fit GLMM using glmer function from lme4 package library(lme4) mod.glmer &lt;- glmer(cbind(fledge,die) ~ colsize.z + cow + dung.z + (1|farm.f) , data=dat, family=binomial) 15.1.1.2.2 Assessing Model Assumptions for the glmer fit The residuals of the model look fairly normal (top left panel of Figure 15.1 with slightly wider tails. The random intercepts for the farms look perfectly normal as they should. The plot of the residuals vs. fitted values (bottom left panel) shows a slight increase in the residuals with increasing fitted values. Positive correlations between the residuals and the fitted values are common in mixed models due to the shrinkage effect (chapter 13). Due to the same reason the fitted proportions slightly overestimate the observed proportions when these are large, but underestimate them when small (bottom right panel). What is looking like a lack of fit here can be seen as preventing an overestimation of the among farm variance based on the assumption that the farms in the data are a random sample of farms belonging to the same population. The mean of the random effects is close to zero as it should. We check that because sometimes the glmer function fails to correctly separate the farm-specific intercepts from the overall intercept. A non-zero mean of random effects does not mean a lack of fit, but a failure of the model fitting algorithm. In such a case, we recommend using a different fitting algorithm, e.g. brm (see below) or stan_glmer from the rstanarm package. A slight overdispersion (approximated dispersion parameter &gt;1) seems to be present, but nothing to worry about. par(mfrow=c(2,2), mar=c(3,5,1,1)) # check normal distribution of residuals qqnorm(resid(mod.glmer), main=&quot;qq-plot residuals&quot;) qqline(resid(mod.glmer)) # check normal distribution of random intercepts qqnorm(ranef(mod.glmer)$farm.f[,1], main=&quot;qq-plot, farm&quot;) qqline(ranef(mod.glmer)$farm.f[,1]) # residuals vs fitted values to check homoscedasticity plot(fitted(mod.glmer), resid(mod.glmer)) abline(h=0) # plot data vs. predicted values dat$fitted &lt;- fitted(mod.glmer) plot(dat$fitted,dat$fledge/dat$clutch) abline(0,1) Figure 15.1: Diagnostic plots to assess model assumptions for mod.glmer. Uppper left: quantile-quantile plot of the residuals vs. theoretical quantiles of the normal distribution. Upper rihgt: quantile-quantile plot of the random effects farm. Lower left: residuals vs. fitted values. Lower right: observed vs. fitted values. # check distribution of random effects mean(ranef(mod.glmer)$farm.f[,1]) ## [1] -0.001690303 # check for overdispersion dispersion_glmer(mod.glmer) ## [1] 1.192931 detach(package:lme4) 15.1.1.2.3 Using the brm function Now we fit the same model using the function brm from the R package brms. This function allows fitting Bayesian generalized (non-)linear multivariate multilevel models using Stan (Betancourt 2013) for full Bayesian inference. We shortly introduce the fitting algorithm used by Stan, Hamiltonian Monte Carlo, in chapter 18. When using the function brm there is no need to install rstan or write the model in Stan-language. A wide range of distributions and link functions are supported, and the function offers many things more. Here we use it to fit the model as specified by the formula object above. Note that brm requires that a binomial outcome is specified in the format successes|trials(), which is the number of fledged nestlings out of the total clutch size in our case. In contrast, the glmer function required to specify the number of nestlings that fledged and died (which together sum up to clutch size), in the format cbind(successes, failures). The family is also called binomial in brm, but would be bernoulli for a binary outcome, whereas glmer would use binomial in both situations (Bernoulli distribution is a special case of the binomial). However, it is slightly confusing that (at the time of writing this chapter) the documentation for brmsfamily did not mention the binomial family under Usage, where it probably went missing, but it is mentioned under Arguments for the argument family. Prior distributions are an integral part of a Bayesian model, therefore we need to specify prior distributions. We can see what default prior distributions brm is using by applying the get_prior function to the model formula. The default prior for the effect sizes is a flat prior which gives a density of 1 for any value between minus and plus infinity. Because this is not a proper probability distribution it is also called an improper distribution. The intercept gets a t-distribution with mean of 0, standard deviation of 2.5 and 3 degrees of freedoms. Transforming this t-distribution to the proportion scale (using the inverse-logit function) becomes something similar to a uniform distribution between 0 and 1 that can be seen as non-informative for a probability. For the among-farm standard deviation, it uses the same t-distribution as for the intercept. However, because variance parameters such as standard deviations only can take on positive numbers, it will use only the positive half of the t-distribution (this is not seen in the output of get_prior). When we have no prior information on any parameter, or if we would like to base the results solely on the information in the data, we specify weakly informative prior distributions that do not noticeably affect the results but they will facilitate the fitting algorithm. This is true for the priors of the intercept and among-farm standard deviation. However, for the effect sizes, we prefer specifying more narrow distributions (see chapter 10). To do so, we use the function prior. To apply MCMC sampling we need some more arguments: warmup specifies the number of iterations during which we allow the algorithm to be adapted to our specific model and to converge to the posterior distribution. These iterations should be discarded (similar to the burn-in period when using, e.g., Gibbs sampling); iter specifies the total number of iterations (including those discarded); chains specifies the number of chains; init specifies the starting values of the iterations. By default (init=NULL) or by setting init=\"random\" the initial values are randomly chosen which is recommended because then different initial values are chosen for each chain which helps to identify non-convergence. However, sometimes random initial values cause the Markov chains to behave badly. Then you can either use the maximum likelihood estimates of the parameters as starting values, or simply ask the algorithm to start with zeros. thin specifies the thinning of the chain, i.e., whether all iterations should be kept (thin=1) or for example every 4th only (thin=4); cores specifies the number of cores used for the algorithm; seed specifies the random seed, allowing for replication of results. library(brms) # check which parameters need a prior get_prior(fledge|trials(clutch) ~ colsize.z + cow + dung.z + (1|farm.f), data=dat, family=binomial(link=&quot;logit&quot;)) ## prior class coef group resp dpar nlpar lb ub ## (flat) b ## (flat) b colsize.z ## (flat) b cow ## (flat) b dung.z ## student_t(3, 0, 2.5) Intercept ## student_t(3, 0, 2.5) sd 0 ## student_t(3, 0, 2.5) sd farm.f 0 ## student_t(3, 0, 2.5) sd Intercept farm.f 0 ## source ## default ## (vectorized) ## (vectorized) ## (vectorized) ## default ## default ## (vectorized) ## (vectorized) # specify own priors myprior &lt;- prior(normal(0,5), class=&quot;b&quot;) mod.brm &lt;- brm(fledge|trials(clutch) ~ colsize.z + cow + dung.z + (1|farm.f) , data=dat, family=binomial(link=&quot;logit&quot;), prior=myprior, warmup = 500, iter = 2000, chains = 2, init = &quot;random&quot;, cores = 2, seed = 123) # note: thin=1 is default and we did not change this here. 15.1.1.2.4 Checking model convergence for the brm fit We first check whether we find warnings in the R console about problems of the fitting algorithm. Warnings should be taken seriously. Often, we find help in the Stan online documentation (or when typing launch_shinystan(mod.brm) into the R-console) what to change when calling the brm function to get a fit that is running smoothly. Once, we get rid of all warnings, we need to check how well the Markov chains mixed. We can either do that by scanning through the many diagnostic plots given by launch_shinystan(mod) or create the most important plots ourselves such as the traceplot (Figure 15.2). par(mar=c(2,2,2,2)) mcmc_plot(mod.brm, type = &quot;trace&quot;) Figure 15.2: Traceplot of the Markov chains. After convergence, both Markov chains should sample from the same stationary distribution. Indications of non-convergence would be, if the two chains diverge or vary around different means. 15.1.1.2.5 Checking model fit by posterior predictive model checking To assess how well the model fits to the data we do posterior predictive model checking (Chapter 16). For binomial as well as for Poisson models comparing the standard deviation of the data with those of replicated data from the model is particularly important. If the standard deviation of the real data would be much higher compared to the ones of the replicated data from the model, overdispersion would be an issue. However, here, the model is able to capture the variance in the data correctly (Figure 15.3). The fitted vs observed plot also shows a good fit. yrep &lt;- posterior_predict(mod.brm) sdyrep &lt;- apply(yrep, 1, sd) par(mfrow=c(1,3), mar=c(3,4,1,1)) hist(yrep, freq=FALSE, main=NA, xlab=&quot;Number of fledglings&quot;) hist(dat$fledge, add=TRUE, col=rgb(1,0,0,0.3), freq=FALSE) legend(10, 0.15, fill=c(&quot;grey&quot;,rgb(1,0,0,0.3)), legend=c(&quot;yrep&quot;, &quot;y&quot;)) hist(sdyrep) abline(v=sd(dat$fledge), col=&quot;red&quot;, lwd=2) plot(fitted(mod.brm)[,1], dat$fledge, pch=16, cex=0.6) abline(0,1) Figure 15.3: Posterior predictive model checking: Histogram of the number of fledglings simulated from the model together with a histogram of the real data, and a histogram of the standard deviations of replicated data from the model together with the standard deviation of the data (vertical line in red). The third plot gives the fitted vs. observed values. After checking the diagnostic plots, the posterior predictive model checking and the general model fit, we assume that the model describes the data generating process reasonably well, so that we can proceed to drawing conclusions. 15.1.1.3 Drawing Conclusions The generic summary function gives us the results for the model object containing the fitted model, and works for both the model fitted with glmer and brm. Lets start having a look at the summary from mod.glmer. The summary provides the fitting method, the model formula, statistics for the model fit including the Akaike information criterion (AIC), the Bayesian information criterion (BIC), the scaled residuals, the random effects variance and information about observations and groups, a table with coefficient estimates for the fixed effects (with standard errors and a z-test for the coefficient) and correlations between fixed effects. We recommend to always check if the number of observations and groups, i.e., 63 barn swallow nests from 51 farms here, is correct. This information shows if the glmer function has correctly recognized the hierarchical structure in the data. Here, this is correct. To assess the associations between the predictor variables and the outcome analyzed, we need to look at the column Estimate in the table of fixed effects. This column contains the estimated model coefficients, and the standard error for these estimates is given in the column Std. Error, along with a z-test for the null hypothesis of a coefficient of zero. In the random effects table, the among farm variance and standard deviation (square root of the variance) are given. The function confint shows the 95% confidence intervals for the random effects (.sig01) and fixed effects estimates. In the summary output from mod.brm we see the model formula and some information on the Markov chains after the warm-up. In the group-level effects (between group standard deviations) and population-level effects (effect sizes, model coefficients) tables some summary statistics of the posterior distribution of each parameter are given. The Estimate is the mean of the posterior distribution, the Est.Error is the standard deviation of the posterior distribution (which is the standard error of the parameter estimate). Then we see the lower and upper limit of the 95% credible interval. Also, some statistics for measuring how well the Markov chains converged are given: the Rhat and the effective sample size (ESS). The bulk ESS tells us how many independent samples we have to describe the posterior distribution, and the tail ESS tells us on how many samples the limits of the 95% credible interval is based on. Because we used the logit link function, the coefficients are actually on the logit scale and are a bit difficult to interpret. What we can say is that positive coefficients indicate an increase and negative coefficients indicate a decrease in the proportion of nestlings fledged. For continuous predictors, as colsize.z and dung.z, this coefficient refers to the change in the logit of the outcome with a change of one in the predictor (e.g., for colsize.z an increase of one corresponds to an increase of a standard deviation of colsize). For categorical predictors, the coefficients represent a difference between one category and another (reference category is the one not shown in the table). To visualize the coefficients we could draw effect plots. # glmer summary(mod.glmer) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: cbind(fledge, die) ~ colsize.z + cow + dung.z + (1 | farm.f) ## Data: dat ## ## AIC BIC logLik deviance df.resid ## 282.5 293.2 -136.3 272.5 58 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.2071 -0.4868 0.0812 0.6210 1.8905 ## ## Random effects: ## Groups Name Variance Std.Dev. ## farm.f (Intercept) 0.2058 0.4536 ## Number of obs: 63, groups: farm.f, 51 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.09533 0.19068 -0.500 0.6171 ## colsize.z 0.05087 0.11735 0.434 0.6646 ## cow 0.39370 0.22692 1.735 0.0827 . ## dung.z -0.14236 0.10862 -1.311 0.1900 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) clsz.z cow ## colsize.z 0.129 ## cow -0.828 -0.075 ## dung.z 0.033 0.139 -0.091 confint.95 &lt;- confint(mod.glmer); confint.95 ## 2.5 % 97.5 % ## .sig01 0.16809483 0.7385238 ## (Intercept) -0.48398346 0.2863200 ## colsize.z -0.18428769 0.2950063 ## cow -0.05360035 0.8588134 ## dung.z -0.36296714 0.0733620 # brm summary(mod.brm) ## Family: binomial ## Links: mu = logit ## Formula: fledge | trials(clutch) ~ colsize.z + cow + dung.z + (1 | farm.f) ## Data: dat (Number of observations: 63) ## Draws: 2 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup draws = 3000 ## ## Group-Level Effects: ## ~farm.f (Number of levels: 51) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.55 0.15 0.26 0.88 1.00 858 1513 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.09 0.22 -0.52 0.34 1.00 3151 2341 ## colsize.z 0.05 0.13 -0.21 0.30 1.00 2951 2043 ## cow 0.40 0.25 -0.09 0.89 1.00 3269 2245 ## dung.z -0.15 0.12 -0.39 0.08 1.00 2882 2257 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). From the results we conclude that in farms without cows (when cow=0) and for average colony sizes (when colsize.z=0) and average number of dung heaps (when dung.z=0) the average nestling survival of Barn swallows is the inverse-logit function of the Intercept, thus, plogis(-0.09) = 0.48 with a 95% uncertainty interval of 0.37 - 0.58. We further see that colony size and number of dung heaps are less important than whether cows are present or not. Their estimated partial effect is small and their uncertainty interval includes only values close to zero. However, whether cows are present or not may be important for the survival of nestlings. The average nestling survival in farms with cows is plogis(-0.09+0.4) = 0.58. For getting the uncertainty interval of this survival estimate, we need to do the calculation for every simulation from the posterior distribution of both parameters. bsim &lt;- posterior_samples(mod.brm) # survival of nestlings on farms with cows: survivalest &lt;- plogis(bsim$b_Intercept + bsim$b_cow) quantile(survivalest, probs=c(0.025, 0.975)) # 95% uncertainty interval ## 2.5% 97.5% ## 0.5078202 0.6400263 In medical research, it is standard to report the fixed-effects coefficients from GLMM with binomial or Bernoulli error as odds ratios by taking the exponent (R function exp for \\(e^{()}\\)) of the coefficient on the logit-scale. For example, the coefficient for cow from mod.glmer, 0.39 (95% CI from -0.05 to -0.05), represents an odds ratio of exp( 0.39)=1.48 (95% CI from 0.95 to 0.95). This means that the odds for fledging (vs. not fledging) from a clutch from a farm with livestock present is about 1.5 times larger than the odds for fledging if no livestock is present (relative effect). 15.2 Summary "],["modelchecking.html", "16 Posterior predictive model checking 16.1 Introduction 16.2 Summary", " 16 Posterior predictive model checking THIS CHAPTER IS UNDER CONSTRUCTION!!! 16.1 Introduction 16.2 Summary xxx "],["model_comparison.html", "17 Model comparison and multimodel inference 17.1 Introduction 17.2 Summary", " 17 Model comparison and multimodel inference THIS CHAPTER IS UNDER CONSTRUCTION!!! 17.1 Introduction literature to refer to: Tredennick et al. (2021) 17.2 Summary xxx "],["stan.html", "18 MCMC using Stan 18.1 Background 18.2 Install rstan 18.3 Writing a Stan model 18.4 Run Stan from R Further reading", " 18 MCMC using Stan 18.1 Background Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. 18.2 Install rstan In this book we use the program Stan to draw random samples from the joint posterior distribution of the model parameters given a model, the data, prior distributions, and initial values. To do so, it uses the no-U-turn sampler, which is a type of Hamiltonian Monte Carlo simulation (Hoffman and Gelman 2014; Betancourt 2013), and optimization-based point estimation. These algorithms are more efficient than the ones implemented in BUGS programs and they can handle larger data sets. Stan works particularly well for hierar- chical models (Betancourt and Girolami 2013). Stan runs on Windows, Mac, and Linux and can be used via the R interface rstan. Stan is automatically installed when the R package rstan is installed. For installing rstan, it is advised to follow closely the system-specific instructions. 18.3 Writing a Stan model The statistical model is written in the Stan language and saved in a text file. The Stan language is rather strict, forcing the user to write unambiguous models. Stan is very well documented and the Stan Documentation contains a comprehensive Language Manual, a Wiki documentation and various tutorials. We here provide a normal regression with one predictor variable as a worked example. The entire Stan model is as following (saved as linreg.stan) data { int&lt;lower=0&gt; n; vector[n] y; vector[n] x; } parameters { vector[2] beta; real&lt;lower=0&gt; sigma; } model { //priors beta ~ normal(0,5); sigma ~ cauchy(0,5); // likelihood y ~ normal(beta[1] + beta[2] * x, sigma); } A Stan model consists of different named blocks. These blocks are (from first to last): data, transformed data, parameters, trans- formed parameters, model, and generated quantities. The blocks must appear in this order. The model block is mandatory; all other blocks are optional. In the data block, the type, dimension, and name of every variable has to be declared. Optionally, the range of possible values can be specified. For example, vector[N] y; means that y is a vector (type real) of length N, and int&lt;lower=0&gt; N; means that N is an integer with nonnegative values (the bounds, here 0, are included). Note that the restriction to a possible range of values is not strictly necessary but this will help specifying the correct model and it will improve speed. We also see that each line needs to be closed by a column sign. In the parameters block, all model parameters have to be defined. The coefficients of the linear predictor constitute a vector of length 2, vector[2] beta;. Alternatively, real beta[2]; could be used. The sigma parameter is a one-number parameter that has to be positive, therefore real&lt;lower=0&gt; sigma;. The model block contains the model specification. Stan functions can handle vectors and we do not have to loop over all observations as typical for BUGS . Here, we use a Cauchy distribution as a prior distribution for sigma. This distribution can have negative values, but because we defined the lower limit of sigma to be 0 in the parameters block, the prior distribution actually used in the model is a truncated Cauchy distribution (truncated at zero). In Chapter 10.2 we explain how to choose prior distributions. Further characteristics of the Stan language that are good to know include: The variance parameter for the normal distribution is specified as the standard deviation (like in R but different from BUGS, where the precision is used). If no prior is specified, Stan uses a uniform prior over the range of possible values as specified in the parameter block. Variable names must not contain periods, for example, x.z would not be allowed, but x_z is allowed. To comment out a line, use double forward-slashes //. 18.4 Run Stan from R We fit the model to simulated data. Stan needs a vector containing the names of the data objects. In our case, x, y, and N are objects that exist in the R console. The function stan() starts Stan and returns an object containing MCMCs for every model parameter. We have to specify the name of the file that contains the model specification, the data, the number of chains, and the number of iterations per chain we would like to have. The first half of the iterations of each chain is declared as the warm-up. During the warm-up, Stan is not simulating a Markov chain, because in every step the algorithm is adapted. After the warm-up the algorithm is fixed and Stan simulates Markov chains. library(rstan) # Simulate fake data n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # random numbers of the covariate simresid &lt;- rnorm(n, 0, sd=sigma) # residuals y &lt;- b0 + b1*x + simresid # calculate y, i.e. the data # Bundle data into a list datax &lt;- list(n=length(y), y=y, x=x) # Run STAN fit &lt;- stan(file = &quot;stanmodels/linreg.stan&quot;, data=datax, verbose = FALSE) ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.5e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.055 seconds (Warm-up) ## Chain 1: 0.043 seconds (Sampling) ## Chain 1: 0.098 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.049 seconds (Warm-up) ## Chain 2: 0.043 seconds (Sampling) ## Chain 2: 0.092 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.049 seconds (Warm-up) ## Chain 3: 0.048 seconds (Sampling) ## Chain 3: 0.097 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 6e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.051 seconds (Warm-up) ## Chain 4: 0.046 seconds (Sampling) ## Chain 4: 0.097 seconds (Total) ## Chain 4: Further reading Stan-Homepage: It contains the documentation for Stand a a lot of tutorials. "],["ridge_regression.html", "19 Ridge Regression 19.1 Introduction", " 19 Ridge Regression THIS CHAPTER IS UNDER CONSTRUCTION!!! We should provide an example in Stan. 19.1 Introduction # Settings library(R2OpenBUGS) bugslocation &lt;- &quot;C:/Program Files/OpenBUGS323/OpenBugs.exe&quot; # location of OpenBUGS bugsworkingdir &lt;- file.path(getwd(), &quot;BUGS&quot;) # Bugs working directory #------------------------------------------------------------------------------- # Simulate fake data #------------------------------------------------------------------------------- library(MASS) n &lt;- 50 # sample size b0 &lt;- 1.2 b &lt;- rnorm(5, 0, 2) Sigma &lt;- matrix(c(10,3,3,2,1, 3,2,3,2,1, 3,3,5,3,2, 2,2,3,10,3, 1,1,2,3,15),5,5) Sigma x &lt;- mvrnorm(n = n, rep(0, 5), Sigma) simresid &lt;- rnorm(n, 0, sd=3) # residuals x.z &lt;- x for(i in 1:ncol(x)) x.z[,i] &lt;- (x[,i]-mean(x[,i]))/sd(x[,i]) y &lt;- b0 + x.z%*%b + simresid # calculate y, i.e. the data #------------------------------------------------------------------------------- # Function to generate initial values #------------------------------------------------------------------------------- inits &lt;- function() { list(b0=runif(1, -2, 2), b=runif(5, -2, 2), sigma=runif(1, 0.1, 2)) } #------------------------------------------------------------------------------- # Run OpenBUGS #------------------------------------------------------------------------------- parameters &lt;- c(&quot;b0&quot;, &quot;b&quot;, &quot;sigma&quot;) lambda &lt;- c(1, 2, 10, 25, 50, 100, 500, 1000, 10000) bs &lt;- matrix(ncol=length(lambda), nrow=length(b)) bse &lt;- matrix(ncol=length(lambda), nrow=length(b)) for(j in 1:length(lambda)){ datax &lt;- list(y=as.numeric(y), x=x, n=n, mb=rep(0, 5), lambda=lambda[j]) fit &lt;- bugs(datax, inits, parameters, model.file=&quot;ridge_regression.txt&quot;, n.thin=1, n.chains=2, n.burnin=5000, n.iter=10000, debug=FALSE, OpenBUGS.pgm = bugslocation, working.directory=bugsworkingdir) bs[,j] &lt;- fit$mean$b bse[,j] &lt;- fit$sd$b } range(bs) plot(1:length(lambda), seq(-2, 1, length=length(lambda)), type=&quot;n&quot;) colkey &lt;- rainbow(length(b)) for(j in 1:nrow(bs)){ lines(1:length(lambda), bs[j,], col=colkey[j], lwd=2) lines(1:length(lambda), bs[j,]-2*bse[j,], col=colkey[j], lty=3) lines(1:length(lambda), bs[j,]+2*bse[j,], col=colkey[j], lty=3) } abline(h=0) round(fit$summary,2) #------------------------------------------------------------------------------- # Run WinBUGS #------------------------------------------------------------------------------- library(R2WinBUGS) bugsdir &lt;- &quot;C:/Users/fk/WinBUGS14&quot; # mod &lt;- bugs(datax, inits= inits, parameters, model.file=&quot;normlinreg.txt&quot;, n.chains=2, n.iter=1000, n.burnin=500, n.thin=1, debug=TRUE, bugs.directory=bugsdir, program=&quot;WinBUGS&quot;, working.directory=bugsworkingdir) #------------------------------------------------------------------------------- # Test convergence and make inference #------------------------------------------------------------------------------- library(blmeco) # Make Figure 12.2 par(mfrow=c(3,1)) historyplot(fit, &quot;beta0&quot;) historyplot(fit, &quot;beta1&quot;) historyplot(fit, &quot;sigmaRes&quot;) # Parameter estimates print(fit$summary, 3) # Make predictions for covariate values between 10 and 30 newdat &lt;- data.frame(x=seq(10, 30, length=100)) Xmat &lt;- model.matrix(~x, data=newdat) predmat &lt;- matrix(ncol=fit$n.sim, nrow=nrow(newdat)) for(i in 1:fit$n.sim) predmat[,i] &lt;- Xmat%*%c(fit$sims.list$beta0[i], fit$sims.list$beta1[i]) newdat$lower.bugs &lt;- apply(predmat, 1, quantile, prob=0.025) newdat$upper.bugs &lt;- apply(predmat, 1, quantile, prob=0.975) plot(y~x, pch=16, las=1, cex.lab=1.4, cex.axis=1.2, type=&quot;n&quot;, main=&quot;&quot;) polygon(c(newdat$x, rev(newdat$x)), c(newdat$lower.bugs, rev(newdat$upper.bugs)), col=grey(0.7), border=NA) abline(c(fit$mean$beta0, fit$mean$beta1), lwd=2) box() points(x,y) "],["SEM.html", "20 Structural equation models 20.1 Introduction", " 20 Structural equation models THIS CHAPTER IS UNDER CONSTRUCTION!!! We should provide an example in Stan. 20.1 Introduction ------------------------------------------------------------------------------------------------------ # General settings #------------------------------------------------------------------------------------------------------ library(MASS) library(rjags) library(MCMCpack) #------------------------------------------------------------------------------------------------------ # Simulation #------------------------------------------------------------------------------------------------------ n &lt;- 100 heffM &lt;- 0.6 # effect of H on M heffCS &lt;- 0.0 # effect of H on Clutch size meffCS &lt;- 0.6 # effect of M on Clutch size SigmaM &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffm1 &lt;- 0.6 meffm2 &lt;- 0.7 SigmaH &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffh1 &lt;- 0.6 meffh2 &lt;- -0.7 # Latente Variablen H &lt;- rnorm(n, 0, 1) M &lt;- rnorm(n, heffM * H, 0.1) # Clutch size CS &lt;- rnorm(n, heffCS * H + meffCS * M, 0.1) # Indicators eM &lt;- cbind(meffm1 * M, meffm2 * M) datM &lt;- matrix(NA, ncol = 2, nrow = n) eH &lt;- cbind(meffh1 * H, meffh2 * H) datH &lt;- matrix(NA, ncol = 2, nrow = n) for(i in 1:n) { datM[i,] &lt;- mvrnorm(1, eM[i,], SigmaM) datH[i,] &lt;- mvrnorm(1, eH[i,], SigmaH) } #------------------------------------------------------------------------------ # JAGS Model #------------------------------------------------------------------------------ dat &lt;- list(datM = datM, datH = datH, n = n, CS = CS, #H = H, M = M, S3 = matrix(c(1,0,0,1),nrow=2)/1) # Function to create initial values inits &lt;- function() { list( meffh = runif(2, 0, 0.1), meffm = runif(2, 0, 0.1), heffM = runif(1, 0, 0.1), heffCS = runif(1, 0, 0.1), meffCS = runif(1, 0, 0.1), tauCS = runif(1, 0.1, 0.3), tauMH = runif(1, 0.1, 0.3), tauH = rwish(3,matrix(c(.02,0,0,.04),nrow=2)), tauM = rwish(3,matrix(c(.02,0,0,.04),nrow=2)) # M = as.numeric(rep(0, n)) ) } t.n.thin &lt;- 50 t.n.chains &lt;- 2 t.n.burnin &lt;- 20000 t.n.iter &lt;- 50000 # Run JAGS jagres &lt;- jags.model(&#39;JAGS/BUGSmod1.R&#39;,data = dat, n.chains = t.n.chains, inits = inits, n.adapt = t.n.burnin) params &lt;- c(&quot;meffh&quot;, &quot;meffm&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) mod &lt;- coda.samples(jagres, params, n.iter=t.n.iter, thin=t.n.thin) res &lt;- round(data.frame(summary(mod)$quantiles[, c(3, 1, 5)]), 3) res$TRUEVALUE &lt;- c(heffCS, heffM, meffCS, meffh1, meffh2, meffm1, meffm2) res # Traceplots post &lt;- data.frame(rbind(mod[[1]], mod[[2]])) names(post) &lt;- dimnames(mod[[1]])[[2]] par(mfrow = c(3,3)) param &lt;- c(&quot;meffh[1]&quot;, &quot;meffh[2]&quot;, &quot;meffm[1]&quot;, &quot;meffm[2]&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) traceplot(mod[, match(param, names(post))]) "],["spatial_glmm.html", "21 Modeling spatial data using GLMM 21.1 Introduction 21.2 Summary", " 21 Modeling spatial data using GLMM THIS CHAPTER IS UNDER CONSTRUCTION!!! 21.1 Introduction 21.2 Summary xxx "],["PART-III.html", "22 Introduction to PART III 22.1 Model notations", " 22 Introduction to PART III This part is a collection of more complicated ecological models to analyse data that may not be analysed with the traditional linear models that we covered in PART I of this book. 22.1 Model notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Thomson et al. (2009) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. "],["zeroinflated-poisson-lmm.html", "23 Zero-inflated Poisson Mixed Model 23.1 Introduction 23.2 Example data 23.3 Model", " 23 Zero-inflated Poisson Mixed Model 23.1 Introduction Usually we describe the outcome variable with a single distribution, such as the normal distribution in the case of linear (mixed) models, and Poisson or binomial distributions in the case of generalized linear (mixed) models. In life sciences, however, quite often the data are actually generated by more than one process. In such cases the distribution of the data could be the result of two or more different distributions. If we do not account for these different processes our inferences are likely to be biased. In this chapter, we introduce a mixture model that explicitly include two processes that generated the data. The zero-inflated Poisson model is a mixture of a binomial and a Poisson distribution. We belief that two (or more)-level models are very useful tools in life sciences because they can help uncover the different processes that generate the data we observe. 23.2 Example data We used the blackstork data from the blmeco-package. They contain the breeding success of Black-stork in Latvia. The data was collected and kindly provided by Maris Stradz. The data contains the number of nestlings of more then 300 Black-stork nests in different years. Counting animals or plants is a typical example of data that contain a lot of zero counts. For example, the number of nestlings produced by a breeding pair is often zero because the whole nest was depredated or because a catastrophic event occurred such as a flood. However, when the nest succeeds, the number of nestlings varies among the successful nests depending on how many eggs the female has laid, how much food the parents could bring to the nest, or other factors that affect the survival of a nestling in an intact nest. Thus the factors that determine how many zero counts there are in the data differ from the factors that determine how many nestlings there are, if a nest survives. Count data that are produced by two different processesone produces the zero counts and the other the variance in the count for the ones that were not zero in the first processare called zero-inflated data. Histograms of zero-inflated data look bimodal, with one peak at zero (Figure 23.1). Figure 23.1: Histogram of the number of nestlings counted in black stork nests Ciconia nigra in Latvia (n = 1130 observations of 279 nests). 23.3 Model The Poisson distribution does not fit well to such data, because the data contain more zero counts than expected under the Poisson distribution. Mullahy (1986) and Lambert (1992) formulated two different types of models that combine the two processes in one model and therefore account for the zero excess in the data and allow the analysis of the two processes separately. The hurdle model (Mullahy, 1986) combines a left-truncated count data model (Poisson or negative binomial distribution that only describes the distribution of data larger than zero) with a zero-hurdle model that describes the distribution of the data that are either zero or nonzero. In other words, the hurdle model divides the data into two data subsets, the zero counts and the nonzero counts, and fits two separate models to each subset of the data. To account for this division of the data, the two models assume left truncation (all measurements below 1 are missing in the data) and right censoring (all measurements larger than 1 have the value 1), respectively, in their error distributions. A hurdle model can be fitted in R using the function hurdle from the package pscl (Jackman, 2008). See the tutorial by Zeileis et al. (2008) for an introduction. In contrast to the hurdle model, the zero-inflated models (Mullahy, 1986; Lambert, 1992) combine a Bernoulli model (zero vs. nonzero) with a conditional Poisson model; conditional on the Bernoulli process being nonzero. Thus this model allows for a mixture of zero counts: some zero counts are zero because the outcome of the Bernoulli process was zero (these zero counts are sometimes called structural zero values), and others are zero because their outcome from the Poisson process was zero. The function `zeroinfl from the package pscl fits zero-inflated models (Zeileis et al., 2008). The zero-inflated model may seem to reflect the true process that has generated the data closer than the hurdle model. However, sometimes the fit of zero-inflated models is impeded because of high correlation of the model parameters between the zero model and the count model. In such cases, a hurdle model may cause less troubles. Both functions (hurdle and zeroinfl) from the package pscl do not allow the inclusion of random factors. The functions MCMCglmm from the package MCMCglmm (Hadfield, 2010) and glmmadmb from the package glmmADMB (http://glmmadmb.r-forge.r-project.org/) provide the possibility to account for zero-inflation with a GLMM. However, these functions are not very flexible in the types of zero-inflated models they can fit; for example, glmmadmb only includes a constant proportion of zero values. A zero-inflation model using BUGS is described in Ke ry and Schaub (2012). Here we use Stan to fit a zero- inflated model. Once we understand the basic model code, it is easy to add predictors and/or random effects to both the zero and the count model. The example data contain numbers of nestlings in black stork Ciconia nigra nests in Latvia collected by Maris Stradz and collaborators at 279 nests be- tween 1979 and 2010. Black storks build solid and large aeries on branches of large trees. The same aerie is used for up to 17 years until it collapses. The black stork population in Latvia has drastically declined over the last decades. Here, we use the nestling data as presented in Figure 14-2 to describe whether the number of black stork nestlings produced in Latvia decreased over time. We use a zero-inflated Poisson model to separately estimate temporal trends for nest survival and the number of nestlings in successful nests. Since the same nests have been measured repeatedly over 1 to 17 years, we add nest ID as a random factor to both models, the Bernoulli and the Poisson model. After the first model fit, we saw that the between-nest variance in the number of nest- lings for the successful nests was close to zero. Therefore, we decide to delete the random effect from the Poisson model. Here is our final model: zit is a latent (unobserved) variable that takes the values 0 or 1 for each nest i during year t. It indicates a structural zero, that is, if zit 14 1 the number of nestlings yit always is zero, because the expected value in the Poisson model lit(1 zit) becomes zero. If zit 14 0, the expected value in the Poisson model becomes lit. To fit this model in Stan, we first write the Stan model code and save it in a separated text-file with name zeroinfl.stan. Here is a handy package: https://cran.r-project.org/web/packages/GLMMadaptive/vignettes/ZeroInflated_and_TwoPart_Models.html "],["dailynestsurv.html", "24 Daily nest survival 24.1 Background 24.2 Models for estimating daily nest survival 24.3 Known fate model 24.4 The Stan model 24.5 Prepare data and run Stan 24.6 Check convergence 24.7 Look at results 24.8 Known fate model for irregular nest controls Further reading", " 24 Daily nest survival 24.1 Background Analyses of nest survival is important for understanding the mechanisms of population dynamics. The life-span of a nest could be used as a measure of nest survival. However, this measure very often is biased towards nests that survived longer because these nests are detected by the ornithologists with higher probability (Mayfield 1975). In order not to overestimate nest survival, daily nest survival conditional on survival to the previous day can be estimated. 24.2 Models for estimating daily nest survival What model is best used depends on the type of data available. Data may look: Regular (e.g. daily) nest controls, all nests monitored from their first egg onward Regular nest controls, nests found during the course of the study at different stages and nestling ages Irregular nest controls, all nests monitored from their first egg onward Irregular nest controls, nests found during the course of the study at different stages and nestling ages Table 24.1: Models useful for estimating daily nest survival. Data numbers correspond to the descriptions above. Model Data Software, R-code Binomial or Bernoulli model 1, (3) glm, glmer, Cox proportional hazard model 1,2,3,4 brm, soon: stan_cox Known fate model 1, 2 Stan code below Known fate model 3, 4 Stan code below Logistic exposure model 1,2,3,4 glm, glmerusing a link function that depends on exposure time Shaffer (2004) explains how to adapt the link function in a Bernoulli model to account for having found the nests at different nest ages (exposure time). Ben Bolker explains how to implement the logistic exposure model in R here. 24.3 Known fate model A natural model that allows estimating daily nest survival is the known-fate survival model. It is a Markov model that models the state of a nest \\(i\\) at day \\(t\\) (whether it is alive, \\(y_{it}=1\\) or not \\(y_{it}=0\\)) as a Bernoulli variable dependent on the state of the nest the day before. \\[ y_{it} \\sim Bernoulli(y_{it-1}S_{it})\\] The daily nest survival \\(S_{it}\\) can be linearly related to predictor variables that are measured on the nest or on the day level. \\[logit(S_{it}) = \\textbf{X} \\beta\\] It is also possible to add random effects if needed. 24.4 The Stan model The following Stan model code is saved as daily_nest_survival.stan. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; last[Nnests]; // day of last observation (alive or dead) int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last int&lt;lower=0&gt; y[Nnests, maxage]; // indicator of alive nests real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date } parameters { vector[3] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(last[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t]); } } // priors b[1]~normal(0,5); b[2]~normal(0,3); b[3]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):last[i]){ y[i,t]~bernoulli(y[i,t-1]*S[i,t-1]); } } } 24.5 Prepare data and run Stan Data is from (Grendelmeier2018?). load(&quot;RData/nest_surv_data.rda&quot;) str(datax) ## List of 7 ## $ y : int [1:156, 1:31] 1 NA 1 NA 1 NA NA 1 1 1 ... ## $ Nnests: int 156 ## $ last : int [1:156] 26 30 31 27 31 30 31 31 31 31 ... ## $ first : int [1:156] 1 14 1 3 1 24 18 1 1 1 ... ## $ cover : num [1:156] -0.943 -0.215 0.149 0.149 -0.215 ... ## $ age : num [1:31] -1.65 -1.54 -1.43 -1.32 -1.21 ... ## $ maxage: int 31 datax$y[is.na(datax$y)] &lt;- 0 # Stan does not allow for NA&#39;s in the outcome # Run STAN library(rstan) mod &lt;- stan(file = &quot;stanmodels/daily_nest_survival.stan&quot;, data=datax, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) 24.6 Check convergence We love exploring the performance of the Markov chains by using the function launch_shinystan from the package shinystan. 24.7 Look at results It looks like cover does not affect daily nest survival, but daily nest survival decreases with the age of the nestlings. #launch_shinystan(mod) print(mod) ## Inference for Stan model: anon_model. ## 5 chains, each with iter=2500; warmup=1250; thin=1; ## post-warmup draws per chain=1250, total post-warmup draws=6250. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b[1] 4.04 0.00 0.15 3.76 3.94 4.04 4.14 4.35 3828 1 ## b[2] 0.00 0.00 0.13 -0.25 -0.09 -0.01 0.08 0.25 4524 1 ## b[3] -0.70 0.00 0.16 -1.02 -0.81 -0.69 -0.59 -0.39 3956 1 ## lp__ -298.98 0.03 1.30 -302.39 -299.52 -298.65 -298.05 -297.53 2659 1 ## ## Samples were drawn using NUTS(diag_e) at Thu Jan 19 22:33:33 2023. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # effect plot bsim &lt;- as.data.frame(mod) nsim &lt;- nrow(bsim) newdat &lt;- data.frame(age=seq(1, datax$maxage, length=100)) newdat$age.z &lt;- (newdat$age-mean(1:datax$maxage))/sd((1:datax$maxage)) Xmat &lt;- model.matrix(~age.z, data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- plogis(Xmat%*%as.numeric(bsim[i,c(1,3)])) newdat$fit &lt;- apply(fitmat, 1, median) newdat$lwr &lt;- apply(fitmat, 1, quantile, prob=0.025) newdat$upr &lt;- apply(fitmat, 1, quantile, prob=0.975) plot(newdat$age, newdat$fit, ylim=c(0.8,1), type=&quot;l&quot;, las=1, ylab=&quot;Daily nest survival&quot;, xlab=&quot;Age [d]&quot;) lines(newdat$age, newdat$lwr, lty=3) lines(newdat$age, newdat$upr, lty=3) Figure 24.1: Estimated daily nest survival probability in relation to nest age. Dotted lines are 95% uncertainty intervals of the regression line. 24.8 Known fate model for irregular nest controls When nest are controlled only irregularly, it may happen that a nest is found predated or dead after a longer break in controlling. In such cases, we know that the nest was predated or it died due to other causes some when between the last control when the nest was still alive and when it was found dead. In such cases, we need to tell the model that the nest could have died any time during the interval when we were not controlling. To do so, we create a variable that indicates the time (e.g. day since first egg) when the nest was last seen alive (lastlive). A second variable indicates the time of the last check which is either the equal to lastlive when the nest survived until the last check, or it is larger than lastlive when the nest failure has been recorded. A last variable, gap, measures the time interval in which the nest failure occurred. A gap of zero means that the nest was still alive at the last control, a gapof 1 means that the nest failure occurred during the first day after lastlive, a gap of 2 means that the nest failure either occurred at the first or second day after lastlive. # time when nest was last observed alive lastlive &lt;- apply(datax$y, 1, function(x) max(c(1:length(x))[x==1])) # time when nest was last checked (alive or dead) lastcheck &lt;- lastlive+1 # here, we turn the above data into a format that can be used for # irregular nest controls. WOULD BE NICE TO HAVE A REAL DATA EXAMPLE! # when nest was observed alive at the last check, then lastcheck equals lastlive lastcheck[lastlive==datax$last] &lt;- datax$last[lastlive==datax$last] datax1 &lt;- list(Nnests=datax$Nnests, lastlive = lastlive, lastcheck= lastcheck, first=datax$first, cover=datax$cover, age=datax$age, maxage=datax$maxage) # time between last seen alive and first seen dead (= lastcheck) datax1$gap &lt;- datax1$lastcheck-datax1$lastlive In the Stan model code, we specify the likelihood for each gap separately. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; lastlive[Nnests]; // day of last observation (alive) int&lt;lower=0&gt; lastcheck[Nnests]; // day of observed death or, if alive, last day of study int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date int&lt;lower=0&gt; gap[Nnests]; // obsdead - lastlive } parameters { vector[3] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(lastcheck[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t]); } } // priors b[1]~normal(0,1.5); b[2]~normal(0,3); b[3]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):lastlive[i]){ 1~bernoulli(S[i,t-1]); } if(gap[i]==1){ target += log(1-S[i,lastlive[i]]); // } if(gap[i]==2){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1])); // } if(gap[i]==3){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1]) + prod(S[i,lastlive[i]:(lastlive[i]+1)])*(1-S[i,lastlive[i]+2])); // } if(gap[i]==4){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1]) + prod(S[i,lastlive[i]:(lastlive[i]+1)])*(1-S[i,lastlive[i]+2]) + prod(S[i,lastlive[i]:(lastlive[i]+2)])*(1-S[i,lastlive[i]+3])); // } } } # Run STAN mod1 &lt;- stan(file = &quot;stanmodels/daily_nest_survival_irreg.stan&quot;, data=datax1, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) Further reading Helpful links: https://deepai.org/publication/bayesian-survival-analysis-using-the-rstanarm-r-package (Brilleman et al. 2020) https://www.hammerlab.org/2017/06/26/introducing-survivalstan/ "],["cjs_with_mix.html", "25 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 25.1 Introduction 25.2 Data description 25.3 Model description 25.4 The Stan code 25.5 Call Stan from R, check convergence and look at results", " 25 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 25.1 Introduction In some species the identification of the sex is not possible for all individuals without sampling DNA. For example, morphological dimorphism is absent or so weak that parts of the individuals cannot be assigned to one of the sexes. Particularly in ornithological long-term capture recapture data sets that typically are obtained by voluntary bird ringers who do normaly not have the possibilities to analyse DNA, often the sex identification is missing in parts of the individuals. For estimating survival, it would nevertheless be valuable to include data of all individuals, use the information on sex-specific effects on survival wherever possible but account for the fact that of parts of the individuals the sex is not known. We here explain how a Cormack-Jolly-Seber model can be integrated with a mixture model in oder to allow for a combined analyses of individuals with and without sex identified. An introduction to the Cormack-Jolly-Seber model we gave in Chapter 14.5 of the book Korner-Nievergelt et al. (2015). We here expand this model by a mixture structure that allows including individuals with a missing categorical predictor variable, such as sex. 25.2 Data description ## simulate data # true parameter values theta &lt;- 0.6 # proportion of males nocc &lt;- 15 # number of years in the data set b0 &lt;- matrix(NA, ncol=nocc-1, nrow=2) b0[1,] &lt;- rbeta((nocc-1), 3, 4) # capture probability of males b0[2,] &lt;- rbeta((nocc-1), 2, 4) # capture probability of females a0 &lt;- matrix(NA, ncol=2, nrow=2) a1 &lt;- matrix(NA, ncol=2, nrow=2) a0[1,1]&lt;- qlogis(0.7) # average annual survival for adult males a0[1,2]&lt;- qlogis(0.3) # average annual survival for juveniles a0[2,1] &lt;- qlogis(0.55) # average annual survival for adult females a0[2,2] &lt;- a0[1,2] a1[1,1] &lt;- 0 a1[1,2] &lt;- -0.5 a1[2,1] &lt;- -0.8 a1[2,2] &lt;- a1[1,2] nindi &lt;- 1000 # number of individuals with identified sex nindni &lt;- 1500 # number of individuals with non-identified sex nind &lt;- nindi + nindni # total number of individuals y &lt;- matrix(ncol=nocc, nrow=nind) z &lt;- matrix(ncol=nocc, nrow=nind) first &lt;- sample(1:(nocc-1), nind, replace=TRUE) sex &lt;- sample(c(1,2), nind, prob=c(theta, 1-theta), replace=TRUE) juvfirst &lt;- sample(c(0,1), nind, prob=c(0.5, 0.5), replace=TRUE) juv &lt;- matrix(0, nrow=nind, ncol=nocc) for(i in 1:nind) juv[i,first[i]] &lt;- juv[i] x &lt;- runif(nocc-1, -2, 2) # a time dependent covariate covariate p &lt;- b0 # recapture probability phi &lt;- array(NA, dim=c(2, 2, nocc-1)) # for ad males phi[1,1,] &lt;- plogis(a0[1,1]+a1[1,1]*x) # for ad females phi[2,1,] &lt;- plogis(a0[2,1]+a1[2,1]*x) # for juvs phi[1,2,] &lt;- phi[2,2,] &lt;- plogis(a0[2,2]+a1[2,2]*x) for(i in 1:nind){ z[i,first[i]] &lt;- 1 y[i, first[i]] &lt;- 1 for(t in (first[i]+1):nocc){ z[i, t] &lt;- rbinom(1, size=1, prob=z[i,t-1]*phi[sex[i],juv[i,t-1]+1, t-1]) y[i, t] &lt;- rbinom(1, size=1, prob=z[i,t]*p[sex[i],t-1]) } } y[is.na(y)] &lt;- 0 The mark-recapture data set consists of capture histories of 2500 individuals over 15 time periods. For each time period \\(t\\) and individual \\(i\\) the capture history matrix \\(y\\) contains \\(y_{it}=1\\) if the individual \\(i\\) is captured during time period \\(t\\), or \\(y_{it}=0\\) if the individual \\(i\\) is not captured during time period \\(t\\). The marking time period varies between individuals from 1 to 14. At the marking time period, the age of the individuals was classified either as juvenile or as adult. Juveniles turn into adults after one time period, thus age is known for all individuals during all time periods after marking. For 1000 individuals of the 2500 individuals, the sex is identified, whereas for 1500 individuals, the sex is unknown. The example data contain one covariate \\(x\\) that takes on one value for each time period. # bundle the data for Stan i &lt;- 1:nindi ni &lt;- (nindi+1):nind datax &lt;- list(yi=y[i,], nindi=nindi, sex=sex[i], nocc=nocc, yni=y[ni,], nindni=nindni, firsti=first[i], firstni=first[ni], juvi=juv[i,]+1, juvni=juv[ni,]+1, year=1:nocc, x=x) 25.3 Model description The observations \\(y_{it}\\), an indicator of whether individual i was recaptured during time period \\(t\\) is modelled conditional on the latent true state of the individual birds \\(z_{it}\\) (0 = dead or permanently emigrated, 1 = alive and at the study site) as a Bernoulli variable. The probability \\(P(y_{it} = 1)\\) is the product of the probability that an alive individual is recaptured, \\(p_{it}\\), and the state of the bird \\(z_{it}\\) (alive = 1, dead = 0). Thus, a dead bird cannot be recaptured, whereas for a bird alive during time period \\(t\\), the recapture probability equals \\(p_{it}\\): \\[y_{it} \\sim Bernoulli(z_{it}p_{it})\\] The latent state variable \\(z_{it}\\) is a Markovian variable with the state at time \\(t\\) being dependent on the state at time \\(t-1\\) and the apparent survival probability \\[\\phi_{it}\\]: \\[z_{it} \\sim Bernoulli(z_{it-1}\\phi_{it})\\] We use the term apparent survival in order to indicate that the parameter \\(\\phi\\) is a product of site fidelity and survival. Thus, individuals that permanently emigrated from the study area cannot be distinguished from dead individuals. In both models, the parameters \\(\\phi\\) and \\(p\\) were modelled as sex-specific. However, for parts of the individuals, sex could not be identified, i.e. sex was missing. Ignoring these missing values would most likely lead to a bias because they were not missing at random. The probability that sex can be identified is increasing with age and most likely differs between sexes. Therefore, we included a mixture model for the sex: \\[Sex_i \\sim Categorical(q_i)\\] where \\(q_i\\) is a vector of length 2, containing the probability of being a male and a female, respectively. In this way, the sex of the non-identified individuals was assumed to be male or female with probability \\(q[1]\\) and \\(q[2]=1-q[1]\\), respectively. This model corresponds to the finite mixture model introduced by Pledger, Pollock, and Norris (2003) in order to account for unknown classes of birds (heterogeneity). However, in our case, for parts of the individuals the class (sex) was known. In the example model, we constrain apparent survival to be linearly dependent on a covariate x with different slopes for males, females and juveniles using the logit link function. \\[logit(\\phi_{it}) = a0_{sex-age-class[it]} + a1_{sex-age-class[it]}x_i\\] Annual recapture probability was modelled for each year and age and sex class independently: \\[p_{it} = b0_{t,sex-age-class[it]}\\] Uniform prior distributions were used for all parameters with a parameter space limited to values between 0 and 1 (probabilities) and a normal distribution with a mean of 0 and a standard deviation of 1.5 for the intercept \\(a0\\), and a standard deviation of 5 was used for \\(a1\\). 25.4 The Stan code The trick for coding the CMR-mixture model in Stan is to formulate the model 3 times: 1. For the individuals with identified sex 2. For the males that were not identified 3. For the females that were not identified Then for the non-identified individuals a mixture model is formulated that assigns a probability of being a female or a male to each individual. data { int&lt;lower=2&gt; nocc; // number of capture events int&lt;lower=0&gt; nindi; // number of individuals with identified sex int&lt;lower=0&gt; nindni; // number of individuals with non-identified sex int&lt;lower=0,upper=2&gt; yi[nindi,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firsti[nindi]; // year of first capture int&lt;lower=0,upper=2&gt; yni[nindni,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firstni[nindni]; // year of first capture int&lt;lower=1, upper=2&gt; sex[nindi]; int&lt;lower=1, upper=2&gt; juvi[nindi, nocc]; int&lt;lower=1, upper=2&gt; juvni[nindni, nocc]; int&lt;lower=1&gt; year[nocc]; real x[nocc-1]; // a covariate } transformed data { int&lt;lower=0,upper=nocc+1&gt; lasti[nindi]; // last[i]: ind i last capture int&lt;lower=0,upper=nocc+1&gt; lastni[nindni]; // last[i]: ind i last capture lasti = rep_array(0,nindi); lastni = rep_array(0,nindni); for (i in 1:nindi) { for (k in firsti[i]:nocc) { if (yi[i,k] == 1) { if (k &gt; lasti[i]) lasti[i] = k; } } } for (ii in 1:nindni) { for (kk in firstni[ii]:nocc) { if (yni[ii,kk] == 1) { if (kk &gt; lastni[ii]) lastni[ii] = kk; } } } } parameters { real&lt;lower=0, upper=1&gt; theta[nindni]; // probability of being male for non-identified individuals real&lt;lower=0, upper=1&gt; b0[2,nocc-1]; // intercept of p real a0[2,2]; // intercept for phi real a1[2,2]; // coefficient for phi } transformed parameters { real&lt;lower=0,upper=1&gt;p_male[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p_female[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p[nindi,nocc]; // capture probability real&lt;lower=0,upper=1&gt;phi_male[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_male[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi_female[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_female[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi[nindi,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi[nindi,nocc+1]; // probability that an individual // is never recaptured after its // last capture { int k; int kk; for(ii in 1:nindi){ if (firsti[ii]&gt;1) { for (z in 1:(firsti[ii]-1)){ phi[ii,z] = 1; } } for(tt in firsti[ii]:(nocc-1)) { // linear predictor for phi: phi[ii,tt] = inv_logit(a0[sex[ii], juvi[ii,tt]] + a1[sex[ii], juvi[ii,tt]]*x[tt]); } } for(ii in 1:nindni){ if (firstni[ii]&gt;1) { for (z in 1:(firstni[ii]-1)){ phi_female[ii,z] = 1; phi_male[ii,z] = 1; } } for(tt in firstni[ii]:(nocc-1)) { // linear predictor for phi: phi_male[ii,tt] = inv_logit(a0[1, juvni[ii,tt]] + a1[1, juvni[ii,tt]]*x[tt]); phi_female[ii,tt] = inv_logit(a0[2, juvni[ii,tt]]+ a1[2, juvni[ii,tt]]*x[tt]); } } for(i in 1:nindi) { // linear predictor for p for identified individuals for(w in 1:firsti[i]){ p[i,w] = 1; } for(kkk in (firsti[i]+1):nocc) p[i,kkk] = b0[sex[i],year[kkk-1]]; chi[i,nocc+1] = 1.0; k = nocc; while (k &gt; firsti[i]) { chi[i,k] = (1 - phi[i,k-1]) + phi[i,k-1] * (1 - p[i,k]) * chi[i,k+1]; k = k - 1; } if (firsti[i]&gt;1) { for (u in 1:(firsti[i]-1)){ chi[i,u] = 0; } } chi[i,firsti[i]] = (1 - p[i,firsti[i]]) * chi[i,firsti[i]+1]; }// close definition of transformed parameters for identified individuals for(i in 1:nindni) { // linear predictor for p for non-identified individuals for(w in 1:firstni[i]){ p_male[i,w] = 1; p_female[i,w] = 1; } for(kkkk in (firstni[i]+1):nocc){ p_male[i,kkkk] = b0[1,year[kkkk-1]]; p_female[i,kkkk] = b0[2,year[kkkk-1]]; } chi_male[i,nocc+1] = 1.0; chi_female[i,nocc+1] = 1.0; k = nocc; while (k &gt; firstni[i]) { chi_male[i,k] = (1 - phi_male[i,k-1]) + phi_male[i,k-1] * (1 - p_male[i,k]) * chi_male[i,k+1]; chi_female[i,k] = (1 - phi_female[i,k-1]) + phi_female[i,k-1] * (1 - p_female[i,k]) * chi_female[i,k+1]; k = k - 1; } if (firstni[i]&gt;1) { for (u in 1:(firstni[i]-1)){ chi_male[i,u] = 0; chi_female[i,u] = 0; } } chi_male[i,firstni[i]] = (1 - p_male[i,firstni[i]]) * chi_male[i,firstni[i]+1]; chi_female[i,firstni[i]] = (1 - p_female[i,firstni[i]]) * chi_female[i,firstni[i]+1]; } // close definition of transformed parameters for non-identified individuals } // close block of transformed parameters exclusive parameter declarations } // close transformed parameters model { // priors theta ~ beta(1, 1); for (g in 1:(nocc-1)){ b0[1,g]~beta(1,1); b0[2,g]~beta(1,1); } a0[1,1]~normal(0,1.5); a0[1,2]~normal(0,1.5); a1[1,1]~normal(0,3); a1[1,2]~normal(0,3); a0[2,1]~normal(0,1.5); a0[2,2]~normal(a0[1,2],0.01); // for juveniles, we assume that the effect of the covariate is independet of sex a1[2,1]~normal(0,3); a1[2,2]~normal(a1[1,2],0.01); // likelihood for identified individuals for (i in 1:nindi) { if (lasti[i]&gt;0) { for (k in firsti[i]:lasti[i]) { if(k&gt;1) target+= (log(phi[i, k-1])); if (yi[i,k] == 1) target+=(log(p[i,k])); else target+=(log1m(p[i,k])); } } target+=(log(chi[i,lasti[i]+1])); } // likelihood for non-identified individuals for (i in 1:nindni) { real log_like_male = 0; real log_like_female = 0; if (lastni[i]&gt;0) { for (k in firstni[i]:lastni[i]) { if(k&gt;1){ log_like_male += (log(phi_male[i, k-1])); log_like_female += (log(phi_female[i, k-1])); } if (yni[i,k] == 1){ log_like_male+=(log(p_male[i,k])); log_like_female+=(log(p_female[i,k])); } else{ log_like_male+=(log1m(p_male[i,k])); log_like_female+=(log1m(p_female[i,k])); } } } log_like_male += (log(chi_male[i,lastni[i]+1])); log_like_female += (log(chi_female[i,lastni[i]+1])); target += log_mix(theta[i], log_like_male, log_like_female); } } 25.5 Call Stan from R, check convergence and look at results # Run STAN library(rstan) fit &lt;- stan(file = &quot;stanmodels/cmr_mixture_model.stan&quot;, data=datax, verbose = FALSE) # for above simulated data (25000 individuals x 15 time periods) # computing time is around 48 hours on an intel corei7 laptop # for larger data sets, we recommed moving the transformed parameters block # to the model block in order to avoid monitoring of p_male, p_female, # phi_male and phi_female producing memory problems # launch_shinystan(fit) # diagnostic plots summary(fit) ## mean se_mean sd 2.5% 25% ## b0[1,1] 0.60132367 0.0015709423 0.06173884 0.48042366 0.55922253 ## b0[1,2] 0.70098709 0.0012519948 0.04969428 0.60382019 0.66806698 ## b0[1,3] 0.50293513 0.0010904085 0.04517398 0.41491848 0.47220346 ## b0[1,4] 0.28118209 0.0008809447 0.03577334 0.21440931 0.25697691 ## b0[1,5] 0.34938289 0.0009901335 0.03647815 0.27819918 0.32351323 ## b0[1,6] 0.13158569 0.0006914740 0.02627423 0.08664129 0.11286629 ## b0[1,7] 0.61182981 0.0010463611 0.04129602 0.53187976 0.58387839 ## b0[1,8] 0.48535193 0.0010845951 0.04155762 0.40559440 0.45750793 ## b0[1,9] 0.52531291 0.0008790063 0.03704084 0.45247132 0.50064513 ## b0[1,10] 0.87174780 0.0007565552 0.03000936 0.80818138 0.85259573 ## b0[1,11] 0.80185454 0.0009425675 0.03518166 0.73173810 0.77865187 ## b0[1,12] 0.33152443 0.0008564381 0.03628505 0.26380840 0.30697293 ## b0[1,13] 0.42132288 0.0012174784 0.04140382 0.34062688 0.39305210 ## b0[1,14] 0.65180372 0.0015151039 0.05333953 0.55349105 0.61560493 ## b0[2,1] 0.34237039 0.0041467200 0.12925217 0.12002285 0.24717176 ## b0[2,2] 0.18534646 0.0023431250 0.07547704 0.05924694 0.12871584 ## b0[2,3] 0.61351083 0.0024140550 0.07679100 0.46647727 0.56242546 ## b0[2,4] 0.37140208 0.0024464965 0.06962399 0.24693888 0.32338093 ## b0[2,5] 0.19428215 0.0034618302 0.11214798 0.02800056 0.11146326 ## b0[2,6] 0.27371336 0.0026553769 0.09054020 0.11827243 0.20785316 ## b0[2,7] 0.18611173 0.0014387436 0.05328492 0.09122869 0.14789827 ## b0[2,8] 0.25648337 0.0018258589 0.05287800 0.16255769 0.21913271 ## b0[2,9] 0.20378754 0.0021367769 0.07380004 0.07777998 0.15215845 ## b0[2,10] 0.52679548 0.0024625568 0.08696008 0.36214334 0.46594844 ## b0[2,11] 0.47393354 0.0032593161 0.10555065 0.28843967 0.39781278 ## b0[2,12] 0.22289155 0.0017082729 0.05551514 0.12576797 0.18203335 ## b0[2,13] 0.26191486 0.0024159794 0.07016314 0.14106495 0.21234017 ## b0[2,14] 0.65111737 0.0055743944 0.18780555 0.29279480 0.50957591 ## a0[1,1] 0.95440670 0.0013771881 0.04808748 0.86301660 0.92146330 ## a0[1,2] 0.01529770 0.0469699511 1.46995922 -2.82218067 -0.95533706 ## a0[2,1] 0.16384995 0.0049928331 0.12634422 -0.06399631 0.07533962 ## a0[2,2] 0.01535679 0.0469634175 1.47006964 -2.81864060 -0.95515751 ## a1[1,1] 0.15937249 0.0028992587 0.08864790 -0.01288607 0.10017613 ## a1[1,2] 0.08055953 0.1007089857 3.02148727 -5.95525636 -1.96662599 ## a1[2,1] -0.83614134 0.0074143920 0.18655882 -1.21033848 -0.95698565 ## a1[2,2] 0.08071668 0.1006904255 3.02145647 -5.94617355 -1.96508733 ## 50% 75% 97.5% n_eff Rhat ## b0[1,1] 0.60206306 0.6431566 0.7206343 1544.5301 1.002331 ## b0[1,2] 0.70165494 0.7355204 0.7946280 1575.4617 1.001482 ## b0[1,3] 0.50367411 0.5330078 0.5898079 1716.3196 1.001183 ## b0[1,4] 0.27997512 0.3046483 0.3544592 1649.0040 1.000760 ## b0[1,5] 0.34936442 0.3751935 0.4191138 1357.3073 1.002072 ## b0[1,6] 0.12987449 0.1481661 0.1873982 1443.8040 1.003676 ## b0[1,7] 0.61203228 0.6397577 0.6933929 1557.5904 1.001458 ## b0[1,8] 0.48513822 0.5134314 0.5672066 1468.1355 1.002511 ## b0[1,9] 0.52534212 0.5501747 0.5994060 1775.7335 1.000824 ## b0[1,10] 0.87324112 0.8934047 0.9258033 1573.3747 1.000719 ## b0[1,11] 0.80300311 0.8261868 0.8675033 1393.1817 1.001172 ## b0[1,12] 0.33044476 0.3552199 0.4052902 1794.9956 1.000566 ## b0[1,13] 0.42116690 0.4492297 0.5026942 1156.5339 1.000289 ## b0[1,14] 0.64956850 0.6864706 0.7607107 1239.4056 1.004061 ## b0[2,1] 0.33493631 0.4251416 0.6150923 971.5524 1.004049 ## b0[2,2] 0.17981663 0.2358847 0.3446097 1037.6210 1.001474 ## b0[2,3] 0.61326419 0.6644156 0.7628427 1011.8737 1.005727 ## b0[2,4] 0.36837778 0.4158585 0.5190457 809.8949 1.003803 ## b0[2,5] 0.17910449 0.2591418 0.4533117 1049.4733 1.001499 ## b0[2,6] 0.26739172 0.3299594 0.4685139 1162.6006 1.001170 ## b0[2,7] 0.18254607 0.2198969 0.3003156 1371.6455 1.000878 ## b0[2,8] 0.25280556 0.2895585 0.3704113 838.7174 1.005624 ## b0[2,9] 0.19724053 0.2501298 0.3694806 1192.8747 1.003687 ## b0[2,10] 0.52587075 0.5845730 0.7061694 1247.0027 1.002851 ## b0[2,11] 0.46874445 0.5392302 0.7046892 1048.7425 0.999473 ## b0[2,12] 0.21961656 0.2580782 0.3397127 1056.1081 1.000907 ## b0[2,13] 0.25601959 0.3056204 0.4142888 843.3960 1.003130 ## b0[2,14] 0.65824835 0.7973674 0.9698829 1135.0669 1.003838 ## a0[1,1] 0.95368445 0.9862439 1.0515747 1219.2071 1.003898 ## a0[1,2] 0.01633534 0.9911055 2.9717839 979.4231 1.003726 ## a0[2,1] 0.15519648 0.2472483 0.4230776 640.3489 1.004625 ## a0[2,2] 0.01587281 0.9898084 2.9659552 979.8429 1.003744 ## a1[1,1] 0.15647489 0.2205720 0.3354845 934.8953 1.007190 ## a1[1,2] 0.06683287 2.1568781 6.0295208 900.1297 1.003701 ## a1[2,1] -0.83503982 -0.7075691 -0.4814539 633.1119 1.010568 ## a1[2,2] 0.06586905 2.1557247 6.0239735 900.4432 1.003704 "],["referenzen.html", "Referenzen", " Referenzen "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
