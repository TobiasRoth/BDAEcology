[["index.html", "Bayesian Data Analysis in Ecology with R and Stan Preface Why this book? About this book How to contribute? Acknowledgments", " Bayesian Data Analysis in Ecology with R and Stan Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Pius Korner-Nievergelt 2022-12-21 Preface Why this book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (Korner-Nievergelt et al. 2015). You can order it here. People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. About this book We do not copy text from the book into the e-book. Therefore, we refer to the book (Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. While we show the R-code behind most of the analyses, we sometimes choose not to show all the code in the html version of the book. This is particularly the case for some of the illustrations. An intrested reader can always consult the public GitHub repository with the rmarkdown-files that were used to generate the book. How to contribute? It is open so that everybody with a GitHub account can make comments and suggestions for improvement. Readers can contribute in two ways. One way is to add an issue. The second way is to contribute content directly through the edit button at the top of the page (i.e. a symbol showing a pencil in a square). That button is linked to the rmarkdown source file of each page. You can correct typos or add new text and then submit a GitHub pull request. We try to respond to you as quickly as possible. We are looking forward to your contribution! Acknowledgments We thank Yihui Xie for providing bookdown which makes it much fun to write open books such as ours. We thank many anonymous students and collaborators who searched information on new software, reported updates and gave feedback on earlier versions of the book. Specifically, we thank Carole Niffenegger for looking up the difference between the bulk and tail ESS in the brm output, Martin Küblbeck for using the conditional logistic regression in rstanarm,  "],["PART-I.html", "1 Introduction to PART I 1.1 Further reading", " 1 Introduction to PART I During our courses we are sometimes asked to give an introduction to some R-related stuff covering data analysis, presentation of results or rather specialist topics in ecology. In this part we present collected these introduction and try to keep them updated. This is also a commented collection of R-code that we documented for our own work. We hope this might be useful olso for other readers. 1.1 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],["basics.html", "2 Prerequisits: Basic statistical terms 2.1 Variables and observations 2.2 Displaying and summarizing variables 2.3 Inferential statistics 2.4 Bayes theorem and the common aim of frequentist and Bayesian methods 2.5 Classical frequentist tests and alternatives 2.6 Summary", " 2 Prerequisits: Basic statistical terms This chapter introduces some important terms useful for doing data analyses. It also introduces the essentials of the classical frequentist tests such as t- and Chisquare test. Even though we will not use nullhypotheses tests later (Amrhein, Greenland, and McShane 2019), we introduce them here because we need to understand 100 years of scientific literature. For each classical test, we provide a suggestion how to do it in a Bayesian way and we discuss some differences between the Bayesian and frequentist statistics. 2.1 Variables and observations Empirical research involves data collection. Data are collected by recording measurements of variables for observational units. An observational unit may be, for example, an individual, a plot, a time interval or a combination of those. The collection of all units ideally build a random sample of the entire population of units in that we are interested. The measurements (or observations) of the random sample are stored in a data table (sometimes also called data set, but a data set may include several data tables. A collection of data tables belonging to the same study or system is normally bundled and stored in a data base). A data table is a collection of variables (columns). Data tables normally are handled as objects of class data.frame in R. All measurements on a row in a data table belong to the same observational unit. The variables can be of different scales (Table 2.1). Table 2.1: Scales of measurements Scale Examples Properties Coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Elevational zones Identity and magnitude (values have an ordered relationship) ordered() Numeric Discrete: counts; continuous: body weight, wing length Identity, magnitude, and intervals or ratios intgeger() numeric() The aim of many studies is to describe how a variable of interest (y) is related to one or more predictor variables (x). How these variables are named differs between authors. The y-variable is called outcome variable, response or dependent variable. The x-variables are called predictors, explanatory variables or independent variables. The choose of the terms for x and y is a matter of taste. We avoid the terms dependent and independent variables because often we do not know whether the variable y is in fact depending on the x variables and also, often the x-variables are not independent of each other. In this book, we try to use outcome and predictor variables because these terms sound most neutral to us in that they refer to how the statistical model is constructed rather than to a real life relationship. 2.2 Displaying and summarizing variables While nominal and ordinal variables are summarized by giving the absolute number or the proportion of observations for each category, numeric variables normally are summarized by a location and a scatter statistics, such as the mean and the standard deviation or the median and some quantiles. The distribution of a numeric variable is graphically displayed in a histogram (Fig. 2.1). Figure 2.1: Histogram of the length of ell of statistics course participants. To draw a histogram, the variable is displayed on the x-axis and the \\(x_i\\)-values are assigned to classes. The edges of the classes are called breaks. They can be set with the argument breaks= within the function hist. The values given in the breaks= argument must at least span the values of the variable. If the argument breaks= is not specified, R searches for breaks-values that make the histogram look smooth. The number of observations falling in each class is given on the y-axis. The y-axis can be re-scaled so that the area of the histogram equals 1 by setting the argument density=TRUE. In that case, the values on the y-axis correspond to the density values of a probability distribution (Chapter 4). Location statistics are mean, median or mode. A common mean is the - arithmetic mean: \\(\\hat{\\mu} = \\bar{x} = \\frac{i=1}{n} x_i \\sum_{1}^{n}\\)(R function mean), where \\(n\\) is the sample size. The parameter \\(\\mu\\) is the (unknown) true mean of the entire population of which the \\(1,...,n\\) measurements are a random sample of. \\(\\bar{x}\\) is called the sample mean and used as an estimate for \\(\\mu\\). The \\(^\\) above any parameter indicates that the parameter value is obtained from a sample and, therefore, it may be different from the true value. The median is the 50% quantile. We find 50% of the measurements below and 50% above the median. If \\(x_1,..., x_n\\) are the ordered measurements of a variable, then the median is: - median \\(= x_{(n+1)/2}\\) for uneven \\(n\\), and median \\(= \\frac{1}{2}(x_{n/2} + x_{n/2+1})\\) for even \\(n\\) (R function median). The mode is the value that is occurring with highest frequency or that has the highest density. Scatter also is called spread, scale or variance. Variance parameters describe how far away from the location parameter single observations can be found, or how the measurements are scattered around their mean. The variance is defined as the average squared difference between the observations and the mean: variance \\(\\hat{\\sigma^2} = s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) The term \\((n-1)\\) is called the degrees of freedom. It is used in the denominator of the variance formula instead of \\(n\\) to prevent underestimating the variance. Because \\(\\bar{x}\\) is in average closer to \\(x_i\\) than the unknown true mean \\(\\mu\\) would be, the variance would be underestimated if \\(n\\) is used in the denominator. The maximum likelihood estimate (Chapter xxx.xx) of the variance corresponds to the variance formula using \\(n\\) instead of \\(n-1\\) in the denominator, see, e.g., Royle and Dorazio (2008). The variance is used to compare the degree of scatter among different groups. However, its values are difficult to interpret because of the squared unit. Therefore, the square root of the variance, the standard deviation is normally reported: standard deviation \\(\\hat{\\sigma} = s = \\sqrt{s^2}\\) (R Function sd) The standard deviation is approximately the average deviation of an observation from the sample mean. In the case of a normal distribution, about two thirds (68%) of the data are expected within one standard deviation around the mean. The variance and standard deviation each describe the scatter with a single value. Thus, we have to assume that the observations are scattered symmetrically around their mean in order to get a picture of the distribution of the measurements. When the measurements are spread asymmetrically (skewed distribution), then it may be more precise to describe the scatter with more than one value. Such statistics could be quantiles from the lower and upper tail of the data. Quantiles inform us about both location and spread of a distribution. The \\(p\\)th-quantile is the value with the property that a proportion \\(p\\) of all values are less than or equal to the value of the quantile. The median is the 50% quantile.The 25% quantile and the 75% quantile are also called the lower and upper quartiles, respectively. The range between the 25% and the 75% quartile is called the interquartile range. This range includes 50% of the distribution and is also used as a measure of scatter. The R function quantile extracts sample quantiles. The median, the quartiles, and the interquartile range can be graphically displayed using box and-whisker plots (boxplots in short, R function boxplot). The horizontal fat bars are the medians (Fig. 2.2). The boxes mark the interquartile range. The whiskers reach out to the last observation within 1.5 times the interquartile range from the quartile. Circles mark observations beyond 1.5 times the interquartile range from the quartile. par(mar=c(5,4,1,1)) boxplot(ell~car, data=dat, las=1, ylab=&quot;Lenght of ell [cm]&quot;, col=&quot;tomato&quot;, xaxt=&quot;n&quot;) axis(1, at=c(1,2), labels=c(&quot;Not owing a car&quot;, &quot;Car owner&quot;)) n &lt;- table(dat$car) axis(1, at=c(1,2), labels=paste(&quot;n=&quot;, n, sep=&quot;&quot;), mgp=c(3,2, 0)) Figure 2.2: Boxplot of the length of ell of statistics course participants who are or ar not owner of a car. The boxplot is an appealing tool for comparing location, variance and distribution of measurements among groups. 2.2.1 Correlations A correlation measures the strength with which characteristics of two variables are associated with each other (co-occur). If both variables are numeric, we can visualize the correlation using a scatterplot. par(mar=c(5,4,1,1)) plot(temp~ell, data=dat, las=1, xlab=&quot;Lenght of ell [cm]&quot;, ylab=&quot;Comfort temperature [°C]&quot;, pch=16) Figure 2.3: Scatterplot of the length of ell and the comfort temperature of statistics course participants. The covariance between variable \\(x\\) and \\(y\\) is defined as: covariance \\(q = \\frac{1}{n-1}\\sum_{i=1}^{n}((x_i-\\bar{x})*(y_i-\\bar{y}))\\) (R function cov) As for the variance, also the units of the covariance are sqared and therefore covariance values are difficult to interpret. A standardized covariance is the Pearson correlation coefficient: Pearson correlation coefficient: \\(r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\) (R function cor) Means, variances, standard deviations, covariances and correlations are sensible for outliers. Single observations containing extreme values normally have a overproportional influence on these statistics. When outliers are present in the data, we may prefer a more robust correlation measure such as the Spearman correlation or Kendalls tau. Both are based on the ranks of the measurements instead of the measurements themselves. Spearman correlation coefficient: correlation between rank(x) and rank(y) (R function cor(x,y, method=\"spearman\")) Kendalls tau: \\(\\tau = 1-\\frac{4I}{(n(n-1))}\\), where \\(I\\) = number of pairs \\((i,k)\\) for which \\((x_i &lt; x_k)\\) &amp; \\((y_i &gt; y_k)\\) or viceversa. (R function cor(x,y, method=\"kendall\")) 2.2.2 Principal components analyses PCA The principal components analysis (PCA) is a multivariate correlation analysis. A multidimensional data set with \\(k\\) variables can be seen as a cloud of points (observations) in a \\(k\\)-dimensional space. Imagine, we could move around in the space and look at the cloud from different locations. From some locations, the data looks highly correlated, whereas from others, we cannot see the correlation. That is what PCA is doing. It is rotating the coordinate system (defined by the original variables) of the data cloud so that the correlations are no longer visible. The axes of the new coordinates system are linear combinations of the original variables. They are called principal components. There are as many principal coordinates as there are original variables, i.e. \\(k\\), \\(p_1, ..., p_k\\). The principal components meet further requirements: the first component explains most variance the second component explains most of the remaining variance and is perpendicular (= uncorrelated) to the first one third component explains most of the remaining variance and is perpendicular to the first two  For example, in a two-dimensional data set \\((x_1, x_2)\\) the principal components become \\(pc_{1i} = b_{11}x_{1i} + b_{12}x_{2i}\\) \\(pc_{2i} = b_{21}x_{1i} + b_{22}x_{2i}\\) with \\(b_{jk}\\) being loadings of principal component \\(j\\) and original variable \\(k\\). Fig. 2.4 shows the two principal components for a two-dimensional data set. They can be calculated using matrix algebra: principal components are eigenvectors of the covariance or correlation matrix. Figure 2.4: Principal components of a two dimensional data set based on the covariance matrix (green) and the correlation matrix (brown). The choice between correlation or covariance matrix is essential and important. The covariance matrix is an unstandardized correlation matrix. Therefore, the units, i.e., whether cm or m are used, influence the results of the PCA if it is based on the covariance matrix. When the PCA is based on the covariance matrix, the results will change, when we change the units of one variable, e.g., from cm to m. Basing the PCA on the covariance matrix only makes sense, when the variances are comparable among the variables, i.e., if all variables are measured in the same unit and we would like to weight each variable according to its variance. If this is not the case, the PCA must be based on the correlation matrix. pca &lt;- princomp(cbind(x1,x2)) # PCA based on covariance matrix pca &lt;- princomp(cbind(x1,x2), cor=TRUE) # PCA based on correlation matrix loadings(pca) ## ## Loadings: ## Comp.1 Comp.2 ## x1 0.707 0.707 ## x2 0.707 -0.707 ## ## Comp.1 Comp.2 ## SS loadings 1.0 1.0 ## Proportion Var 0.5 0.5 ## Cumulative Var 0.5 1.0 The loadings measure the correlation of each variable with the principal components. They inform about what aspects of the data each component is measuring. The signs of the loadings are arbitrary, thus we can multiplied them by -1 without changing the PCA. Sometimes this can be handy for describing the meaning of the principal component in a paper. For example, Zbinden et al. (2018) combined the number of hunting licenses, the duration of the hunting period and the number of black grouse cocks that were allowed to be hunted per hunter in a principal component in order to measure hunting pressure. All three variables had a negative loading in the first component, so that high values of the component meant low hunting pressure. Before the subsequent analyses, for which a measure of hunting pressure was of interest, the authors changed the signs of the loadings so that this component measured hunting pressure. The proportion of variance explained by each component is, beside the loadings, an important information. If the first few components explain the main part of the variance, it means that maybe not all \\(k\\) variables are necessary to describe the data, or, in other words, the original \\(k\\) variables contain a lot of redundant information. # extract the variance captured by each component summary(pca) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.2679406 0.6263598 ## Proportion of Variance 0.8038367 0.1961633 ## Cumulative Proportion 0.8038367 1.0000000 Ridge regression is similar to doing a PCA within a linear model while components with low variance are shrinked to a higher degree than components with a high variance. 2.3 Inferential statistics 2.3.1 Uncertainty there is never a yes-or-no answer there will always be uncertainty Amrhein (2017)[https://peerj.com/preprints/26857] The decision whether an effect is important or not cannot not be done based on data alone. For a decision we should carefully consider the consequences of each decision, the aims we would like to achieve and the data. Consequences, needs and wishes of different stakeholders can be formally combined with the information in data by using decision theoretical methods. In most data analyses, particularly in basic research and when working on case studies, we normally do not consider consequences of decisions. In these cases, our job is extracting the information of data so that this information later can be used by other scientists, stakeholders and politicians to make decisions. Therefore, statistics is describing pattern in data and quantifying the uncertainty of the described patterns that is due to the fact that the data is just a (small) random sample from the population we would like to know. Quantification of uncertainty is only possible if: 1. the mechanisms that generated the data are known 2. the observations are a random sample from the population of interest Most studies aim at understanding the mechanisms that generated the data, thus they are most likely not known. To overcome that problem, we construct models, e.g. statistical models, that are (strong) abstractions of the data generating process. And we report the model assumptions. All uncertainty measures are conditional on the model we used to analyze the data, i.e., they are only reliable, if the model we used somehow realistically describes the data generating process. Because most statistical models do not describe the data generating process well, the true uncertainty almost always is much higher than the one we report. In order to obtain a random sample from the population under study, a good study design is a prerequisite. To illustrate how inference about a big population is drawn from a small sample, we here use simulated data. The advantage of using simulated data is that the mechanism that generated the data is known. However, in the example, we use different models for simulation and analysis. Imagine there are 300000 PhD students on the world and we would like to know how many statistics courses they have taken before they started their PhD (Fig. 2.5). # simulate the virtual true population set.seed(235325) # set seed for random number generator # simulate fake data of the whole population # using an overdispersed Poisson distribution # There is no need to understand more of this model # at this moment of the course than that this model # produces integer numbers (counts). Poisson models # will be introduced later. statscourses &lt;- rpois(300000, rgamma(300000, 2, 3)) # draw a random sample from the population n &lt;- 12 # sample size y &lt;- sample(statscourses, 12, replace=FALSE) Figure 2.5: Histogram of the number of statistics courses 300000 virtual PhD students have taken before their PhD started. The rugs on the x-axis indicate a random sample of the 300000 students. The black vertical line indicates the mean of the 300000 students (true mean) and the blue line indicates the mean of the sample. We observe the sample mean, what do we know about the population mean? There are two different approaches to answer this question. 1) We could ask us, how much the sample mean would scatter, if we repeat the study many times? This approach is called the frequentist statistics. 2) We could ask us for any possible value, what is the probability that it is the true population mean? To do so, we use probability theory and that is called the Bayesian statistics. Both approaches use (essentially similar) models. Only the mathematical techniques to calculate uncertainty measures differ between the two approaches. In cases when beside the data no other information is used to construct the model, then the results are approximately identical (at least for large enough sample sizes). We illustrate what uncertainty intervals mean in Fig. 2.6. A frequentist 95% confidence interval (blue horizontal semgent in Fig. 2.6) is constructed such that, if you were to (hypothetically) repeat the experiment or sampling many many times, 95% of the intervals constructed would contain the true value of the parameter. From the Bayesian posterior distribution we could construct a 95% interval (e.g., by using the 2.5% and 97.5% quantiles). This interval has traditionally been called credible interval. It can be interpreted that we are 95% sure that the true mean is inside that interval. Both interpretations only are reliable if the model is a realistic abstraction of the data generating process (or if the model assumptions are realistic). Because both terms, confidence and credible interval, suggest that the interval indicates confidence or credibility but the intervals actually show uncertainty, it has been suggested to rename the interval into compatibility or uncertainty interval Andrew Gelman and Greenland (2019). Figure 2.6: Histogram of means of repeated samples from the true populations. The scatter of these means visualize the true uncertainty of the mean in this example. The blue vertical line indicates the mean of the original sample. The blue segment shows the 95% confidence interval (obtained by fequensist methods) and the violet line shows the posterior distribution of the mean (obtained by Bayesian methods). For both solutions, we assumed a Normal distribution for the data that is different from the true mechanism that generated the data (which was an overdispersed Poisson model. The star indicates the true mean. 2.3.2 Standard error The standard error SE is, beside the uncertainty interval, an alternative possibility to measure uncertainty. It measures an average deviation of the sample mean from the (unknown) true population mean. The frequentist method for obtaining the SE is based on the central limit theorem. According to the central limit theorem the sum of independent, not necessarily normally distributed random numbers are normally distributed when sample size is large enough (Chapter 4). Because the mean is a sum (divided by a constant, the sample size) it can be assumed that the distribution of many means of samples is normal. It can be mathematically shown that the standard error SE equals the standard deviation SD of the sample divided by the square root of the sample size: frequentist SE = SD/sqrt(n) = \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\) Bayesian SE: Using Bayesian methods, the SE is the SD of the posterior distribution. It is very important to keep the difference between SE and SD in mind! SD measures the scatter of the data, whereas SE measures the uncertainty of the mean (or of another estimated parameter, Fig. 2.7). SD is a descriptive statistics describing a characteristics of the data, whereas SE is an inferential statistics showing us how far away we possibly are from the true parameter value. When sample size increases, SE becomes smaller, whereas SD does not change (given the added observations are drawn at random from the same big population as the ones already in the sample). Figure 2.7: Illustration of the difference between SD and SE. The SD measures the scatter in the data (for the sample (green tickmarks) in green and for the true population (light grey histogram) in a thin black line. The SE measures the uncertainty of the sample mean (in blue for the sample mean and in bold black for the means of the repeated samples (dark grey histogram)). 2.4 Bayes theorem and the common aim of frequentist and Bayesian methods 2.4.1 Bayes theorem for discrete events The Bayes theorem describes the probability of event A conditional on event B (the probability of A after B has already occurred) from the probability of B conditional on A and the two probabilities of the events A and B: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) Imagine, event A is The person likes wine as a birthday present. and event B The person has no car.. The conditional probability of A given B is the probability that a person not owing a car likes wine. Answers from students whether they have a car and what they like as a birthday present are summarized in Table 2.2. Table 2.2: Cross table of the students birthday preference and car ownership. car/birthday flowers wine sum no car 6 9 15 car 1 6 7 sum 7 15 22 We can apply the Bayes theorem to obtain the probability that the student likes wine given it has no car, \\(P(A|B)\\). Lets assume that only the ones who prefer wine above flowers among the students go together for having a glass of wine at the bar after the statistics course. While they drink wine, the tell each other about their cars and they obtain the probability that a student has no car given it likes wine, \\(P(B|A) = 0.6\\). During the statistics class the teacher asked the students about their car ownership and birthday preference. Therefore, they know that \\(P(A) =\\) likes wine \\(= 0.68\\) and \\(P(B) =\\) no car \\(= 0.68\\). With these information, they can obtain the probability that a student likes wine given it has no car, even if not all students without cars went to the bar: \\(P(A|B) = \\frac{0.6*0.68}{0.68} = 0.6\\). 2.4.2 Bayes theorem for continuous parameters When we use the Bayes theorem for analyzing data, then the aim is to make probability statements for parameters. Such a probability statement could be the description of what we know about the population mean after having looked at the data. Because the mean of the population is measured at a continuous scale we use a probability density function to describe what we know about the mean. The Bayes theorem can be formulated for probability density functions denoted with \\(p(\\theta)\\), e.g. for a parameter \\(\\theta\\) (for example probability density functions see Chapter 4). What we are interested in is the probability of the parameter \\(\\theta\\) given the data, i.e., \\(p(\\theta|y)\\). This probability density function is called the posterior distribution of the parameter \\(\\theta\\). Here is the Bayes theorem formulated for obtaining the posterior distribution of a parameter from the data \\(y\\), the prior distribution of the parameter \\(p(\\theta)\\) and assuming a model for the data generating process. The data model is defined by the likelihood that specifies how the data \\(y\\) is distributed given the parameters \\(p(y|\\theta)\\): \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} = \\frac{p(y|\\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta) d\\theta}\\) The probability of the data \\(p(y)\\) is also called the scaling constant, because it is a constant. It is the integral of the likelihood over all possible values of the parameter(s) of the model. 2.4.3 Estimating a mean assuming that the variance is known For illustration, we first describe a simple (unrealistic) example for which it is almost possible to follow the mathematical steps for solving the Bayes theorem even for non-mathematicians. Even if we cannot follow all steps, this example will illustrate the principle how the Bayesian theorem works for continuous parameters. The example is unrealistic because we assume that the variance \\(\\sigma^2\\) in the data \\(y\\) is known. We construct a data model by assuming that \\(y\\) is normally distributed: \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known. The function \\(Norm\\) defines the probability density function of the normal distribution (Chapter 4). The parameter, for which we would like to get the posterior distribution is \\(\\theta\\), the mean. We specify a prior distribution for \\(\\theta\\). Because the normal distribution is a conjugate prior for a normal data model with known variance, we use the normal distribution. Conjugate priors have nice mathematical properties (see Chapter 10) and are therefore preferred when the posterior distribution is obtained algebraically. That is the prior: \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) With the above data, data model and prior, the posterior distribution of the mean \\(\\theta\\) is defined by: \\(p(\\theta|y) = Norm(\\mu_n, \\tau_n)\\), where \\(\\mu_n= \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}}\\) and \\(\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\) \\(\\bar{y}\\) is the arithmetic mean of the data. Because only this value is needed in order to obtain the posterior distribution, this value is called the sufficient statistics. From the mathematical formulas above and also from Fig. 2.8 we see that the mean of the posterior distribution is a weighted average between the prior mean and \\(\\bar{y}\\) with weights equal to the precisions (\\(\\frac{1}{\\tau_0^2}\\) and \\(\\frac{n}{\\sigma^2}\\)). Figure 2.8: Hypothetical example showing the result of applying the Bayes theorem for obtaining a posterior distribution of a continuous parameter. The likelhood is defined by the data and the model, the prior is expressing the knowledge about the parameter before looking at the data. Combining the two distributions using the Bayes theorem results in the posterior distribution. 2.4.4 Estimating the mean and the variance We now move to a more realistic example, which is estimating the mean and the variance of a sample of weights of Snowfinches Montifringilla nivalis (Fig. 2.9). To analyze those data, a model with two parameters (the mean and the variance or standard deviation) is needed. The data model (or likelihood) is specified as \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\). Figure 2.9: Snowfinches stay above the treeline for winter. They come to feeders. # weight (g) y &lt;- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5) n &lt;- length(y) Because there are two parameters, we need to specify a two-dimensional prior distribution. We looked up in A. Gelman et al. (2014b) that the conjugate prior distribution in our case is an Normal-Inverse-Chisquare distribution: \\(p(\\theta, \\sigma) = N-Inv-\\chi^2(\\mu_0, \\sigma_0^2/\\kappa_0; v_0, \\sigma_0^2)\\) From the same reference we looked up how the posterior distribution looks like in our case: \\(p(\\theta,\\sigma|y) = \\frac{p(y|\\theta, \\sigma)p(\\theta, \\sigma)}{p(y)} = N-Inv-\\chi^2(\\mu_n, \\sigma_n^2/\\kappa_n; v_n, \\sigma_n^2)\\), with \\(\\mu_n= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0 + \\frac{n}{\\kappa_0+n}\\bar{y}\\), \\(\\kappa_n = \\kappa_0+n\\), \\(v_n = v_0 +n\\), \\(v_n\\sigma_n^2=v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\) For this example, we need the arithmetic mean \\(\\bar{y}\\) and standard deviation \\(s^2\\) from the sample for obtaining the posterior distribution. Therefore, these two statistics are the sufficient statistics. The above formula look intimidating, but we never really do that calculations. We let Rdoing that for us in most cases by simulating many numbers from the posterior distribution. In the end, we can visualize the distribution of these many numbers to have a look at the posterior distribution. In Fig. 2.10 the two-dimensional \\((\\theta, \\sigma)\\) posterior distribution is visualized by using simulated values. The two dimensional distribution is called the joint posterior distribution. The mountain of dots in Fig. 2.10 visualize the Normal-Inverse-Chisquare that we calculated above. When all values of one parameter is displayed in a histogram ignoring the values of the other parameter, it is called the marginal posterior distribution. Algebraically, the marginal distribution is obtained by integrating one of the two parameters out over joint posterior distribution. This step is definitively way easier when simulated values from the posterior distribution are available! For the Snowfinch weights example, with a normal data model and a conjugate prior the marginal posterior distribution of the mean \\(\\theta\\) becomes a t-distribution and the one of \\(\\sigma\\) a inverse-Chisquare distribution. Another nice property of the posterior distribution in our example is that a horizontal section through the joint posterior distribution (at a fixed value of \\(\\sigma\\), i.e. a fixed variance) leads to a normal distribution. In other words, the density of the dots in the plot at a fixed value of \\(\\sigma\\) follows a normal density function. Figure 2.10: Visualization of the joint posterior distribution for the mean and standard deviation of Snowfinch weights. The lower left panel shows the two-dimensional joint posterior distribution, whereas the upper and right panel show the marginal posterior distributions of each parameter separately. The marginal posterior distributions of every parameter is what we normally report in a paper. They inform us about what we know about these parameters (of the big unmeasured population) after having looked at the data. In our example the the marginal distribution of the mean is a t-distribution (Chapter 4). Frequentist statistical methods also use a t-distribution to describe the uncertainty of a mean for the case when the variance is not known. Thus, frequentist methods came to the same solution using a completely different approach and different techniques. Doesnt that increase dramatically our trust in statistical methods? 2.5 Classical frequentist tests and alternatives 2.5.1 Nullhypothesis testing Null hypothesis testing is constructing a model that describes how the data would look like in case of what we expect to be would not be. Then, the data is compared to how the model thinks the data should look like. If the data does not look like the model thinks they should, we reject that model and accept that our expectation may be true. To decide whether the data looks like the model constructed from the null hypothesis thinks the data should look like the p-value is used. The p-value is the probability of observing the data or more extreme data given the null hypothesis is true. Small p-values indicate that it is rather unlikely to observe the data or more extreme data given the null hypothesis \\(H_0\\) is true. Null hypothesis testing is problematic. We discuss some of the problems after having introduces the most commonly used classical tests. 2.5.2 Comparison of a sample with a fixed values (one-sample t-test) A null hypothesis is used to construct a model that can generate (hypothetical) data. For example, a null hypothesis could be \\(H_0:\\)The mean of Snowfinch weights is exactly 40g. A normal distribution with a mean of \\(\\mu_0=40\\) and a variance equal to the estimated variance in the data \\(s^2\\) is then assumed to describe how we would expect the data to look like given the null hypothesis was true. From that model it is possible to calculate the distribution of hypothetical means of many different hypothetical samples of sample size \\(n\\). The result is a t-distribution (Fig. 2.11). In classical tests, the distribution is standardized so that its variance was one. Then the sample mean, or in classical tests a standardized difference between the mean and the hypothetical mean of the null hypothesis (here 40g), called test statistics \\(t = \\frac{\\bar{y}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\), is compared to that (standardized) t-distribution. If the test statistics falls well within the expected distribution the null hypothesis is accepted. Then, the data is well compatible with the null hypothesis. However, if the test statistics falls in the tails or outside the distribution, then the null hypothesis is rejected and we could write that the mean weight of Snowfinches is statistically significantly different from 40g. Unfortunately, we cannot infer about the probability of the null hypothesis and how relevant the result is. Figure 2.11: Illustration of a one-sample t-test. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the t-distribution that shows how hypothetical sample means are expected to be distributed if the big population of Snowfinches has a mean weight of 40g (i.e., if the null hypothesis was true). Orange area shows the area of the t-distribution that lays equal or farther away from 40g than the sample mean. The orange area is the p-value. We can use the r-function t.test to calculate the p-value of a one sample t-test. t.test(y, mu=40) ## ## One Sample t-test ## ## data: y ## t = 3.0951, df = 7, p-value = 0.01744 ## alternative hypothesis: true mean is not equal to 40 ## 95 percent confidence interval: ## 40.89979 46.72521 ## sample estimates: ## mean of x ## 43.8125 The output of the r-function t.test also includes the mean and the 95% confidence interval (or compatibility or uncertainty interval) of the mean. The CI could, alternatively, be obtained as the 2.5% and 97.5% quantiles of a t-distribution with a mean equal to the sample mean, degrees of freedom equal to the sample size minus one and a standard deviation equal to the standard error of the mean. # lower limit of 95% CI mean(y) + qt(0.025, df=length(y)-1)*sd(y)/sqrt(n) ## [1] 40.89979 # upper limit of 95% CI mean(y) + qt(0.975, df=length(y)-1)*sd(y)/sqrt(n) ## [1] 46.72521 When applying the Bayes theorem for obtaining the posterior distribution of the mean we end up with the same t-distribution as described above, in case we use flat prior distributions for the mean and the standard deviation. Thus, the two different approaches end up with the same result! par(mar=c(4.5, 5, 2, 2)) hist(y, col=&quot;blue&quot;, xlim=c(30,52), las=1, freq=FALSE, main=NA, ylim=c(0, 0.3)) abline(v=mean(y), lwd=2, col=&quot;lightblue&quot;) abline(v=40, lwd=2) lines(density(bsim@coef)) text(45, 0.3, &quot;posterior distribution\\nof the mean of y&quot;, cex=0.8, adj=c(0,1), xpd=NA) Figure 2.12: Illustration of the posterior distribution of the mean. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the posterior distribution that shows what we know about the mean after having looked at the data. The area under the posterior density function that is larger than 40 is the posterior probability of the hypothesis that the true mean Snwofinch weight is larger than 40g. The posterior probability of the hypothesis that the true mean Snowfinch weight is larger than 40g, \\(P(H:\\mu&gt;40) =\\), is equal to the proportion of simulated random values from the posterior distribution, saved in the vector bsim@coef, that are larger than 40. # Two ways of calculating the proportion of values # larger than a specific value within a vector of values round(sum(bsim@coef[,1]&gt;40)/nrow(bsim@coef),2) ## [1] 0.99 round(mean(bsim@coef[,1]&gt;40),2) ## [1] 0.99 # Note: logical values TRUE and FALSE become # the numeric values 1 and 0 within the functions sum() and mean() We, thus, can be pretty sure that the mean Snowfinch weight (in the big world population) is larger than 40g. Such a conclusion is not very informative, because it does not tell us how much larger we can expect the mean Snowfinch weight to be. Therefore, we prefer reporting a credible interval (or compatibility interval or uncertainty interval) that tells us what values for the mean Snowfinch weight are compatible with the data (given the data model we used realistically reflects the data generating process). Based on such an interval, we can conclude that we are pretty sure that the mean Snowfinch weight is between 40 and 48g. # 80% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.1, 0.9)) ## 10% 90% ## 42.07725 45.54080 # 95% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.025, 0.975)) ## 2.5% 97.5% ## 40.90717 46.69152 # 99% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.005, 0.995)) ## 0.5% 99.5% ## 39.66181 48.10269 2.5.3 Comparison of the locations between two groups (two-sample t-test) Many research questions aim at measuring differences between groups. For example, we could be curious to know how different in size car owner are from people not owning a car. A boxplot is a nice possibility to visualize the ell length measurements of two (or more) groups (Fig. 2.13). From the boxplot, we do not see how many observations are in the two samples. We can add that information to the plot. The boxplot visualizes the samples but it does not show what we know about the big (unmeasured) population mean. To show that, we need to add a compatibility interval (or uncertainty interval, credible interval, confidence interval, in brown in Fig. 2.13). Figure 2.13: Ell length of car owners (Y) and people not owning a car (N). Horizontal bar = median, box = interquartile range, whiskers = extremest observation within 1.5 times the interquartile range from the quartile, circles=observations farther than 1.5 times the interquartile range from the quartile. Filled brown circles = means, vertical brown bars = 95% compatibility interval. When we added the two means with a compatibility interval, we see what we know about the two means, but we do still not see what we know about the difference between the two means. The uncertainties of the means do not show the uncertainty of the difference between the means. To do so, we need to extract the difference between the two means from a model that describes (abstractly) how the data has been generated. Such a model is a linear model that we will introduce in Chapter 11. The second parameter measures the differences in the means of the two groups. And from the simulated posterior distribution we can extract a 95% compatibility interval. Thus, we can conclude that the average ell length of car owners is with high probability between 0.5 cm smaller and 2.5 cm larger than the averag ell of people not having a car. mod &lt;- lm(ell~car, data=dat) mod ## ## Call: ## lm(formula = ell ~ car, data = dat) ## ## Coefficients: ## (Intercept) carY ## 43.267 1.019 bsim &lt;- sim(mod, n.sim=nsim) quantile(bsim@coef[,&quot;carY&quot;], prob=c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -0.501348 1.014478 2.494324 The corresponding two-sample t-test gives a p-value for the null hypothesis: The difference between the two means equals zero., a confidence interval for the difference and the two means. While the function lmgives the difference Y minus N, the function t.testgives the difference N minus Y. Luckily the two means are also given in the output, so we know which group mean is the larger one. t.test(ell~car, data=dat, var.equal=TRUE) ## ## Two Sample t-test ## ## data: ell by car ## t = -1.4317, df = 20, p-value = 0.1677 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.5038207 0.4657255 ## sample estimates: ## mean in group N mean in group Y ## 43.26667 44.28571 In both possibilities, we used to compare the to means, the Bayesian posterior distribution of the difference and the t-test or the confidence interval of the difference, we used a data model. We thus assumed that the observations are normally distributed. In some cases, such an assumption is not a reasonable assumption. Then the result is not reliable. In such cases, we can either search for a more realistic model or use non-parametric (also called distribution free) methods. Nowadays, we have almost infinite possibilities to construct data models (e.g. generalized linear models and beyond). Therefore, we normally start looking for a model that fits the data better. However, in former days, all these possiblities did not exist (or were not easily available for non-mathematicians). Therefore, we here introduce two of such non-parametric methods, the Wilcoxon-test (or Mann-Whitney-U-test) and the randomisation test. Some of the distribution free statistical methods are based on the rank instead of the value of the observations. The principle of the Wilcoxon-test is to rank the observations and sum the ranks per group. It is not completely true that the non-parametric methods do not have a model. The model of the Wilcoxon-test knows how the difference in the sum of the ranks between two groups is distributed given the mean of the two groups do not differ (null hypothesis). Therefore, it is possible to get a p-value, e.g. by the function wilcox.test. wilcox.test(ell~car, data=dat) ## ## Wilcoxon rank sum test with continuity correction ## ## data: ell by car ## W = 34.5, p-value = 0.2075 ## alternative hypothesis: true location shift is not equal to 0 The note in the output tells us that ranking is ambiguous, when some values are equal. Equal values are called ties when they should be ranked. The result of the Wilcoxon-test tells us how probable it is to observe the difference in the rank sum between the two sample or a more extreme difference given the means of the two groups are equal. That is at least something. A similar result is obtained by using a randomisation test. This test is not based on ranks but on the original values. The aim of the randomisation is to simulate a distribution of the difference in the arithmetic mean between the two groups assuming this difference would be zero. To do so, the observed values are randomly distributed among the two groups. Because of the random distribution among the two groups, we expect that, if we repeat that virtual experiment many times, the average difference between the group means would be zero (both virtual samples are drawn from the same big population). We can use a loop in R for repeating the random re-assignement to the two groups and, each time, extracting the difference between the group means. As a result, we have a vector of many (nsim) values that all are possible differences between group means given the two samples were drawn from the same population. The proportion of these values that have an equal or larger absolute value give the probability that the observed or a larger difference between the group means is observed given the null hypothesis would be true, thus that is a p-value. diffH0 &lt;- numeric(nsim) for(i in 1:nsim){ randomcars &lt;- sample(dat$car) rmod &lt;- lm(ell~randomcars, data=dat) diffH0[i] &lt;- coef(rmod)[&quot;randomcarsY&quot;] } mean(abs(diffH0)&gt;abs(coef(mod)[&quot;carY&quot;])) # p-value ## [1] 0.1858 Visualizing the possible differences between the group means given the null hypothesis was true shows that the observed difference is well within what is expected if the two groups would not differ in their means (Fig. 2.14). Figure 2.14: Histogram if differences between the means of randomly assigned groups (grey) and the difference between the means of the two observed groups (red) The randomization test results in a p-value and, we could also report the observed difference between the group means. However, it does not tell us, what values of the difference all would be compatible with the data. We do not get an uncertainty measurement for the difference. In order to get a compatibility interval without assuming a distribution for the data (thus non-parametric) we could bootstrap the samples. Bootstrapping is sampling observations from the data with replacement. For example, if we have a sample of 8 observations, we draw 8 times a random observation from the 8 observation. Each time, we assume that all 8 observations are available. Thus a bootstrapped sample could include some observations several times, whereas others are missing. In this way, we simulate the variance in the data that is due to the fact that our data do not contain the whole big population. Also bootstrapping can be programmed in R using a loop. diffboot &lt;- numeric(1000) for(i in 1:nsim){ ngroups &lt;- 1 while(ngroups==1){ bootrows &lt;- sample(1:nrow(dat), replace=TRUE) ngroups &lt;- length(unique(dat$car[bootrows])) } rmod &lt;- lm(ell~car, data=dat[bootrows,]) diffboot[i] &lt;- coef(rmod)[2] } quantile(diffboot, prob=c(0.025, 0.975)) ## 2.5% 97.5% ## -0.3395643 2.4273810 The resulting values for the difference between the two group means can be interpreted as the distribution of those differences, if we had repeated the study many times (Fig. 2.15). A 95% interval of the distribution corresponds to a 95% compatibility interval (or confidence interval or uncertainty interval). hist(diffboot); abline(v=coef(mod)[2], lwd=2, col=&quot;red&quot;) Figure 2.15: Histogram of the boostrapped differences between the group means (grey) and the observed difference. For both methods, randomisation test and bootstrapping, we have to assume that all observations are independent. Randomization and bootstrapping becomes complicated or even unfeasible when data are structured. 2.6 Summary Bayesian data analysis is applying the Bayes theorem for summarizing knowledge based on data, priors and the model assumptions. Frequentist statistics is quantifying uncertainty by hypothetical repetitions. "],["analyses-steps.html", "3 Data analysis step by step 3.1 Plausibility of Data 3.2 Relationships 3.3 Data Distribution 3.4 Preparation of Explanatory Variables 3.5 Data Structure 3.6 Define Prior Distributions 3.7 Fit the Model 3.8 Check Model 3.9 Model Uncertainty 3.10 Draw Conclusions Further reading", " 3 Data analysis step by step In this chapter we provide a checklist with some guidance for data analysis. However, do not expect the list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps 3.2 to 3.8 until we find a model that fit the data well and that is realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful model. There is a chance and danger at the same time: we may find interesting results that answer different questions than we asked originally. They may be very exciting and important, however they may be biased. We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we have to be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process. 3.1 Plausibility of Data Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries. See chapter ?? for useful R-code that can be used for data preparation and to make plausibility controls. 3.2 Relationships Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful. We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible. 3.3 Data Distribution What is the nature of the variable of interest (outcome, dependent variable)? At this stage, there is no use of formally comparing the distribution of the outcome variable to a statistical distribution, because the rawdata is not required to follow a specific distribution. The models assume that conditional on the explanatory variables and the model structure, the outcome variable follows a specific distribution. Therefore, checking how well the chosen distribution fits to the data is done after the model fit 3.8. This first choice is solely done based on the nature of the data. Normally, our first choice is one of the classical distributions for which robust software for model fitting is available. Here is a rough guideline for this first choice: continuous measurements \\(\\Longrightarrow\\) normal distribution &gt; exceptions: time-to-event data \\(\\Longrightarrow\\) see survival analysis count \\(\\Longrightarrow\\) Poisson or negative-binomial distribution count with upper bound (proportion) \\(\\Longrightarrow\\) binomial distribution binary \\(\\Longrightarrow\\) Bernoully distribution rate (count by a reference) \\(\\Longrightarrow\\) Poisson including an offset nominal \\(\\Longrightarrow\\) multinomial distribution Chapter 4 gives an overview of the distributions that are most relevant for ecologists. 3.4 Preparation of Explanatory Variables Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step: Is the variance (of the explanatory variable) big enough so that an effect of the variable can be measured? Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable (e.g., log, square-root). Does it show a bimodal distribution? Consider making the variable binary. Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a trans- formation (e.g., log) could linearize the relationship between x and y. Centering: Centering (\\(x.c = x-mean(x)\\)) is a transformation that produces a variable with a mean of 0. Centering is optional. We have two reasons to center a predictor variable. First, it helps the model fitting algorithm to better converge because it reduces correlations among model parameters. Second, with centered predictors, the intercept and main effects in the linear model are better interpretable (they are measured at the center of the data instead of at the covariate value of 0 which may be far off). Scaling: Scaling (\\(x.s = x/c\\), where \\(c\\) is a constant) is a transformation that changes the unit of the variable. Also scaling is optional. We have three reasons to scale an predictor variable. First, to make the effect sizes better understandable. For example, a population change from one year to the next may be very small and hard to interpret. When we give the change for a 10-year period, its ecological meaning is better understandable. Second, to make the estimate of the effect sizes comparable between variables, we may use \\(x.s = x/sd(x)\\). The resulting variable has a unit of one standard deviation. A standard deviation may be comparable between variables that oritinally are measured in different units (meters, seconds etc). A. Gelman and Hill (2007) (p. 55 f) propose to scale the variables by two times the standard deviation (\\(x.s = x/(2*sd(x))\\)) to make effect sizes comparable between numeric and binary variables. Third, scaling can be important for model convergence, especially when polynomials are included. Also, consider the use of orthogonal polynomials, see Chapter 4.2.9 in Korner-Nievergelt et al. (2015). Collinearity: Look at the correlation among the explanatory variables (pairs plot or correlation matrix). If the explanatory variables are correlated, go back to step 2. Also, Chapter 4.2.7 in Korner-Nievergelt et al. (2015) discusses collinearity. Are interactions and polynomial terms needed in the model? If not already done in step 2, think about the relationship between each explanatory variable and the dependent variable. Is it linear or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM). May the effect of one explanatory variable depend on the value of another explanatory variable (interaction)? 3.5 Data Structure After having taken into account all of the (fixed effect) terms from step 4: are the observations independent or grouped/structured? What random factors are needed in the model? Are the data obviously temporally or spatially correlated? Or, are other correlation structures present, such as phylogenetic relationships? Our strategy is to start with a rather simple model that may not account for all correlation structures that in fact are present in the data. We first, only include those that are known to be important a priory. Only when residual analyses reveals important additional correlation structures, we include them in the model. 3.6 Define Prior Distributions Decide whether we would like to use informative prior distributions or whether we would like use priors that only have a negligible effect on the results. When the results are later used for informing authorities or for making a decision (as usual in applied sciences), then we would like to base the results on all information available. Information from the literature is then used to construct informative prior distributions. In contrast to applied sciences, in basic research we often would like to show only the information in the data that should not be influenced by earlier results. Therefore, in basic research we look for priors that do not influence the results. 3.7 Fit the Model Fit the model. 3.8 Check Model We assess model fit by graphical analyses of the residuals (Chapter 6 in Korner-Nievergelt et al. (2015)), by predictive model checking (Section 10.1 in Korner-Nievergelt et al. (2015)), or by sensitivity analysis (Chapter 15 in Korner-Nievergelt et al. (2015)). For non-Gaussian models it is often easier to assess model fit using pos- terior predictive checks (Chapter 10 in Korner-Nievergelt et al. (2015)) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, we list in Chapter 16 of Korner-Nievergelt et al. (2015) some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases. 3.9 Model Uncertainty If, while working through steps 1 to 8, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we then turn to the methods presented in Chapter 11 (Korner-Nievergelt et al. (2015)) to draw inference from more than one model. If we have only one model, we proceed to 3.10. 3.10 Draw Conclusions Simulate values from the joint posterior distribution of the model parameters (sim or Stan). Use these samples to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities. Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],["distributions.html", "4 Probability distributions 4.1 Introduction 4.2 Normal distribution 4.3 Poisson distribution 4.4 Gamma distribution 4.5 F-distribution", " 4 Probability distributions 4.1 Introduction 4.2 Normal distribution normal distribution = Gaussian distribution \\(p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(\\theta -\\mu)^2) = Normal(\\mu, \\sigma)\\) \\(E(\\theta) = \\mu\\), \\(var(\\theta) = \\sigma^2\\), \\(mode(\\theta) = \\mu\\) The variance parameter can be specified to be a variance, a standard deviation or a precision. Depending on the software used (or author of the paper) different habits exist. 4.3 Poisson distribution xxx 4.4 Gamma distribution xxx 4.4.1 Cauchy distribution We sometimes use the Cauchy distiribution to specify the prior distirbution of the standart deviation and similar parameters in a model. An example for this is the regression model that we used to introduce Stan (Chapter 18.3). 4.4.2 t-distribution marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution \\(p(\\theta|v,\\mu,\\sigma) = \\frac{\\Gamma((v+1)/2)}{\\Gamma(v/2)\\sqrt{v\\pi}\\sigma}(1+\\frac{1}{v}(\\frac{\\theta-\\mu}{\\sigma})^2)^{-(v+1)/2}\\) \\(v\\) degrees of freedom \\(\\mu\\) location \\(\\sigma\\) scale 4.5 F-distribution Ratios of sample variances drawn from populations with equal variances follow an F-distribution. The density function of the F-distribution is even more complicated than the one of the t-distribution! We do not copy it here. Further, we have not yet met any Bayesian example where the F-distribution is used (that does not mean that there is no). It is used in frequentist analyses in order to compare variances, and, within the ANOVA, to compare means between groups. If two variances only differ because of natural variance in the data (nullhypothesis) then \\(\\frac{Var(X_1)}{Var(X_2)}\\sim F_{df_1,df_2}\\). Figure 4.1: Different density functions of the F statistics "],["rfunctions.html", "5 Important R-functions 5.1 Data preparation 5.2 Figures 5.3 Summary", " 5 Important R-functions THIS CHAPTER IS UNDER CONSTRUCTION!!! 5.1 Data preparation 5.2 Figures 5.3 Summary "],["reproducibleresearch.html", "6 Reproducible research 6.1 Summary 6.2 Further reading", " 6 Reproducible research THIS CHAPTER IS UNDER CONSTRUCTION!!! 6.1 Summary 6.2 Further reading Rmarkdown: The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages. Bookdown by Yihui Xie: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. The book can be exported to HTML, PDF, and e-books (e.g. EPUB). The book style is customizable. You can easily write and preview the book in RStudio IDE or other editors, and host the book wherever you want (e.g. bookdown.org). Our book is written using bookdown. "],["furthertopics.html", "7 Further topics 7.1 Bioacoustic analyse 7.2 Python", " 7 Further topics This is a collection of short introductions or links with commented R code that cover other topics that might be useful for ecologists. 7.1 Bioacoustic analyse Bioacoustic analyses are nicely covered in a blog by Marcelo Araya-Salas. 7.2 Python Like R, python is a is a high-level programming language that is used by many ecologists. The reticulate package provides a comprehensive set of tools for interoperability between Python and R. "],["PART-II.html", "8 Introduction to PART II Further reading", " 8 Introduction to PART II Further reading A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. A. Gelman et al. 2014b) and Trevor Hastie (e.g. Efron and Hastie (2016)) because both explain complicated things in a concise and understandable way. "],["bayesian-paradigm.html", "9 The Bayesian paradigm 9.1 Introduction 9.2 Summary", " 9 The Bayesian paradigm THIS CHAPTER IS UNDER CONSTRUCTION!!! 9.1 Introduction 9.2 Summary xxx "],["priors.html", "10 Prior distributions 10.1 Introduction 10.2 How to choose a prior 10.3 Prior sensitivity", " 10 Prior distributions THIS CHAPTER IS UNDER CONSTRUCTION!!! 10.1 Introduction The prior is an integral part of a Bayesian model. We must specify one. When to use informative priors: in applied research, when information from two different data sets should be combined. when using non-informative priors: in basic reasearch when results should only report the information in the current data set. discussion on priors 10.2 How to choose a prior Tabelle von Fränzi (CourseIII_glm_glmmm/course2018/presentations_handouts/presentations) new references to be included: Lemoine.2019 10.3 Prior sensitivity xxx "],["lm.html", "11 Normal Linear Models 11.1 Linear regression 11.2 Regression Variants: ANOVA, ANCOVA, and Multiple Regression", " 11 Normal Linear Models 11.1 Linear regression 11.1.1 Background Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression. Typical questions that can be answered using linear regression are: How does \\(y\\) change with changes in \\(x\\)? How is y predicted from \\(x\\)? An ordinary linear regression (i.e., one numeric \\(x\\) and one numeric \\(y\\) variable) can be represented by a scatterplot of \\(y\\) against \\(x\\). We search for the line that ts best and describe how the observations scatter around this regression line (see Fig. 11.1 for an example). The model formula of a simple linear regression with one continuous predictor variable \\(x_i\\) (the subscript \\(i\\) denotes the \\(i=1,\\dots,n\\) data points) is: \\[\\begin{align} \\mu_i &amp;=\\beta_0 + \\beta_1 x_i \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{11.1} \\end{align}\\] While the first part of Equation (11.1) describes the regression line, the second part describes the differences between predicted values \\(\\mu_i\\) and observations \\(y_i\\). In other words: the observation \\(y_i\\) stems from a normal distribution with mean \\(\\mu_i\\) and variance \\(\\sigma^2\\) . The mean of the normal distribution, \\(\\mu_i\\) , equals the sum of the intercept (\\(b_0\\) ) and the product of the slope (\\(b_1\\)) and the continuous predictor value, \\(x_i\\). The differences between observation \\(y_i\\) and the predicted values \\(\\mu_i\\) are the residuals (i.e., \\(\\epsilon_i=y_i-\\mu_i\\)). Equivalently to Equation (11.1), the regression could thus be written as: \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\ \\epsilon_i &amp;\\sim Norm(0, \\sigma^2) \\tag{11.2} \\end{align}\\] We prefer the notation in Equation (11.1) because, in this formula, the stochastic part (second row) is nicely separated from the deterministic part (first row) of the model, whereas, in the second notation (11.2) the rst row contains both stochastic and deterministic parts. Using matrix notation equation (11.1) can also be written in one row: \\[\\boldsymbol{y} \\sim Norm(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2\\boldsymbol{I})\\] where \\(\\boldsymbol{ I}\\) is the \\(n \\times n\\) identity matrix (it transforms the variance parameter to a \\(n \\times n\\) matrix with its diagonal elements equal \\(\\sigma^2\\) ; \\(n\\) is the sample size). The multiplication by \\(\\boldsymbol{ I}\\) is necessary because we use vector notation, \\(\\boldsymbol{y}\\) instead of \\(y_{i}\\) . Here, \\(\\boldsymbol{y}\\) is the vector of all observations, whereas \\(y_{i}\\) is a single observation, \\(i\\). When using vector notation, we can write the linear predictor of the model, \\(\\beta_0 + \\beta_1 x_i\\) , as a multiplication of the vector of the model coefcients \\[\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\] times the model matrix \\[\\boldsymbol{X} = \\begin{pmatrix} 1 &amp; x_1 \\\\ \\dots &amp; \\dots \\\\ 1 &amp; x_n \\end{pmatrix}\\] where \\(x_1 , \\dots, x_n\\) are the observed values for the predictor variable, \\(x\\). The rst column of \\(\\boldsymbol{X}\\) contains only ones because the values in this column are multiplied with the intercept, \\(\\beta_0\\) . To the intercept, the product of the second element of \\(\\boldsymbol{\\beta}\\), \\(\\beta_1\\) , with each element in the second column of \\(\\boldsymbol{X}\\) is added to obtain the predicted value for each observation, \\(\\boldsymbol{\\mu}\\): \\[\\begin{align} \\boldsymbol{\\beta X}= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} \\times \\begin{pmatrix} 1 &amp; x_1 \\\\ \\dots &amp; \\dots \\\\ 1 &amp; x_n \\end{pmatrix} = \\begin{pmatrix} \\beta_0 + \\beta_1x_1 \\\\ \\dots \\\\ \\beta_0 + \\beta_1x_n \\end{pmatrix}= \\begin{pmatrix} \\hat{y}_1 \\\\ \\dots \\\\ \\hat{y}_n \\end{pmatrix} = \\boldsymbol{\\mu} \\tag{11.3} \\end{align}\\] 11.1.2 Fitting a Linear Regression in R In Equation (11.1), the predicted (synonym: fitted) values \\(\\mu_i\\) are directly dened by the model coefcients, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) . Therefore, when we can estimate \\(\\beta_{0}\\), \\(\\beta_{1}\\) , and \\(\\sigma^2\\), the model is fully dened . The last parameter \\(\\sigma^2\\) describes how the observations scatter around the regression line and relies on the assumption that the residuals are normally distributed. The estimates for the model parameters of a linear regression are obtained by searching for the best tting regression line. To do so, we search for the regression line that minimizes the sum of the squared residuals. This model tting method is called the least-squares method, abbreviated as LS. It has a very simple solution using matrix algebra (see e.g., Aitkin et al. 2009). Note that we can apply LS techniques independent of whether we use a Bayesian or frequentist framework to draw inference. In Bayesian statistics, Equation (11.1) is called the data model, because it describes mathematically the process that has (or, better, that we think has) produced the data. This nomenclature also helps to distinguish data models from models for parameters such as prior distributions. The least-squares estimates for the model parameters of a linear regression are obtained in R using the function lm. For illustration, we rst simulate a data set and then t a linear regression to these simulated data. The advantage of simulating data is that the following analyses can be reproduced without having to read data into R. set.seed(34) # set a seed for the random number generator n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # sample values of the covariate mu &lt;- b0 + b1 * x y &lt;- rnorm(n, mu, sd = sigma) # Save data in table dat &lt;- tibble(x = x, y = y) Then, we t a linear regression to the data to obtain the results of the three parameters of the linear regression, that is intercept, slope and residual standard deviation. mod &lt;- lm(y ~ x, data = dat) coef(mod) ## (Intercept) x ## 2.0049517 0.6880415 summary(mod)$sigma ## [1] 5.04918 The object mod produced by lm contains the estimates for the intercept, \\(\\beta_0\\) , and the slope, \\(\\beta_1\\). The residual standard deviation \\(\\sigma^2\\) is extracted using the function summary. We can show the result of the linear regression as a line in a scatter plot with the covariate (x) on the x-axis and the observations (y) on the y-axis (Fig. 11.1). Figure 11.1: Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The &lt;U+FB01&gt;tted values lie where the orange dotted lines touch the blue regression line. Conclusions drawn from a model depend on the model assumptions. When model assumptions are violated, estimates usually are biased and inappropriate conclusions can be drawn. We devote Chapter 12 to the assessment of model assumptions, given its importance. 11.1.3 Drawing Conclusions To answer the question about how strongly \\(y\\) is related to \\(x\\), or to predict \\(y\\) from \\(x\\), and because we usually draw inference in a Bayesian framework, we are interested in the joint posterior distribution of \\(\\boldsymbol{\\beta}\\) (vector that contains \\(\\beta_{0}\\) and \\(\\beta_{1}\\) ) and \\(\\sigma^2\\) , the residual variance. The function sim does this for us. To somewhat demystify the sim function we briey explain what sim does. The principle is to rst draw a random value from the marginal posterior distribution of \\(\\sigma^2\\), and then to draw random values from the conditional posterior distribution for \\(\\boldsymbol{\\beta}\\) (A. Gelman et al. 2014a). The conditional posterior distribution of the parameter vector \\(\\boldsymbol{\\beta}\\), \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})\\) is the posterior distribution of \\(\\boldsymbol{\\beta}\\) given a specic value for \\(\\sigma^2\\) . This conditional distribution can be analytically derived. With at prior distributions, it is a uni- or multivariate normal distribution \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})=Norm(\\boldsymbol{\\hat{\\beta}},V_\\beta,\\sigma^2)\\) with: \\[\\begin{align} \\boldsymbol{\\hat{\\beta}=(\\boldsymbol{X^TX})^{-1}X^Ty} \\tag{11.4} \\end{align}\\] and \\(V_\\beta = (\\boldsymbol{X^T X})^{-1}\\) . For models with the normal error distribution, the LS estimates for \\(\\boldsymbol{\\beta}\\) (given by Eq. (11.4)) equal the maximum likelihood (ML) estimates ==(see Chapter 5)==. The marginal posterior distribution of \\(\\sigma^2\\) is independent of specic values of \\(\\boldsymbol{\\beta}\\). It is, for at prior distributions, an inverse chi-square distribution \\(p(\\sigma^2|\\boldsymbol{y,X})=Inv-\\chi^2(n-k,\\sigma^2)\\), where \\(\\sigma^2 = \\frac{1}{n-k}(\\boldsymbol{y}-\\boldsymbol{X,\\hat{\\beta}})^T(\\boldsymbol{y}-\\boldsymbol{X,\\hat{\\beta}})\\), and \\(k\\) is the number of parameters. The marginal posterior distribution of \\(\\boldsymbol{\\beta}\\) can be obtained by integrating the conditional posterior distribution \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})=Norm(\\boldsymbol{\\hat{\\beta}},V_\\beta\\sigma^2)\\) over the distribution of \\(\\sigma^2\\) . This results in a uni- or multivariate \\(t\\)-distribution. However, it is not necessary to do this analytically. Using the function sim from the package, we can draw samples from \\(p(\\sigma^2|\\boldsymbol{y,X})\\) and describe the marginal posterior distributions of \\(\\boldsymbol{\\beta}\\) using the simulated values. library(arm) nsim &lt;- 1000 bsim &lt;- sim(mod, n.sim = nsim) The function sim simulates (in our example) 1000 values from the joint posterior distribution of the three model parameters \\(\\beta_0\\) , \\(\\beta_1\\), and \\(\\sigma\\). These simulated values are shown in Figure 11.2. Figure 11.2: Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters. The posterior distributions describe the range of plausible parameter values given the data and the model. They express our uncertainty about the model parameters; they show what we know about the model parameters after having looked at the data and given the model is realistic. The 2.5% and 97.5% quantiles of the marginal posterior distributions can be used as 95% credible intervals of the model parameters. The function coef extracts the simulated values for the beta coefcients, returning a matrix with nsim rows and the number of columns corresponding to the number of parameters. In our example, the rst column contains the simulated values from the posterior distribution of the intercept and the second column contains values from the posterior distribution of the slope. The 2 in the second argument of the apply-function (see Chapter ??) indicates that the quantile function is applied columnwise. apply(X = coef(bsim), MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975)) %&gt;% round(2) ## (Intercept) x ## 2.5% -2.95 0.44 ## 97.5% 7.17 0.92 We also can calculate a credible interval of the estimated residual standard deviation, \\(\\hat{\\sigma}\\). quantile(bsim@sigma, probs = c(0.025, 0.975)) %&gt;% round(1) ## 2.5% 97.5% ## 4.2 6.3 Using Bayesian methods allows us to get a posterior probability for specic hypotheses, such as The slope parameter is larger than 1 or The slope parameter is larger than 0.5. These probabilities are the proportion of simulated values from the posterior distribution that are larger than 1 and 0.5, respectively. sum(coef(bsim)[,2] &gt; 1) / nsim ## [1] 0.008 sum(coef(bsim)[,2] &gt; 0.5) / nsim ## [1] 0.936 From this, there is very little evidence in the data that the slope is larger than 1, but we are quite condent that the slope is larger than 0.5 (assuming that our model is realistic). We often want to show the effect of \\(x\\) on \\(y\\) graphically, with information about the uncertainty of the parameter estimates included in the graph. To draw such effect plots, we use the simulated values from the posterior distribution of the model parameters. From the deterministic part of the model, we know the regression line \\(\\mu = \\beta_0 + \\beta_1 x_i\\). The simulation from the joint posterior distribution of \\(\\beta_0\\) and \\(\\beta_1\\) gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We can draw these regression lines in an x-y plot (scatter plot) to show the uncertainty in the regression line estimation (Fig. 11.3, left). Note, that in this case it is not advisable to use ggplot because we draw many lines in one plot, which makes ggplot rather slow. par(mar = c(4, 4, 0, 0)) plot(x, y, pch = 16, las = 1, xlab = &quot;Covariate (x)&quot;, ylab = &quot;Dependend variable (y)&quot;) for(i in 1:nsim) { abline(coef(bsim)[i,1], coef(bsim)[i,2], col = rgb(0, 0, 0, 0.05)) } Figure 11.3: Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line. A more convenient way to show uncertainty is to draw the 95% credible interval, CrI, of the regression line. To this end, we rst dene new x-values for which we would like to have the tted values (about 100 points across the range of x will produce smooth-looking lines when connected by line segments). We save these new x-values within the new tibble newdat. Then, we create a new model matrix that contains these new x-values (newmodmat) using the function model.matrix. We then calculate the 1000 tted values for each element of the new x (one value for each of the 1000 simulated regressions, Fig. 11.3), using matrix multiplication (%*%). We save these values in the matrix tmat. Finally, we extract the 2.5% and 97.5% quantiles for each x-value from tmat, and draw the lines for the lower and upper limits of the credible interval (Fig. 11.4). # Calculate 95% credible interval newdat &lt;- tibble(x = seq(10, 30, by = 0.1)) newmodmat &lt;- model.matrix( ~ x, data = newdat) fitmat &lt;- matrix(ncol = nsim, nrow = nrow(newdat)) for(i in 1:nsim) {fitmat[,i] &lt;- newmodmat %*% coef(bsim)[i,]} newdat$CrI_lo &lt;- apply(fitmat, 1, quantile, probs = 0.025) newdat$CrI_up &lt;- apply(fitmat, 1, quantile, probs = 0.975) # Make plot regplot &lt;- ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_smooth(method = lm, se = FALSE) + geom_line(data = newdat, aes(x = x, y = CrI_lo), lty = 3) + geom_line(data = newdat, aes(x = x, y = CrI_up), lty = 3) + labs(x = &quot;Covariate (x)&quot;, y = &quot;Dependend variable (y)&quot;) regplot Figure 11.4: Regression with 95% credible interval of the posterior distribution of the &lt;U+FB01&gt;tted values. The interpretation of the 95% credible interval is straightforward: We are 95% sure that the true regression line is within the credible interval. As always, this interpretation is only true if the the model is correct. The larger the sample size, the narrower the interval, because each additional data point increases information about the true regression line. The credible interval measures uncertainty of the regression line, but it does not describe how new observations would scatter around the regression line. If we want to describe where future observations will be, we have to report the posterior predictive distribution. We can get a sample of random draws from the posterior predictive distribution \\(\\hat{y}|\\boldsymbol{\\beta},\\sigma^2,\\boldsymbol{X}\\sim Norm( \\boldsymbol{X \\beta, \\sigma^2})\\) using the simulated joint posterior distributions of the model parameters, thus taking the uncertainty of the parameter estimates into account. We draw a new \\(\\hat{y}\\)-value from \\(Norm( \\boldsymbol{X \\beta, \\sigma^2})\\) for each simulated set of model parameters. Then, we can visualize the 2.5% and 97.5% quantiles of this distribution for each new x-value. # increase number of simulation to procude smooth lines of the posterior # predictive distribution set.seed(34) nsim &lt;- 50000 bsim &lt;- sim(mod, n.sim=nsim) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- newmodmat%*%coef(bsim)[i,] # prepare matrix for simulated new data newy &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) # for each simulated tted value, simulate one new y-value for(i in 1:nsim) { newy[,i] &lt;- rnorm(nrow(newdat), mean = fitmat[,i], sd = bsim@sigma[i]) } # Calculate 2.5% and 97.5% quantiles newdat$pred_lo &lt;- apply(newy, 1, quantile, probs = 0.025) newdat$pred_up &lt;- apply(newy, 1, quantile, probs = 0.975) # Add the posterior predictive distribution to plot regplot + geom_line(data = newdat, aes(x = x, y = pred_lo), lty = 2) + geom_line(data = newdat, aes(x = x, y = pred_up), lty = 2) Figure 11.5: Regression line with 95% credible interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines. Future observations are expected to be within the interval dened by the broken lines in Fig. 11.5 with a probability of 0.95. Increasing sample size will not necessarily give a narrower predictive distribution because the predictive distribution also depends on the residual variance \\(\\sigma^2\\). The way we produced Fig. 11.5 is somewhat tedious compared to how easy we could have obtained the same gure using frequentist methods: predict(mod, newdata = newdat, interval = \"prediction\") would have produced the y-values for the lower and upper lines in Fig. 11.5 in one R-code line. However, once we have a simulated sample of the posterior predictive distribution, we have much more information than is contained in the frequentist prediction interval. For example, we could give an estimate for the proportion of observations greater than 20, given \\(x = 25\\). sum(newy[newdat$x == 25, ] &gt; 20) / nsim ## [1] 0.44504 Thus, we expect 44% of future observations with \\(x = 25\\) to be higher than 20. We can extract similar information for any relevant threshold value. Another reason to learn the more complicated R code we presented here, compared to the frequentist methods, is that, for more complicated models such as mixed models, the frequentist methods to obtain condence intervals of tted values are much more complicated than the Bayesian method just presented. The latter can be used with only slight adaptations for mixed models and also for generalized linear mixed models. 11.1.4 Frequentist Results The solution for \\(\\boldsymbol{\\beta}\\) is the Equation (11.3). Most statistical software, including R, return an estimated frequentist standard error for each \\(\\beta_k\\). We extract these standard errors together with the estimates for the model parameters using the summary function. summary(mod) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5777 -3.6280 -0.0532 3.9873 12.1374 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0050 2.5349 0.791 0.433 ## x 0.6880 0.1186 5.800 0.000000507 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.049 on 48 degrees of freedom ## Multiple R-squared: 0.412, Adjusted R-squared: 0.3998 ## F-statistic: 33.63 on 1 and 48 DF, p-value: 0.0000005067 The summary output rst gives a rough summary of the residual distribution. However, we will do more rigorous residual analyses in Chapter 12. The estimates of the model coefcients follow. The column Estimate contains the estimates for the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\) . The column Std. Error contains the estimated (frequentist) standard errors of the estimates. The last two columns contain the t-value and the p-value of the classical t-test for the null hypothesis that the coefcient equals zero. The last part of the summary output gives the parameter \\(\\sigma\\) of the model, named residual standard error and the residual degrees of freedom. We try to avoid the name residual standard error and use sigma instead, because \\(\\sigma\\) is not a measurement of uncertainty of a parameter estimate like the standard errors of the model coefcients are. \\(\\sigma\\) is a model parameter that describes how the observations scatter around the tted values, that is, it is a standard deviation. It is independent of sample size, whereas the standard errors of the estimates for the model parameters will decrease with increasing sample size. Such a standard error of the estimate of \\(\\sigma\\), however, is not given in the summary output. Note that, by using Bayesian methods, we could easily obtain the standard error of the estimated \\(\\sigma\\) by calculating the standard deviation of the posterior distribution of \\(\\sigma\\). The \\(R^2\\) and the adjusted \\(R^2\\) are explained in Section == Posterior Predictive Model Checking and Proportion of Explained Variance==. 11.2 Regression Variants: ANOVA, ANCOVA, and Multiple Regression 11.2.1 One-Way ANOVA The aim of analysis of variance (ANOVA) is to compare means of an outcome variable y between different groups (categorical variables). To do so in the frequentists framework, variances between and within the groups are compared (hence the name analysis of variance). If the variance between the group means is larger than expected by chance , we reject the null hypothesis of no differences between the groups. When doing an ANOVA in a Bayesian way, inference is based on the posterior distributions of the group means and the differences between the group means. One-way ANOVA means that we only have one explanatory variable (a factor). We illustrate the one-way ANOVA based on an example of simulated data (Fig. 11.6). We have measured weights of 30 virtual individuals for each of 3 groups. Possible research questions could be: How big are the differences between the group means? Are individuals from group 2 heavier than the ones from group 1? Which group mean is higher than 7.5 g? # settings for the simulation set.seed(626436) b0 &lt;- 12 # mean of group 1 (reference group) sigma &lt;- 2 # residual standard deviation b1 &lt;- 3 # difference between group 1 and group 2 b2 &lt;- -5 # difference between group 1 and group 3 n &lt;- 90 # sample size # generate data group &lt;- factor(rep(c(&quot;group 1&quot;,&quot;group 2&quot;, &quot;group 3&quot;), each=30)) simresid &lt;- rnorm(n, mean=0, sd=sigma) # simulate residuals y &lt;- b0 + as.numeric(group==&quot;group 2&quot;) * b1 + as.numeric(group==&quot;group 3&quot;) * b2 + simresid dat &lt;- tibble(y, group) # make figure dat %&gt;% ggplot(aes(x = group, y = y)) + geom_boxplot(fill = &quot;orange&quot;) + labs(y = &quot;Weight (g)&quot;, x = &quot;&quot;) + ylim(0, NA) Figure 11.6: Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box. An ANOVA is a linear regression with a categorical predictor variable instead of a continuous one. The categorical predictor variable with \\(k\\) levels is (as a default in R) transformed to \\(k-1\\) indicator variables. An indicator variable is a binary variable containing 0 and 1 where 1 indicates a specic level (a category of a nominal variable). Often, one indicator variable is constructed for every level except for the reference level. In our example, the categorical variable is group (\\(g\\)) with the three levels group 1, group 2, and group 3 (\\(k = 3\\)). Group 1 is taken as the reference level, and for each of the other two groups an indicator variable is constructed, \\(I(g_i = 2)\\) and \\(I(g_i = 3)\\). We can write the model as a formula: \\[\\begin{align} \\mu_i &amp;=\\beta_0 + \\beta_1 I(g_i=2) + \\beta_1 I(g_i=3) \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{11.5} \\end{align}\\] where \\(yi\\) is the \\(i\\)-th observation (weight measurement for individual i in our example), and \\(\\beta_{0,1,2}\\) are the model coefcients. The residual variance is \\(\\sigma^2\\). The model coefcients \\(\\beta_{0,1,2}\\) constitute the deterministic part of the model. From the model formula it follows that the group means, \\(m_g\\), are: \\[\\begin{align} m_1 &amp;=\\beta_0 \\\\ m_2 &amp;=\\beta_0 + \\beta_1 \\\\ m_3 &amp;=\\beta_0 + \\beta_2 \\\\ \\tag{11.6} \\end{align}\\] There are other possibilities to describe three group means with three parameters, for example: \\[\\begin{align} m_1 &amp;=\\beta_1 \\\\ m_2 &amp;=\\beta_2 \\\\ m_3 &amp;=\\beta_3 \\\\ \\tag{11.7} \\end{align}\\] In this case, the model formula would be: \\[\\begin{align} \\mu_i &amp;= \\beta_1 I(g_i=1) + \\beta_2 I(g_i=2) + \\beta_3 I(g_i=3) \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{11.8} \\end{align}\\] The way the group means are described is called the parameterization of the model. Different statistical softwares use different parameterizations. The parameterization used by R by default is the one shown in Equation (11.5). R automatically takes the rst level as the reference (the rst level is the rst one alphabetically unless the user denes a different order for the levels). The mean of the rst group (i.e., of the rst factor level) is the intercept, \\(b_0\\) , of the model. The mean of another factor level is obtained by adding, to the intercept, the estimate of the corresponding parameter (which is the difference from the reference group mean). R calls this parameterization treatment contrasts. The parameterization of the model is dened by the model matrix. In the case of a one-way ANOVA, there are as many columns in the model matrix as there are factor levels (i.e., groups); thus there are k factor levels and k model coefcients. Recall from Equation (11.3) that for each observation, the entry in the \\(j\\)-th column of the model matrix is multiplied by the \\(j\\)-th element of the model coefcients and the \\(k\\) products are summed to obtain the tted values. For a data set with \\(n = 5\\) observations of which the rst two are from group 1, the third from group 2, and the last two from group 3, the model matrix used for the parameterization described in Equation (11.6) is \\[\\begin{align} \\boldsymbol{X}= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{align}\\] If parameterization of Equation (11.7) were used, \\[\\begin{align} \\boldsymbol{X}= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{align}\\] Other possibilities of model parameterizations, particularly for ordered factors, are introduced in Section 11.2.4. To obtain the parameter estimates for model parameterized according to Equation (11.6) we t the model in R: # fit the model mod &lt;- lm(y~group) # parameter estimates mod ## ## Call: ## lm(formula = y ~ group) ## ## Coefficients: ## (Intercept) groupgroup 2 groupgroup 3 ## 12.367 2.215 -5.430 summary(mod)$sigma ## [1] 1.684949 The Intercept is \\(\\beta_0\\). The other coefcients are named with the factor name (group) and the factor level (either group 2 or group 3). These are \\(\\beta_1\\) and \\(\\beta_2\\) , respectively. Before drawing conclusions from an R output we need to examine whether the model assumptions are met, that is, we need to do a residual analysis as described in Chapter 12. Different questions can be answered using the above ANOVA: What are the group means? What is the difference in the means between group 1 and group 2? What is the difference between the means of the heaviest and lightest group? In a Bayesian framework we can directly assess how strongly the data support the hypothesis that the mean of the group 2 is larger than the mean of group 1. We rst simulate from the posterior distribution of the model parameters. library(arm) nsim &lt;- 1000 bsim &lt;- sim(mod, n.sim=nsim) Then we obtain the posterior distributions for the group means according to the parameterization of the model formula (Equation (11.6)). m.g1 &lt;- coef(bsim)[,1] m.g2 &lt;- coef(bsim)[,1] + coef(bsim)[,2] m.g3 &lt;- coef(bsim)[,1] + coef(bsim)[,3] The histograms of the simulated values from the posterior distributions of the three means are given in Fig. 11.7. The three means are well separated and, based on our data, we are condent that the group means differ. From these simulated posterior distributions we obtain the means and use the 2.5% and 97.5% quantiles as limits of the 95% credible intervals (Fig. 11.7, right). # save simulated values from posterior distribution in tibble post &lt;- tibble(`group 1` = m.g1, `group 2` = m.g2, `group 3` = m.g3) %&gt;% gather(&quot;groups&quot;, &quot;Group means&quot;) # histograms per group leftplot &lt;- ggplot(post, aes(x = `Group means`, fill = groups)) + geom_histogram(aes(y=..density..), binwidth = 0.5, col = &quot;black&quot;) + labs(y = &quot;Density&quot;) + theme(legend.position = &quot;top&quot;, legend.title = element_blank()) # plot mean and 95%-CrI rightplot &lt;- post %&gt;% group_by(groups) %&gt;% dplyr::summarise( mean = mean(`Group means`), CrI_lo = quantile(`Group means`, probs = 0.025), CrI_up = quantile(`Group means`, probs = 0.975)) %&gt;% ggplot(aes(x = groups, y = mean)) + geom_point() + geom_errorbar(aes(ymin = CrI_lo, ymax = CrI_up), width = 0.1) + ylim(0, NA) + labs(y = &quot;Weight (g)&quot;, x =&quot;&quot;) multiplot(leftplot, rightplot, cols = 2) Figure 11.7: Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% credible intervals obtained from the simulated distributions (right). To obtain the posterior distribution of the difference between the means of group 1 and group 2, we simply calculate this difference for each draw from the joint posterior distribution of the group means. d.g1.2 &lt;- m.g1 - m.g2 mean(d.g1.2) ## [1] -2.209551 quantile(d.g1.2, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## -3.128721 -1.342693 The estimated difference is -2.2095511. We are 95% sure that the difference between the means of group 1 and 2 is between -3.1287208 and -1.3426929. How strongly do the data support the hypothesis that the mean of group 2 is larger than the mean of group 1? To answer this question we calculate the proportion of the draws from the joint posterior distribution for which the mean of group 2 is larger than the mean of group 1. sum(m.g2 &gt; m.g1) / nsim ## [1] 1 This means that in all of the 1000 simulations from the joint posterior distribution, the mean of group 2 was larger than the mean of group 1. Therefore, there is a very high probability (i.e., it is close to 1; because probabilities are never exactly 1, we write &gt;0.999) that the mean of group 2 is larger than the mean of group 1. 11.2.2 Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression 11.2.3 Partial coefficients and some comments on collinearity Many biologists think that it is forbidden to include correlated predictor variables in a model. They use variance inflating factors (VIF) to omit some of the variables. However, omitting important variables from the model just because a correlation coefficient exceeds a threshold value can have undesirable effects. Here, we explain why and we present the usefulness and limits of partial coefficients (also called partial correlation or partial effects). We start with an example illustrating the usefulness of partial coefficients and then, give some guidelines on how to assess and deal with collinearity. As an example, we look at hatching dates of Snowfinches and how these dates relate to the date when snow melt started defined as the first date in the season when a minimum of 5% ground is snow free. A thorough analyses of the data is presented by Schano et al. (2021). An important question is how well can Snowfinches adjust their hatching dates to the snow conditions. Snowfinch nestlings grow faster when they are reared during the snow melt compared to after snow has completely melted, because their parents find nutrient rich insect larvae in the edges of melting snow patches. load(&quot;RData/snowfinch_hatching_date.rda&quot;) # Pearson&#39;s correlation coefficient cor(datsf$elevation, datsf$meltstart, use = &quot;pairwise.complete&quot;) ## [1] 0.3274635 mod &lt;- lm(meltstart~elevation, data=datsf) 100*coef(mod)[2] # change in meltstart with 100m change in elevation ## elevation ## 2.97768 Hatching dates of Snowfinch broods were inferred from citizen science data from the Alps, where snow melt starts later at higher elevations compared to lower elevations. Thus, the start of snow melt is correlated with elevation (Pearsons correlation coefficient 0.33). In average, snow starts melting 3 days later with every 100m increase in elevation. mod1 &lt;- lm(hatchday.mean~meltstart, data=datsf) mod1 ## ## Call: ## lm(formula = hatchday.mean ~ meltstart, data = datsf) ## ## Coefficients: ## (Intercept) meltstart ## 167.99457 0.06325 From a a normal linear regression of hatching date on the snow melt date, we obtain an estimate of 0.06 days delay in hatching date with one day later snow melt. This effect sizes describes what we see in the data, i.e. for data collected along an elevational gradient. Along the elevational gradient there are other factors than the start of snow melt that change such as average temperature, air pressure or sun radiation, and that may have an influence on the birds decision to start breeding. However, we are interested in the correlation between hatching date and date of snow melt independent of other factors changing with elevation. In other words, we would like to measure how much in average hatching date delays when snow melt starts one day later while all other factors are kept constant. This is called the partial effect of snow melt date. Therefore, we include elevation as a covariate in the model. library(arm) mod &lt;- lm(hatchday.mean~elevation + meltstart, data=datsf) mod ## ## Call: ## lm(formula = hatchday.mean ~ elevation + meltstart, data = datsf) ## ## Coefficients: ## (Intercept) elevation meltstart ## 154.383936 0.007079 0.037757 From this model, we obtain an estimate of 0.04 days delay in hatching date with one day later snow melt at a given elevation. We further get an estimate of 0.71 days later hatching date for each 100m shift in elevation. The difference in hatching date between early and late years (around one month difference in snow melt date) at a given elevation is 1.13 days (Figure 11.8). The correlation between hatching date and start of snow melt date we estimate in the first model (black regression line in Figure 11.8), that does not include elevation, is called pseudocorrelation because its is not purely caused by the date of snow melt. When we estimate the correlation within a constant elevation (coloured regression lines), it is much lower and closer to a causal relationship. However, in such observational studies, we never can be sure whether the partial coefficients can be interpreted as a causal relationship unless we include all factors that influence hatching date. Nevertheless, partial effects give much more insight into a system compared to univariate analyses because we can separated effects of simultaneously acting variables. Figure 11.8: Illustration of the partial coefficient of snow melt date in a model of hatching date. Panel A shows the entire raw data together with the regression lines drawn for three different elevations. The regression lines span the range of snow melt dates occurring at the respective elevation (shown in panel C). Panel B is the same as panel A, but zoomed in to the better see the regression lines and with an additional regression line (in black) from the model that does not take elevation into account is added. We have seen that it can be very useful to include more than one predictor variable in a model even if they are correlated with each other. In fact, there is nothing wrong with that. However, correlated predictors (collinearity) make things more complicated. For example, partial regression lines should not be drawn across the whole range of values of a variable, to avoid extrapolating out of data. At 2800 m asl snow melt never starts in the beginning of March. Therefore, the blue regression line would not make sense for snow melt dates in March. Further, sometimes correlations among predictors indicate that these predictors measure the same underlying aspect and we are actually interested in the effect of this underlying aspect on our response. For example, we could include also the date of the end of snow melt. Both variables, the start and the end of the snow melt measure the timing of snow melt. Including both as predictor in the model would result in partial coefficients that measure how much hatching date changes when the snow melt starts one day later, while the end date is constant. That interpretation is a mixture of the effect of timing and duration rather than of snow melt timing alone. Similarly, the coefficient of the end of snow melt measures a mixture of duration and timing. Thus, if we include two variables that are correlated because they measure the same aspect (just a little bit differently), we get coefficients that are hard to interpret and may not measure what we actually are interested in. In such a cases, we get easier to interpret model coefficients, if we include just one variable of each aspect that we are interested in, e.g. we could include one timing variable (e.g. start of snow melt) and the duration of snow melt that may or may not be correlated with the start of snow melt. To summarize, the decision of what to do with correlated predictors primarily relies on the question we are interested in, i.e., what exactly should the partial coefficients be an estimate for. A further drawback of collinearity is that model fitting can become difficult. When strong correlations are present, model fitting algorithms may fail. If they do not fail, the statistical uncertainty of the estimates often becomes large. This is because the partial coefficient of one variable needs to be estimated for constant values of the other predictors in the model which means that a reduced range of values is available as illustrated in Figure 11.8 C. However, if uncertainty intervals (confidence, credible or compatibility intervals) are reported alongside the estimates, then using correlated predictors in the same model is absolutely fine, if the fitting algorithm was successful. The correlations per se can be interesting. Further readings on how to visualize and analyse data with complex correlation structures: principal component analysis (Manly 1994) path analyses (e.g. Shipley (2009)) structural equation models (Hoyle 2012) 11.2.4 Ordered Factors and Contrasts 11.2.5 Quadratic and Higher Polynomial Terms "],["residualanalysis.html", "12 Assessing Model Assumptions 12.1 Model Assumptions 12.2 Independent and Identically Distributed 12.3 The QQ-Plot 12.4 Temporal Autocorrelation 12.5 Spatial Autocorrelation 12.6 Heteroscedasticity", " 12 Assessing Model Assumptions 12.1 Model Assumptions Every statistical model makes assumptions. We try to build models that reect the data-generating process as realistically as possible. However, a model never is the truth. Yet, all inferences drawn from a model, such as estimates of effect size or derived quantities with credible intervals, are based on the assumption that the model is true. However, if a model captures the datagenerating process poorly, for example, because it misses important structures (predictors, interactions, polynomials), inferences drawn from the model are probably biased and results become unreliable. In a (hypothetical) model that captures all important structures of the data generating process, the stochastic part, the difference between the observation and the tted value (the residuals), should only show random variation. Analyzing residuals is a very important part of the data analysis process. Residual analysis can be very exciting, because the residuals show what remains unexplained by the present model. Residuals can sometimes show surprising patterns and, thereby, provide deeper insight into the system. However, at this step of the analysis it is important not to forget the original research questions that motivated the study. Because these questions have been asked without knowledge of the data, they protect against data dredging. Of course, residual analysis may raise interesting new questions. Nonetheless, these new questions have emerged from patterns in the data, which might just be random, not systematic, patterns. The search for a model with good t should be guided by thinking about the process that generated the data, not by trial and error (i.e., do not try all possible variable combinations until the residuals look good; that is data dredging). All changes done to the model should be scientically justied. Usually, model complexity increases, rather than decreases, during the analysis. 12.2 Independent and Identically Distributed Usually, we model an outcome variable as independent and identically distributed (iid) given the model parameters. This means that all observations with the same predictor values behave like independent random numbers from the identical distribution. As a consequence, residuals should look iid. Independent means that: The residuals do not correlate with other variables (those that are included in the model as well as any other variable not included in the model). The residuals are not grouped (i.e., the means of any set of residuals should all be equal). The residuals are not autocorrelated (i.e., no temporal or spatial autocorrelation exist; Sections 12.4 and 12.5). Identically distributed means that: All residuals come from the same distribution. In the case of a linear model with normal error distribution (Chapter 11) the residuals are assumed to come from the same normal distribution. Particularly: The residual variance is homogeneous (homoscedasticity), that is, it does not depend on any predictor variable, and it does not change with the tted value. The mean of the residuals is zero over the whole range of predictor values. When numeric predictors (covariates) are present, this implies that the relationship between x and y can be adequately described by a straight line. Residual analysis is mainly done graphically. R makes it very easy to plot residuals to look at the different aspects just listed. As a rst example, we use the coal tit example from Chapter 11: Hier fehlt noch ein Teil aus dem BUCH. 12.3 The QQ-Plot xxx 12.4 Temporal Autocorrelation 12.5 Spatial Autocorrelation 12.6 Heteroscedasticity "],["lmer.html", "13 Linear Mixed Effect Models 13.1 Background", " 13 Linear Mixed Effect Models 13.1 Background 13.1.1 Why Mixed Effects Models? Mixed effects models (or hierarchical models; see Gelman &amp; Hill, 2007, for a discussion on the terminology) are used to analyze nonindependent, grouped, or hierarchical data. For example, when we measure growth rates of nestlings in different nests by taking mass measurements of each nestling several times during the nestling phase, the measurements are grouped within nestlings (because there are repeated measurements of each) and the nestlings are grouped within nests. Measurements from the same individual are likely to be more similar than measurements from different individuals, and individuals from the same nest are likely to be more similar than nestlings from different nests. Measurements of the same group (here, the groups are individuals or nests) are not independent. If the grouping structure of the data is ignored in the model, the residuals do not fulfill the independence assumption. Predictor variables can be measured on different hierarchical levels. For example, in each nest some nestlings were treated with a hormone implant whereas others received a placebo. Thus, the treatment is measured at the level of the individual, while clutch size is measured at the level of the nest. Clutch size was measured only once per nest but entered in the data file more than once (namely for each individual from the same nest). Similarly, all observations of one individual have the same value for treatment (but different values for individual measures such as weight). This results in pseudoreplication if we do not account for the hierarchical data structure in the model. Mixed models allow modeling of the hierarchical structure of the data and, therefore, account for pseudoreplication. Mixed models are further used to analyze variance components. For example, when the nestlings were cross-fostered so that they were not raised by their genetic parents, we would like to estimate the proportions of the variance (in a measurement, e.g., wing length) that can be assigned to genetic versus to environmental differences. Mixed models contain fixed and random effects. Note that, by definition, fixed and random effects are factors. Fixed effects have a finite (fixed) number of levels; for example, the factor sex has the levels male and female and (in many studies) nothing more. In contrast, random effects have a theoretically infinite number of levels of which we have measured a random sample. For example, we have measured 10 nests, but there are many more nests in the world that we have not measured. Normally, fixed effects have a low number of levels whereas random effects have a large number of levels (at least 3!). For fixed effects we are interested in the specific differences between levels (e.g., between males and females), whereas for random effects we are only interested in the between-level (¼ between-group, e.g., between-nest) variance rather than in differences between specific levels (e.g., nest A versus nest B). Typical fixed effects are: treatment, sex, age classes, or season. Typical random effects are: nest, individual, field, school, or study plot. It depends sometimes on the aim of the study whether a factor should be treated as fixed or random. When we would like to compare the average size of a corn cob between specific regions, then we include region as a fixed factor. However, when we would like to measure the size of a corn cob for a larger area within which we have measurements from a random sample of regions, then we treat region as a random factor. 13.1.2 Random Factors and Partial Pooling In a model with fixed factors, the differences of the group means to the mean of the reference group are separately estimated as model parameters. This produces k  1 (independent) model parameters, where k ¼ number of groups (or number of factor levels). In contrast, for a random factor, only one parameter, namely the between-group variance, is estimated. To estimate this variance, we look at the differences of the group means to the population mean; that is, we look at k differences from the population mean. These k differences are not independent. They are assumed to be realizations of the same (in most cases normal) distribution with mean zero. They are like residuals, and we usually call them bg; each is the difference, b, between the mean of group, g, and the mean of all groups. The variance of the bg values is the between-group variance. Treating a factor as a random factor is equivalent to partial pooling of the data. There are three different ways to obtain means for grouped data. First, the grouping structure of the data can be ignored. This is called complete pooling (left panel in Figure (pooling)). Figure 13.1: Three possibilities to obtain group means for grouped data byi: complete pooling,partial pooling, and no pooling. Open symbols = data, orange dots with vertical bars = group means with 95% credible intervals, horizontal black line with shaded interval = population mean with 95% credible interval. Second, group means may be estimated separately for each group. In this case, the data from all other groups are ignored when estimating a group mean. No pooling occurs in this case (right panel in Figure 7-1). Third, the data of the different groups can be partially pooled (i.e., treated as a random effect). Thereby, the group means are weighted averages of the population mean and the unpooled group means. The weights are proportional to sample size and the inverse of the variance (see Gelman &amp; Hill, 2007, p. 252). "],["glm.html", "14 Generalized linear models 14.1 Introduction 14.2 Summary", " 14 Generalized linear models THIS CHAPTER IS UNDER CONSTRUCTION!!! 14.1 Introduction 14.2 Summary xxx "],["glmm.html", "15 Generalized linear mixed models 15.1 Introduction 15.2 Summary", " 15 Generalized linear mixed models 15.1 Introduction THIS CHAPTER IS UNDER CONSTRUCTION!!! In chapter 13 on linear mixed effect models we have introduced how to analyze metric outcome variables for which a normal distribution of residuals can be assumed (potentially after transformation), when the data have a hierarchical structure and, as a consequence, observations are not independent. In chapter 14 on generalized linear models we have introduced how to analyze outcome variables for which a normal error distribution can not be assumed, as for example binary outcomes or count data. More precisely, we have extended modelling outcomes with normal error to modelling outcomes with error distributions from the exponential family (e.g., binomial or Poisson). Generalized linear mixed models (GLMM) combine the two complexities and are used to analyze outcomes with a non-normal error distribution when the data have a hierarchical structure. In this chapter, we will show how to analyze such data. Remember, a hierarchical structure of the data means that the data are collected at different levels, for example smaller and larger spatial units, or include repeated measurements in time on a specific subject. Typically, the outcome variable is measured/observed at the lowest level but other variables may be measured at different levels. A first example is introduced in the next section. 15.1.1 Binomial Mixed Model 15.1.1.1 Background To illustrate the binomial mixed model we use a subset of a data set used by Grüebler, Korner-Nievergelt, and Von Hirschheydt (2010) on barn swallow Hirundo rustica nestling survival (we selected a nonrandom sample to be able to fit a simple model; hence, the results do not add unbiased knowledge about the swallow biology!). For 63 swallow broods, we know the clutch size and the number of the nestlings that fledged. The broods came from 51 farms (larger unit), thus some of the farms had more than one brood. Note that each farm can harbor one or several broods, and the broods are nested within farms (as opposed to crossed, see chapter 13), i.e., each brood belongs to only one farm. There are three predictors measured at the level of the farm: colony size (the number of swallow broods on that farm), cow (whether there are cows on the farm or not), and dung heap (the number of dung heaps, piles of cow dung, within 500 m of the farm). The aim was to assess how swallows profit from insects that are attracted by livestock on the farm and by dung heaps. Broods from the same farm are not independent of each other because they belong to the same larger unit (farm), and thus share the characteristics of the farm (measured or unmeasured). Predictor variables were measured at the level of the farm, and are thus the same for all broods from a farm. In the model described and fitted below, we account for the non-independence of these clutches when building the model by including a random intercept per farm to model random variation between farms. The outcome variable is a proportion (proportion fledged from clutch) and thus consists of two values for each observation, as seen with the binomial model without random factors (Section 14.2.2): the number of chicks that fledged (successes) and the number of chicks that died (failures), i.e., the clutch size minus number that fledged. The random factor farm adds a farm-specific deviation \\(b_g\\) to the intercept in the linear predictor. These deviations are modeled as normally distributed with mean \\(0\\) and standard deviation \\(\\sigma_g\\). \\[ y_i \\sim Binom\\left(p_i, n_i\\right)\\\\ logit\\left(p_i\\right) = \\beta_0 + b_{g[i]} + \\beta_1\\;colonysize_i + \\beta_2\\;I\\left(cow_i = 1\\right) + \\beta_3\\;dungheap_i\\\\ b_g \\sim Norm\\left(0, \\sigma_g\\right) \\] # Data on Barn Swallow (Hirundo rustica) nestling survival on farms # (a part of the data published in Grüebler et al. 2010, J Appl Ecol 47:1340-1347) library(blmeco) data(swallowfarms) #?swallowfarms # to see the documentation of the data set dat &lt;- swallowfarms str(dat) ## &#39;data.frame&#39;: 63 obs. of 6 variables: ## $ farm : int 1001 1002 1002 1002 1004 1008 1008 1008 1010 1016 ... ## $ colsize: int 1 4 4 4 1 11 11 11 3 3 ... ## $ cow : int 1 1 1 1 1 1 1 1 0 1 ... ## $ dung : int 0 0 0 0 1 1 1 1 2 2 ... ## $ clutch : int 8 9 8 7 13 7 9 16 10 8 ... ## $ fledge : int 8 0 6 5 9 3 7 4 9 8 ... # check number of farms in the data set length(unique(dat$farm)) ## [1] 51 15.1.1.2 Fitting a Binomial Mixed Model in R 15.1.1.2.1 Using the glmer function dat$colsize.z &lt;- scale(dat$colsize) # z-transform values for better model convergence dat$dung.z &lt;- scale(dat$dung) dat$die &lt;- dat$clutch - dat$fledge dat$farm.f &lt;- factor(dat$farm) # for clarity we define farm as a factor The glmer function uses the standard way to formulate a statistical model in R, with the outcome on the left, followed by the ~ symbol, meaning explained by, followed by the predictors, which are separated by +. The notation for the random factor with only a random intercept was introduced in chapter 13 and is (1|farm.f) here. Remember that for fitting a binomial model we have to provide the number of successful events (number of fledglings that survived) and the number of failures (those that died) within a two-column matrix that we create using the function cbind. # fit GLMM using glmer function from lme4 package library(lme4) mod.glmer &lt;- glmer(cbind(fledge,die) ~ colsize.z + cow + dung.z + (1|farm.f) , data=dat, family=binomial) 15.1.1.2.2 Assessing Model Assumptions for the glmer fit The residuals of the model look fairly normal (top left panel of Figure 15.1 with slightly wider tails. The random intercepts for the farms look perfectly normal as they should. The plot of the residuals vs. fitted values (bottom left panel) shows a slight increase in the residuals with increasing fitted values. Positive correlations between the residuals and the fitted values are common in mixed models due to the shrinkage effect (chapter 13). Due to the same reason the fitted proportions slightly overestimate the observed proportions when these are large, but underestimate them when small (bottom right panel). What is looking like a lack of fit here can be seen as preventing an overestimation of the among farm variance based on the assumption that the farms in the data are a random sample of farms belonging to the same population. The mean of the random effects is close to zero as it should. We check that because sometimes the glmer function fails to correctly separate the farm-specific intercepts from the overall intercept. A non-zero mean of random effects does not mean a lack of fit, but a failure of the model fitting algorithm. In such a case, we recommend using a different fitting algorithm, e.g. brm (see below) or stan_glmer from the rstanarm package. A slight overdispersion (approximated dispersion parameter &gt;1) seems to be present, but nothing to worry about. par(mfrow=c(2,2), mar=c(3,5,1,1)) # check normal distribution of residuals qqnorm(resid(mod.glmer), main=&quot;qq-plot residuals&quot;) qqline(resid(mod.glmer)) # check normal distribution of random intercepts qqnorm(ranef(mod.glmer)$farm.f[,1], main=&quot;qq-plot, farm&quot;) qqline(ranef(mod.glmer)$farm.f[,1]) # residuals vs fitted values to check homoscedasticity plot(fitted(mod.glmer), resid(mod.glmer)) abline(h=0) # plot data vs. predicted values dat$fitted &lt;- fitted(mod.glmer) plot(dat$fitted,dat$fledge/dat$clutch) abline(0,1) Figure 15.1: Diagnostic plots to assess model assumptions for mod.glmer. Uppper left: quantile-quantile plot of the residuals vs. theoretical quantiles of the normal distribution. Upper rihgt: quantile-quantile plot of the random effects farm. Lower left: residuals vs. fitted values. Lower right: observed vs. fitted values. # check distribution of random effects mean(ranef(mod.glmer)$farm.f[,1]) ## [1] -0.001690303 # check for overdispersion dispersion_glmer(mod.glmer) ## [1] 1.192931 detach(package:lme4) 15.1.1.2.3 Using the brm function Now we fit the same model using the function brm from the R package brms. This function allows fitting Bayesian generalized (non-)linear multivariate multilevel models using Stan (Betancourt 2013) for full Bayesian inference. We shortly introduce the fitting algorithm used by Stan, Hamiltonian Monte Carlo, in chapter 18. When using the function brm there is no need to install rstan or write the model in Stan-language. A wide range of distributions and link functions are supported, and the function offers many things more. Here we use it to fit the model as specified by the formula object above. Note that brm requires that a binomial outcome is specified in the format successes|trials(), which is the number of fledged nestlings out of the total clutch size in our case. In contrast, the glmer function required to specify the number of nestlings that fledged and died (which together sum up to clutch size), in the format cbind(successes, failures). The family is also called binomial in brm, but would be bernoulli for a binary outcome, whereas glmer would use binomial in both situations (Bernoulli distribution is a special case of the binomial). However, it is slightly confusing that (at the time of writing this chapter) the documentation for brmsfamily did not mention the binomial family under Usage, where it probably went missing, but it is mentioned under Arguments for the argument family. Prior distributions are an integral part of a Bayesian model, therefore we need to specify prior distributions. We can see what default prior distributions brm is using by applying the get_prior function to the model formula. The default prior for the effect sizes is a flat prior which gives a density of 1 for any value between minus and plus infinity. Because this is not a proper probability distribution it is also called an improper distribution. The intercept gets a t-distribution with mean of 0, standard deviation of 2.5 and 3 degrees of freedoms. Transforming this t-distribution to the proportion scale (using the inverse-logit function) becomes something similar to a uniform distribution between 0 and 1 that can be seen as non-informative for a probability. For the among-farm standard deviation, it uses the same t-distribution as for the intercept. However, because variance parameters such as standard deviations only can take on positive numbers, it will use only the positive half of the t-distribution (this is not seen in the output of get_prior). When we have no prior information on any parameter, or if we would like to base the results solely on the information in the data, we specify weakly informative prior distributions that do not noticeably affect the results but they will facilitate the fitting algorithm. This is true for the priors of the intercept and among-farm standard deviation. However, for the effect sizes, we prefer specifying more narrow distributions (see chapter 10). To do so, we use the function prior. To apply MCMC sampling we need some more arguments: warmup specifies the number of iterations during which we allow the algorithm to be adapted to our specific model and to converge to the posterior distribution. These iterations should be discarded (similar to the burn-in period when using, e.g., Gibbs sampling); iter specifies the total number of iterations (including those discarded); chains specifies the number of chains; init specifies the starting values of the iterations. By default (init=NULL) or by setting init=\"random\" the initial values are randomly chosen which is recommended because then different initial values are chosen for each chain which helps to identify non-convergence. However, sometimes random initial values cause the Markov chains to behave badly. Then you can either use the maximum likelihood estimates of the parameters as starting values, or simply ask the algorithm to start with zeros. thin specifies the thinning of the chain, i.e., whether all iterations should be kept (thin=1) or for example every 4th only (thin=4); cores specifies the number of cores used for the algorithm; seed specifies the random seed, allowing for replication of results. library(brms) # check which parameters need a prior get_prior(fledge|trials(clutch) ~ colsize.z + cow + dung.z + (1|farm.f), data=dat, family=binomial(link=&quot;logit&quot;)) ## prior class coef group resp dpar nlpar bound ## (flat) b ## (flat) b colsize.z ## (flat) b cow ## (flat) b dung.z ## student_t(3, 0, 2.5) Intercept ## student_t(3, 0, 2.5) sd ## student_t(3, 0, 2.5) sd farm.f ## student_t(3, 0, 2.5) sd Intercept farm.f ## source ## default ## (vectorized) ## (vectorized) ## (vectorized) ## default ## default ## (vectorized) ## (vectorized) # specify own priors myprior &lt;- prior(normal(0,5), class=&quot;b&quot;) mod.brm &lt;- brm(fledge|trials(clutch) ~ colsize.z + cow + dung.z + (1|farm.f) , data=dat, family=binomial(link=&quot;logit&quot;), prior=myprior, warmup = 500, iter = 2000, chains = 2, init = &quot;random&quot;, cores = 2, seed = 123) # note: thin=1 is default and we did not change this here. 15.1.1.2.4 Checking model convergence for the brm fit We first check whether we find warnings in the R console about problems of the fitting algorithm. Warnings should be taken seriously. Often, we find help in the Stan online documentation (or when typing launch_shinystan(mod.brm) into the R-console) what to change when calling the brm function to get a fit that is running smoothly. Once, we get rid of all warnings, we need to check how well the Markov chains mixed. We can either do that by scanning through the many diagnostic plots given by launch_shinystan(mod) or create the most important plots ourselves such as the traceplot (Figure 15.2). par(mar=c(2,2,2,2)) mcmc_plot(mod.brm, type = &quot;trace&quot;) Figure 15.2: Traceplot of the Markov chains. After convergence, both Markov chains should sample from the same stationary distribution. Indications of non-convergence would be, if the two chains diverge or vary around different means. 15.1.1.2.5 Checking model fit by posterior predictive model checking To assess how well the model fits to the data we do posterior predictive model checking (Chapter 16). For binomial as well as for Poisson models comparing the standard deviation of the data with those of replicated data from the model is particularly important. If the standard deviation of the real data would be much higher compared to the ones of the replicated data from the model, overdispersion would be an issue. However, here, the model is able to capture the variance in the data correctly (Figure 15.3). The fitted vs observed plot also shows a good fit. yrep &lt;- posterior_predict(mod.brm) sdyrep &lt;- apply(yrep, 1, sd) par(mfrow=c(1,3), mar=c(3,4,1,1)) hist(yrep, freq=FALSE, main=NA, xlab=&quot;Number of fledglings&quot;) hist(dat$fledge, add=TRUE, col=rgb(1,0,0,0.3), freq=FALSE) legend(10, 0.15, fill=c(&quot;grey&quot;,rgb(1,0,0,0.3)), legend=c(&quot;yrep&quot;, &quot;y&quot;)) hist(sdyrep) abline(v=sd(dat$fledge), col=&quot;red&quot;, lwd=2) plot(fitted(mod.brm)[,1], dat$fledge, pch=16, cex=0.6) abline(0,1) Figure 15.3: Posterior predictive model checking: Histogram of the number of fledglings simulated from the model together with a histogram of the real data, and a histogram of the standard deviations of replicated data from the model together with the standard deviation of the data (vertical line in red). The third plot gives the fitted vs. observed values. After checking the diagnostic plots, the posterior predictive model checking and the general model fit, we assume that the model describes the data generating process reasonably well, so that we can proceed to drawing conclusions. 15.1.1.3 Drawing Conclusions The generic summary function gives us the results for the model object containing the fitted model, and works for both the model fitted with glmer and brm. Lets start having a look at the summary from mod.glmer. The summary provides the fitting method, the model formula, statistics for the model fit including the Akaike information criterion (AIC), the Bayesian information criterion (BIC), the scaled residuals, the random effects variance and information about observations and groups, a table with coefficient estimates for the fixed effects (with standard errors and a z-test for the coefficient) and correlations between fixed effects. We recommend to always check if the number of observations and groups, i.e., 63 barn swallow nests from 51 farms here, is correct. This information shows if the glmer function has correctly recognized the hierarchical structure in the data. Here, this is correct. To assess the associations between the predictor variables and the outcome analyzed, we need to look at the column Estimate in the table of fixed effects. This column contains the estimated model coefficients, and the standard error for these estimates is given in the column Std. Error, along with a z-test for the null hypothesis of a coefficient of zero. In the random effects table, the among farm variance and standard deviation (square root of the variance) are given. The function confint shows the 95% confidence intervals for the random effects (.sig01) and fixed effects estimates. In the summary output from mod.brm we see the model formula and some information on the Markov chains after the warm-up. In the group-level effects (between group standard deviations) and population-level effects (effect sizes, model coefficients) tables some summary statistics of the posterior distribution of each parameter are given. The Estimate is the mean of the posterior distribution, the Est.Error is the standard deviation of the posterior distribution (which is the standard error of the parameter estimate). Then we see the lower and upper limit of the 95% credible interval. Also, some statistics for measuring how well the Markov chains converged are given: the Rhat and the effective sample size (ESS). The bulk ESS tells us how many independent samples we have to describe the posterior distribution, and the tail ESS tells us on how many samples the limits of the 95% credible interval is based on. Because we used the logit link function, the coefficients are actually on the logit scale and are a bit difficult to interpret. What we can say is that positive coefficients indicate an increase and negative coefficients indicate a decrease in the proportion of nestlings fledged. For continuous predictors, as colsize.z and dung.z, this coefficient refers to the change in the logit of the outcome with a change of one in the predictor (e.g., for colsize.z an increase of one corresponds to an increase of a standard deviation of colsize). For categorical predictors, the coefficients represent a difference between one category and another (reference category is the one not shown in the table). To visualize the coefficients we could draw effect plots. # glmer summary(mod.glmer) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: cbind(fledge, die) ~ colsize.z + cow + dung.z + (1 | farm.f) ## Data: dat ## ## AIC BIC logLik deviance df.resid ## 282.5 293.2 -136.3 272.5 58 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.2071 -0.4868 0.0812 0.6210 1.8905 ## ## Random effects: ## Groups Name Variance Std.Dev. ## farm.f (Intercept) 0.2058 0.4536 ## Number of obs: 63, groups: farm.f, 51 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.09533 0.19068 -0.500 0.6171 ## colsize.z 0.05087 0.11735 0.434 0.6646 ## cow 0.39370 0.22692 1.735 0.0827 . ## dung.z -0.14236 0.10862 -1.311 0.1900 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) clsz.z cow ## colsize.z 0.129 ## cow -0.828 -0.075 ## dung.z 0.033 0.139 -0.091 confint.95 &lt;- confint(mod.glmer); confint.95 ## 2.5 % 97.5 % ## .sig01 0.16809483 0.7385238 ## (Intercept) -0.48398346 0.2863200 ## colsize.z -0.18428769 0.2950063 ## cow -0.05360035 0.8588134 ## dung.z -0.36296714 0.0733620 # brm summary(mod.brm) ## Family: binomial ## Links: mu = logit ## Formula: fledge | trials(clutch) ~ colsize.z + cow + dung.z + (1 | farm.f) ## Data: dat (Number of observations: 63) ## Samples: 2 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 3000 ## ## Group-Level Effects: ## ~farm.f (Number of levels: 51) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.54 0.15 0.25 0.84 1.00 918 1380 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.09 0.21 -0.50 0.30 1.00 2911 2443 ## colsize.z 0.06 0.13 -0.20 0.32 1.00 2413 1601 ## cow 0.40 0.24 -0.06 0.88 1.00 3045 2328 ## dung.z -0.15 0.12 -0.39 0.08 1.00 2871 2496 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). From the results we conclude that in farms without cows (when cow=0) and for average colony sizes (when colsize.z=0) and average number of dung heaps (when dung.z=0) the average nestling survival of Barn swallows is the inverse-logit function of the Intercept, thus, plogis(-0.09) = 0.48 with a 95% uncertainty interval of 0.38 - 0.57. We further see that colony size and number of dung heaps are less important than whether cows are present or not. Their estimated partial effect is small and their uncertainty interval includes only values close to zero. However, whether cows are present or not may be important for the survival of nestlings. The average nestling survival in farms with cows is plogis(-0.09+0.4) = 0.58. For getting the uncertainty interval of this survival estimate, we need to do the calculation for every simulation from the posterior distribution of both parameters. bsim &lt;- posterior_samples(mod.brm) # survival of nestlings on farms with cows: survivalest &lt;- plogis(bsim$b_Intercept + bsim$b_cow) quantile(survivalest, probs=c(0.025, 0.975)) # 95% uncertainty interval ## 2.5% 97.5% ## 0.5107279 0.6460462 In medical research, it is standard to report the fixed-effects coefficients from GLMM with binomial or Bernoulli error as odds ratios by taking the exponent (R function exp for \\(e^{()}\\)) of the coefficient on the logit-scale. For example, the coefficient for cow from mod.glmer, 0.39 (95% CI from -0.05 to -0.05), represents an odds ratio of exp( 0.39)=1.48 (95% CI from 0.95 to 0.95). This means that the odds for fledging (vs. not fledging) from a clutch from a farm with livestock present is about 1.5 times larger than the odds for fledging if no livestock is present (relative effect). 15.2 Summary "],["modelchecking.html", "16 Posterior predictive model checking 16.1 Introduction 16.2 Summary", " 16 Posterior predictive model checking THIS CHAPTER IS UNDER CONSTRUCTION!!! 16.1 Introduction 16.2 Summary xxx "],["model-comparison.html", "17 Model comparison and multimodel inference 17.1 Introduction 17.2 Summary", " 17 Model comparison and multimodel inference THIS CHAPTER IS UNDER CONSTRUCTION!!! 17.1 Introduction 17.2 Summary xxx "],["stan.html", "18 MCMC using Stan 18.1 Background 18.2 Install rstan 18.3 Writing a Stan model 18.4 Run Stan from R Further reading", " 18 MCMC using Stan 18.1 Background Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. 18.2 Install rstan In this book we use the program Stan to draw random samples from the joint posterior distribution of the model parameters given a model, the data, prior distributions, and initial values. To do so, it uses the no-U-turn sampler, which is a type of Hamiltonian Monte Carlo simulation (Hoffman and Gelman 2014; Betancourt 2013), and optimization-based point estimation. These algorithms are more efficient than the ones implemented in BUGS programs and they can handle larger data sets. Stan works particularly well for hierar- chical models (Betancourt and Girolami 2013). Stan runs on Windows, Mac, and Linux and can be used via the R interface rstan. Stan is automatically installed when the R package rstan is installed. For installing rstan, it is advised to follow closely the system-specific instructions. 18.3 Writing a Stan model The statistical model is written in the Stan language and saved in a text file. The Stan language is rather strict, forcing the user to write unambiguous models. Stan is very well documented and the Stan Documentation contains a comprehensive Language Manual, a Wiki documentation and various tutorials. We here provide a normal regression with one predictor variable as a worked example. The entire Stan model is as following (saved as linreg.stan) data { int&lt;lower=0&gt; n; vector[n] y; vector[n] x; } parameters { vector[2] beta; real&lt;lower=0&gt; sigma; } model { //priors beta ~ normal(0,5); sigma ~ cauchy(0,5); // likelihood y ~ normal(beta[1] + beta[2] * x, sigma); } A Stan model consists of different named blocks. These blocks are (from first to last): data, transformed data, parameters, trans- formed parameters, model, and generated quantities. The blocks must appear in this order. The model block is mandatory; all other blocks are optional. In the data block, the type, dimension, and name of every variable has to be declared. Optionally, the range of possible values can be specified. For example, vector[N] y; means that y is a vector (type real) of length N, and int&lt;lower=0&gt; N; means that N is an integer with nonnegative values (the bounds, here 0, are included). Note that the restriction to a possible range of values is not strictly necessary but this will help specifying the correct model and it will improve speed. We also see that each line needs to be closed by a column sign. In the parameters block, all model parameters have to be defined. The coefficients of the linear predictor constitute a vector of length 2, vector[2] beta;. Alternatively, real beta[2]; could be used. The sigma parameter is a one-number parameter that has to be positive, therefore real&lt;lower=0&gt; sigma;. The model block contains the model specification. Stan functions can handle vectors and we do not have to loop over all observations as typical for BUGS . Here, we use a Cauchy distribution as a prior distribution for sigma. This distribution can have negative values, but because we defined the lower limit of sigma to be 0 in the parameters block, the prior distribution actually used in the model is a truncated Cauchy distribution (truncated at zero). In Chapter 10.2 we explain how to choose prior distributions. Further characteristics of the Stan language that are good to know include: The variance parameter for the normal distribution is specified as the standard deviation (like in R but different from BUGS, where the precision is used). If no prior is specified, Stan uses a uniform prior over the range of possible values as specified in the parameter block. Variable names must not contain periods, for example, x.z would not be allowed, but x_z is allowed. To comment out a line, use double forward-slashes //. 18.4 Run Stan from R We fit the model to simulated data. Stan needs a vector containing the names of the data objects. In our case, x, y, and N are objects that exist in the R console. The function stan() starts Stan and returns an object containing MCMCs for every model parameter. We have to specify the name of the file that contains the model specification, the data, the number of chains, and the number of iterations per chain we would like to have. The first half of the iterations of each chain is declared as the warm-up. During the warm-up, Stan is not simulating a Markov chain, because in every step the algorithm is adapted. After the warm-up the algorithm is fixed and Stan simulates Markov chains. library(rstan) # Simulate fake data n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # random numbers of the covariate simresid &lt;- rnorm(n, 0, sd=sigma) # residuals y &lt;- b0 + b1*x + simresid # calculate y, i.e. the data # Bundle data into a list datax &lt;- list(n=length(y), y=y, x=x) # Run STAN fit &lt;- stan(file = &quot;stanmodels/linreg.stan&quot;, data=datax, verbose = FALSE) ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.1 seconds (Warm-up) ## Chain 1: 0.1 seconds (Sampling) ## Chain 1: 0.2 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.116 seconds (Warm-up) ## Chain 2: 0.085 seconds (Sampling) ## Chain 2: 0.201 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.122 seconds (Warm-up) ## Chain 3: 0.078 seconds (Sampling) ## Chain 3: 0.2 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.116 seconds (Warm-up) ## Chain 4: 0.1 seconds (Sampling) ## Chain 4: 0.216 seconds (Total) ## Chain 4: Further reading Stan-Homepage: It contains the documentation for Stand a a lot of tutorials. "],["ridge-regression.html", "19 Ridge Regression 19.1 Introduction", " 19 Ridge Regression THIS CHAPTER IS UNDER CONSTRUCTION!!! We should provide an example in Stan. 19.1 Introduction # Settings library(R2OpenBUGS) bugslocation &lt;- &quot;C:/Program Files/OpenBUGS323/OpenBugs.exe&quot; # location of OpenBUGS bugsworkingdir &lt;- file.path(getwd(), &quot;BUGS&quot;) # Bugs working directory #------------------------------------------------------------------------------- # Simulate fake data #------------------------------------------------------------------------------- library(MASS) n &lt;- 50 # sample size b0 &lt;- 1.2 b &lt;- rnorm(5, 0, 2) Sigma &lt;- matrix(c(10,3,3,2,1, 3,2,3,2,1, 3,3,5,3,2, 2,2,3,10,3, 1,1,2,3,15),5,5) Sigma x &lt;- mvrnorm(n = n, rep(0, 5), Sigma) simresid &lt;- rnorm(n, 0, sd=3) # residuals x.z &lt;- x for(i in 1:ncol(x)) x.z[,i] &lt;- (x[,i]-mean(x[,i]))/sd(x[,i]) y &lt;- b0 + x.z%*%b + simresid # calculate y, i.e. the data #------------------------------------------------------------------------------- # Function to generate initial values #------------------------------------------------------------------------------- inits &lt;- function() { list(b0=runif(1, -2, 2), b=runif(5, -2, 2), sigma=runif(1, 0.1, 2)) } #------------------------------------------------------------------------------- # Run OpenBUGS #------------------------------------------------------------------------------- parameters &lt;- c(&quot;b0&quot;, &quot;b&quot;, &quot;sigma&quot;) lambda &lt;- c(1, 2, 10, 25, 50, 100, 500, 1000, 10000) bs &lt;- matrix(ncol=length(lambda), nrow=length(b)) bse &lt;- matrix(ncol=length(lambda), nrow=length(b)) for(j in 1:length(lambda)){ datax &lt;- list(y=as.numeric(y), x=x, n=n, mb=rep(0, 5), lambda=lambda[j]) fit &lt;- bugs(datax, inits, parameters, model.file=&quot;ridge_regression.txt&quot;, n.thin=1, n.chains=2, n.burnin=5000, n.iter=10000, debug=FALSE, OpenBUGS.pgm = bugslocation, working.directory=bugsworkingdir) bs[,j] &lt;- fit$mean$b bse[,j] &lt;- fit$sd$b } range(bs) plot(1:length(lambda), seq(-2, 1, length=length(lambda)), type=&quot;n&quot;) colkey &lt;- rainbow(length(b)) for(j in 1:nrow(bs)){ lines(1:length(lambda), bs[j,], col=colkey[j], lwd=2) lines(1:length(lambda), bs[j,]-2*bse[j,], col=colkey[j], lty=3) lines(1:length(lambda), bs[j,]+2*bse[j,], col=colkey[j], lty=3) } abline(h=0) round(fit$summary,2) #------------------------------------------------------------------------------- # Run WinBUGS #------------------------------------------------------------------------------- library(R2WinBUGS) bugsdir &lt;- &quot;C:/Users/fk/WinBUGS14&quot; # mod &lt;- bugs(datax, inits= inits, parameters, model.file=&quot;normlinreg.txt&quot;, n.chains=2, n.iter=1000, n.burnin=500, n.thin=1, debug=TRUE, bugs.directory=bugsdir, program=&quot;WinBUGS&quot;, working.directory=bugsworkingdir) #------------------------------------------------------------------------------- # Test convergence and make inference #------------------------------------------------------------------------------- library(blmeco) # Make Figure 12.2 par(mfrow=c(3,1)) historyplot(fit, &quot;beta0&quot;) historyplot(fit, &quot;beta1&quot;) historyplot(fit, &quot;sigmaRes&quot;) # Parameter estimates print(fit$summary, 3) # Make predictions for covariate values between 10 and 30 newdat &lt;- data.frame(x=seq(10, 30, length=100)) Xmat &lt;- model.matrix(~x, data=newdat) predmat &lt;- matrix(ncol=fit$n.sim, nrow=nrow(newdat)) for(i in 1:fit$n.sim) predmat[,i] &lt;- Xmat%*%c(fit$sims.list$beta0[i], fit$sims.list$beta1[i]) newdat$lower.bugs &lt;- apply(predmat, 1, quantile, prob=0.025) newdat$upper.bugs &lt;- apply(predmat, 1, quantile, prob=0.975) plot(y~x, pch=16, las=1, cex.lab=1.4, cex.axis=1.2, type=&quot;n&quot;, main=&quot;&quot;) polygon(c(newdat$x, rev(newdat$x)), c(newdat$lower.bugs, rev(newdat$upper.bugs)), col=grey(0.7), border=NA) abline(c(fit$mean$beta0, fit$mean$beta1), lwd=2) box() points(x,y) "],["SEM.html", "20 Structural equation models 20.1 Introduction", " 20 Structural equation models THIS CHAPTER IS UNDER CONSTRUCTION!!! We should provide an example in Stan. 20.1 Introduction ------------------------------------------------------------------------------------------------------ # General settings #------------------------------------------------------------------------------------------------------ library(MASS) library(rjags) library(MCMCpack) #------------------------------------------------------------------------------------------------------ # Simulation #------------------------------------------------------------------------------------------------------ n &lt;- 100 heffM &lt;- 0.6 # effect of H on M heffCS &lt;- 0.0 # effect of H on Clutch size meffCS &lt;- 0.6 # effect of M on Clutch size SigmaM &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffm1 &lt;- 0.6 meffm2 &lt;- 0.7 SigmaH &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffh1 &lt;- 0.6 meffh2 &lt;- -0.7 # Latente Variablen H &lt;- rnorm(n, 0, 1) M &lt;- rnorm(n, heffM * H, 0.1) # Clutch size CS &lt;- rnorm(n, heffCS * H + meffCS * M, 0.1) # Indicators eM &lt;- cbind(meffm1 * M, meffm2 * M) datM &lt;- matrix(NA, ncol = 2, nrow = n) eH &lt;- cbind(meffh1 * H, meffh2 * H) datH &lt;- matrix(NA, ncol = 2, nrow = n) for(i in 1:n) { datM[i,] &lt;- mvrnorm(1, eM[i,], SigmaM) datH[i,] &lt;- mvrnorm(1, eH[i,], SigmaH) } #------------------------------------------------------------------------------ # JAGS Model #------------------------------------------------------------------------------ dat &lt;- list(datM = datM, datH = datH, n = n, CS = CS, #H = H, M = M, S3 = matrix(c(1,0,0,1),nrow=2)/1) # Function to create initial values inits &lt;- function() { list( meffh = runif(2, 0, 0.1), meffm = runif(2, 0, 0.1), heffM = runif(1, 0, 0.1), heffCS = runif(1, 0, 0.1), meffCS = runif(1, 0, 0.1), tauCS = runif(1, 0.1, 0.3), tauMH = runif(1, 0.1, 0.3), tauH = rwish(3,matrix(c(.02,0,0,.04),nrow=2)), tauM = rwish(3,matrix(c(.02,0,0,.04),nrow=2)) # M = as.numeric(rep(0, n)) ) } t.n.thin &lt;- 50 t.n.chains &lt;- 2 t.n.burnin &lt;- 20000 t.n.iter &lt;- 50000 # Run JAGS jagres &lt;- jags.model(&#39;JAGS/BUGSmod1.R&#39;,data = dat, n.chains = t.n.chains, inits = inits, n.adapt = t.n.burnin) params &lt;- c(&quot;meffh&quot;, &quot;meffm&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) mod &lt;- coda.samples(jagres, params, n.iter=t.n.iter, thin=t.n.thin) res &lt;- round(data.frame(summary(mod)$quantiles[, c(3, 1, 5)]), 3) res$TRUEVALUE &lt;- c(heffCS, heffM, meffCS, meffh1, meffh2, meffm1, meffm2) res # Traceplots post &lt;- data.frame(rbind(mod[[1]], mod[[2]])) names(post) &lt;- dimnames(mod[[1]])[[2]] par(mfrow = c(3,3)) param &lt;- c(&quot;meffh[1]&quot;, &quot;meffh[2]&quot;, &quot;meffm[1]&quot;, &quot;meffm[2]&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) traceplot(mod[, match(param, names(post))]) "],["spatial-glmm.html", "21 Modeling spatial data using GLMM 21.1 Introduction 21.2 Summary", " 21 Modeling spatial data using GLMM THIS CHAPTER IS UNDER CONSTRUCTION!!! 21.1 Introduction 21.2 Summary xxx "],["PART-III.html", "22 Introduction to PART III 22.1 Model notations", " 22 Introduction to PART III This part is a collection of more complicated ecological models to analyse data that may not be analysed with the traditional linear models that we covered in PART I of this book. 22.1 Model notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Thomson et al. (2009) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. "],["zeroinflated-poisson-lmm.html", "23 Zero-inflated Poisson Mixed Model 23.1 Introduction 23.2 Example data 23.3 Model", " 23 Zero-inflated Poisson Mixed Model 23.1 Introduction Usually we describe the outcome variable with a single distribution, such as the normal distribution in the case of linear (mixed) models, and Poisson or binomial distributions in the case of generalized linear (mixed) models. In life sciences, however, quite often the data are actually generated by more than one process. In such cases the distribution of the data could be the result of two or more different distributions. If we do not account for these different processes our inferences are likely to be biased. In this chapter, we introduce a mixture model that explicitly include two processes that generated the data. The zero-inflated Poisson model is a mixture of a binomial and a Poisson distribution. We belief that two (or more)-level models are very useful tools in life sciences because they can help uncover the different processes that generate the data we observe. 23.2 Example data We used the blackstork data from the blmeco-package. They contain the breeding success of Black-stork in Latvia. The data was collected and kindly provided by Maris Stradz. The data contains the number of nestlings of more then 300 Black-stork nests in different years. Counting animals or plants is a typical example of data that contain a lot of zero counts. For example, the number of nestlings produced by a breeding pair is often zero because the whole nest was depredated or because a catastrophic event occurred such as a flood. However, when the nest succeeds, the number of nestlings varies among the successful nests depending on how many eggs the female has laid, how much food the parents could bring to the nest, or other factors that affect the survival of a nestling in an intact nest. Thus the factors that determine how many zero counts there are in the data differ from the factors that determine how many nestlings there are, if a nest survives. Count data that are produced by two different processesone produces the zero counts and the other the variance in the count for the ones that were not zero in the first processare called zero-inflated data. Histograms of zero-inflated data look bimodal, with one peak at zero (Figure 23.1). Figure 23.1: Histogram of the number of nestlings counted in black stork nests Ciconia nigra in Latvia (n = 1130 observations of 279 nests). 23.3 Model The Poisson distribution does not fit well to such data, because the data contain more zero counts than expected under the Poisson distribution. Mullahy (1986) and Lambert (1992) formulated two different types of models that combine the two processes in one model and therefore account for the zero excess in the data and allow the analysis of the two processes separately. The hurdle model (Mullahy, 1986) combines a left-truncated count data model (Poisson or negative binomial distribution that only describes the distribution of data larger than zero) with a zero-hurdle model that describes the distribution of the data that are either zero or nonzero. In other words, the hurdle model divides the data into two data subsets, the zero counts and the nonzero counts, and fits two separate models to each subset of the data. To account for this division of the data, the two models assume left truncation (all measurements below 1 are missing in the data) and right censoring (all measurements larger than 1 have the value 1), respectively, in their error distributions. A hurdle model can be fitted in R using the function hurdle from the package pscl (Jackman, 2008). See the tutorial by Zeileis et al. (2008) for an introduction. In contrast to the hurdle model, the zero-inflated models (Mullahy, 1986; Lambert, 1992) combine a Bernoulli model (zero vs. nonzero) with a conditional Poisson model; conditional on the Bernoulli process being nonzero. Thus this model allows for a mixture of zero counts: some zero counts are zero because the outcome of the Bernoulli process was zero (these zero counts are sometimes called structural zero values), and others are zero because their outcome from the Poisson process was zero. The function `zeroinfl from the package pscl fits zero-inflated models (Zeileis et al., 2008). The zero-inflated model may seem to reflect the true process that has generated the data closer than the hurdle model. However, sometimes the fit of zero-inflated models is impeded because of high correlation of the model parameters between the zero model and the count model. In such cases, a hurdle model may cause less troubles. Both functions (hurdle and zeroinfl) from the package pscl do not allow the inclusion of random factors. The functions MCMCglmm from the package MCMCglmm (Hadfield, 2010) and glmmadmb from the package glmmADMB (http://glmmadmb.r-forge.r-project.org/) provide the possibility to account for zero-inflation with a GLMM. However, these functions are not very flexible in the types of zero-inflated models they can fit; for example, glmmadmb only includes a constant proportion of zero values. A zero-inflation model using BUGS is described in Ke ry and Schaub (2012). Here we use Stan to fit a zero- inflated model. Once we understand the basic model code, it is easy to add predictors and/or random effects to both the zero and the count model. The example data contain numbers of nestlings in black stork Ciconia nigra nests in Latvia collected by Maris Stradz and collaborators at 279 nests be- tween 1979 and 2010. Black storks build solid and large aeries on branches of large trees. The same aerie is used for up to 17 years until it collapses. The black stork population in Latvia has drastically declined over the last decades. Here, we use the nestling data as presented in Figure 14-2 to describe whether the number of black stork nestlings produced in Latvia decreased over time. We use a zero-inflated Poisson model to separately estimate temporal trends for nest survival and the number of nestlings in successful nests. Since the same nests have been measured repeatedly over 1 to 17 years, we add nest ID as a random factor to both models, the Bernoulli and the Poisson model. After the first model fit, we saw that the between-nest variance in the number of nest- lings for the successful nests was close to zero. Therefore, we decide to delete the random effect from the Poisson model. Here is our final model: zit is a latent (unobserved) variable that takes the values 0 or 1 for each nest i during year t. It indicates a structural zero, that is, if zit 14 1 the number of nestlings yit always is zero, because the expected value in the Poisson model lit(1 zit) becomes zero. If zit 14 0, the expected value in the Poisson model becomes lit. To fit this model in Stan, we first write the Stan model code and save it in a separated text-file with name zeroinfl.stan. Here is a handy package: https://cran.r-project.org/web/packages/GLMMadaptive/vignettes/ZeroInflated_and_TwoPart_Models.html "],["dailynestsurv.html", "24 Daily nest survival 24.1 Background 24.2 Models for estimating daily nest survival 24.3 Known fate model 24.4 The Stan model 24.5 Prepare data and run Stan 24.6 Check convergence 24.7 Look at results 24.8 Known fate model for irregular nest controls Further reading", " 24 Daily nest survival 24.1 Background Analyses of nest survival is important for understanding the mechanisms of population dynamics. The life-span of a nest could be used as a measure of nest survival. However, this measure very often is biased towards nests that survived longer because these nests are detected by the ornithologists with higher probability (Mayfield 1975). In order not to overestimate nest survival, daily nest survival conditional on survival to the previous day can be estimated. 24.2 Models for estimating daily nest survival What model is best used depends on the type of data available. Data may look: Regular (e.g. daily) nest controls, all nests monitored from their first egg onward Regular nest controls, nests found during the course of the study at different stages and nestling ages Irregular nest controls, all nests monitored from their first egg onward Irregular nest controls, nests found during the course of the study at different stages and nestling ages Table 24.1: Models useful for estimating daily nest survival. Data numbers correspond to the descriptions above. Model Data Software, R-code Binomial or Bernoulli model 1, (3) glm, glmer, Cox proportional hazard model 1,2,3,4 brm, soon: stan_cox Known fate model 1, 2 Stan code below Known fate model 3, 4 Stan code below 24.3 Known fate model A natural model that allows estimating daily nest survival is the known-fate survival model. It is a Markov model that models the state of a nest \\(i\\) at day \\(t\\) (whether it is alive, \\(y_{it}=1\\) or not \\(y_{it}=0\\)) as a Bernoulli variable dependent on the state of the nest the day before. \\[ y_{it} \\sim Bernoulli(y_{it-1}S_{it})\\] The daily nest survival \\(S_{it}\\) can be linearly related to predictor variables that are measured on the nest or on the day level. \\[logit(S_{it}) = \\textbf{X} \\beta\\] It is also possible to add random effects if needed. 24.4 The Stan model The following Stan model code is saved as daily_nest_survival.stan. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; last[Nnests]; // day of last observation (alive or dead) int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last int&lt;lower=0&gt; y[Nnests, maxage]; // indicator of alive nests real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date } parameters { vector[3] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(last[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t]); } } // priors b[1]~normal(0,5); b[2]~normal(0,3); b[3]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):last[i]){ y[i,t]~bernoulli(y[i,t-1]*S[i,t-1]); } } } 24.5 Prepare data and run Stan Data is from (Grendelmeier2018?). load(&quot;RData/nest_surv_data.rda&quot;) str(datax) ## List of 7 ## $ y : int [1:156, 1:31] 1 NA 1 NA 1 NA NA 1 1 1 ... ## $ Nnests: int 156 ## $ last : int [1:156] 26 30 31 27 31 30 31 31 31 31 ... ## $ first : int [1:156] 1 14 1 3 1 24 18 1 1 1 ... ## $ cover : num [1:156] -0.943 -0.215 0.149 0.149 -0.215 ... ## $ age : num [1:31] -1.65 -1.54 -1.43 -1.32 -1.21 ... ## $ maxage: int 31 datax$y[is.na(datax$y)] &lt;- 0 # Stan does not allow for NA&#39;s in the outcome # Run STAN library(rstan) mod &lt;- stan(file = &quot;stanmodels/daily_nest_survival.stan&quot;, data=datax, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) 24.6 Check convergence We love exploring the performance of the Markov chains by using the function launch_shinystan from the package shinystan. 24.7 Look at results It looks like cover does not affect daily nest survival, but daily nest survival decreases with the age of the nestlings. #launch_shinystan(mod) print(mod) ## Inference for Stan model: daily_nest_survival. ## 5 chains, each with iter=2500; warmup=1250; thin=1; ## post-warmup draws per chain=1250, total post-warmup draws=6250. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b[1] 4.03 0.00 0.15 3.75 3.93 4.03 4.13 4.35 4503 1 ## b[2] 0.00 0.00 0.12 -0.24 -0.09 -0.01 0.08 0.25 5011 1 ## b[3] -0.69 0.00 0.16 -1.01 -0.80 -0.69 -0.59 -0.39 4354 1 ## lp__ -298.96 0.03 1.28 -302.36 -299.48 -298.64 -298.04 -297.53 2386 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Dec 21 20:25:27 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # effect plot bsim &lt;- as.data.frame(mod) nsim &lt;- nrow(bsim) newdat &lt;- data.frame(age=seq(1, datax$maxage, length=100)) newdat$age.z &lt;- (newdat$age-mean(1:datax$maxage))/sd((1:datax$maxage)) Xmat &lt;- model.matrix(~age.z, data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- plogis(Xmat%*%as.numeric(bsim[i,c(1,3)])) newdat$fit &lt;- apply(fitmat, 1, median) newdat$lwr &lt;- apply(fitmat, 1, quantile, prob=0.025) newdat$upr &lt;- apply(fitmat, 1, quantile, prob=0.975) plot(newdat$age, newdat$fit, ylim=c(0.8,1), type=&quot;l&quot;, las=1, ylab=&quot;Daily nest survival&quot;, xlab=&quot;Age [d]&quot;) lines(newdat$age, newdat$lwr, lty=3) lines(newdat$age, newdat$upr, lty=3) Figure 24.1: Estimated daily nest survival probability in relation to nest age. Dotted lines are 95% uncertainty intervals of the regression line. 24.8 Known fate model for irregular nest controls When nest are controlled only irregularly, it may happen that a nest is found predated or dead after a longer break in controlling. In such cases, we know that the nest was predated or it died due to other causes some when between the last control when the nest was still alive and when it was found dead. In such cases, we need to tell the model that the nest could have died any time during the interval when we were not controlling. To do so, we create a variable that indicates the time (e.g. day since first egg) when the nest was last seen alive (lastlive). A second variable indicates the time of the last check which is either the equal to lastlive when the nest survived until the last check, or it is larger than lastlive when the nest failure has been recorded. A last variable, gap, measures the time interval in which the nest failure occurred. A gap of zero means that the nest was still alive at the last control, a gapof 1 means that the nest failure occurred during the first day after lastlive, a gap of 2 means that the nest failure either occurred at the first or second day after lastlive. # time when nest was last observed alive lastlive &lt;- apply(datax$y, 1, function(x) max(c(1:length(x))[x==1])) # time when nest was last checked (alive or dead) lastcheck &lt;- lastlive+1 # here, we turn the above data into a format that can be used for # irregular nest controls. WOULD BE NICE TO HAVE A REAL DATA EXAMPLE! # when nest was observed alive at the last check, then lastcheck equals lastlive lastcheck[lastlive==datax$last] &lt;- datax$last[lastlive==datax$last] datax1 &lt;- list(Nnests=datax$Nnests, lastlive = lastlive, lastcheck= lastcheck, first=datax$first, cover=datax$cover, age=datax$age, maxage=datax$maxage) # time between last seen alive and first seen dead (= lastcheck) datax1$gap &lt;- datax1$lastcheck-datax1$lastlive In the Stan model code, we specify the likelihood for each gap separately. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; lastlive[Nnests]; // day of last observation (alive) int&lt;lower=0&gt; lastcheck[Nnests]; // day of observed death or, if alive, last day of study int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date int&lt;lower=0&gt; gap[Nnests]; // obsdead - lastlive } parameters { vector[3] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(lastcheck[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t]); } } // priors b[1]~normal(0,1.5); b[2]~normal(0,3); b[3]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):lastlive[i]){ 1~bernoulli(S[i,t-1]); } if(gap[i]==1){ target += log(1-S[i,lastlive[i]]); // } if(gap[i]==2){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1])); // } if(gap[i]==3){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1]) + prod(S[i,lastlive[i]:(lastlive[i]+1)])*(1-S[i,lastlive[i]+2])); // } if(gap[i]==4){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1]) + prod(S[i,lastlive[i]:(lastlive[i]+1)])*(1-S[i,lastlive[i]+2]) + prod(S[i,lastlive[i]:(lastlive[i]+2)])*(1-S[i,lastlive[i]+3])); // } } } # Run STAN mod1 &lt;- stan(file = &quot;stanmodels/daily_nest_survival_irreg.stan&quot;, data=datax1, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) Further reading Helpful links: https://deepai.org/publication/bayesian-survival-analysis-using-the-rstanarm-r-package (Brilleman et al. 2020) https://www.hammerlab.org/2017/06/26/introducing-survivalstan/ "],["cjs-with-mix.html", "25 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 25.1 Introduction 25.2 Data description 25.3 Model description 25.4 The Stan code 25.5 Call Stan from R, check convergence and look at results", " 25 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 25.1 Introduction In some species the identification of the sex is not possible for all individuals without sampling DNA. For example, morphological dimorphism is absent or so weak that parts of the individuals cannot be assigned to one of the sexes. Particularly in ornithological long-term capture recapture data sets that typically are obtained by voluntary bird ringers who do normaly not have the possibilities to analyse DNA, often the sex identification is missing in parts of the individuals. For estimating survival, it would nevertheless be valuable to include data of all individuals, use the information on sex-specific effects on survival wherever possible but account for the fact that of parts of the individuals the sex is not known. We here explain how a Cormack-Jolly-Seber model can be integrated with a mixture model in oder to allow for a combined analyses of individuals with and without sex identified. An introduction to the Cormack-Jolly-Seber model we gave in Chapter 14.5 of the book Korner-Nievergelt et al. (2015). We here expand this model by a mixture structure that allows including individuals with a missing categorical predictor variable, such as sex. 25.2 Data description ## simulate data # true parameter values theta &lt;- 0.6 # proportion of males nocc &lt;- 15 # number of years in the data set b0 &lt;- matrix(NA, ncol=nocc-1, nrow=2) b0[1,] &lt;- rbeta((nocc-1), 3, 4) # capture probability of males b0[2,] &lt;- rbeta((nocc-1), 2, 4) # capture probability of females a0 &lt;- matrix(NA, ncol=2, nrow=2) a1 &lt;- matrix(NA, ncol=2, nrow=2) a0[1,1]&lt;- qlogis(0.7) # average annual survival for adult males a0[1,2]&lt;- qlogis(0.3) # average annual survival for juveniles a0[2,1] &lt;- qlogis(0.55) # average annual survival for adult females a0[2,2] &lt;- a0[1,2] a1[1,1] &lt;- 0 a1[1,2] &lt;- -0.5 a1[2,1] &lt;- -0.8 a1[2,2] &lt;- a1[1,2] nindi &lt;- 1000 # number of individuals with identified sex nindni &lt;- 1500 # number of individuals with non-identified sex nind &lt;- nindi + nindni # total number of individuals y &lt;- matrix(ncol=nocc, nrow=nind) z &lt;- matrix(ncol=nocc, nrow=nind) first &lt;- sample(1:(nocc-1), nind, replace=TRUE) sex &lt;- sample(c(1,2), nind, prob=c(theta, 1-theta), replace=TRUE) juvfirst &lt;- sample(c(0,1), nind, prob=c(0.5, 0.5), replace=TRUE) juv &lt;- matrix(0, nrow=nind, ncol=nocc) for(i in 1:nind) juv[i,first[i]] &lt;- juv[i] x &lt;- runif(nocc-1, -2, 2) # a time dependent covariate covariate p &lt;- b0 # recapture probability phi &lt;- array(NA, dim=c(2, 2, nocc-1)) # for ad males phi[1,1,] &lt;- plogis(a0[1,1]+a1[1,1]*x) # for ad females phi[2,1,] &lt;- plogis(a0[2,1]+a1[2,1]*x) # for juvs phi[1,2,] &lt;- phi[2,2,] &lt;- plogis(a0[2,2]+a1[2,2]*x) for(i in 1:nind){ z[i,first[i]] &lt;- 1 y[i, first[i]] &lt;- 1 for(t in (first[i]+1):nocc){ z[i, t] &lt;- rbinom(1, size=1, prob=z[i,t-1]*phi[sex[i],juv[i,t-1]+1, t-1]) y[i, t] &lt;- rbinom(1, size=1, prob=z[i,t]*p[sex[i],t-1]) } } y[is.na(y)] &lt;- 0 The mark-recapture data set consists of capture histories of 2500 individuals over 15 time periods. For each time period \\(t\\) and individual \\(i\\) the capture history matrix \\(y\\) contains \\(y_{it}=1\\) if the individual \\(i\\) is captured during time period \\(t\\), or \\(y_{it}=0\\) if the individual \\(i\\) is not captured during time period \\(t\\). The marking time period varies between individuals from 1 to 14. At the marking time period, the age of the individuals was classified either as juvenile or as adult. Juveniles turn into adults after one time period, thus age is known for all individuals during all time periods after marking. For 1000 individuals of the 2500 individuals, the sex is identified, whereas for 1500 individuals, the sex is unknown. The example data contain one covariate \\(x\\) that takes on one value for each time period. # bundle the data for Stan i &lt;- 1:nindi ni &lt;- (nindi+1):nind datax &lt;- list(yi=y[i,], nindi=nindi, sex=sex[i], nocc=nocc, yni=y[ni,], nindni=nindni, firsti=first[i], firstni=first[ni], juvi=juv[i,]+1, juvni=juv[ni,]+1, year=1:nocc, x=x) 25.3 Model description The observations \\(y_{it}\\), an indicator of whether individual i was recaptured during time period \\(t\\) is modelled conditional on the latent true state of the individual birds \\(z_{it}\\) (0 = dead or permanently emigrated, 1 = alive and at the study site) as a Bernoulli variable. The probability \\(P(y_{it} = 1)\\) is the product of the probability that an alive individual is recaptured, \\(p_{it}\\), and the state of the bird \\(z_{it}\\) (alive = 1, dead = 0). Thus, a dead bird cannot be recaptured, whereas for a bird alive during time period \\(t\\), the recapture probability equals \\(p_{it}\\): \\[y_{it} \\sim Bernoulli(z_{it}p_{it})\\] The latent state variable \\(z_{it}\\) is a Markovian variable with the state at time \\(t\\) being dependent on the state at time \\(t-1\\) and the apparent survival probability \\[\\phi_{it}\\]: \\[z_{it} \\sim Bernoulli(z_{it-1}\\phi_{it})\\] We use the term apparent survival in order to indicate that the parameter \\(\\phi\\) is a product of site fidelity and survival. Thus, individuals that permanently emigrated from the study area cannot be distinguished from dead individuals. In both models, the parameters \\(\\phi\\) and \\(p\\) were modelled as sex-specific. However, for parts of the individuals, sex could not be identified, i.e. sex was missing. Ignoring these missing values would most likely lead to a bias because they were not missing at random. The probability that sex can be identified is increasing with age and most likely differs between sexes. Therefore, we included a mixture model for the sex: \\[Sex_i \\sim Categorical(q_i)\\] where \\(q_i\\) is a vector of length 2, containing the probability of being a male and a female, respectively. In this way, the sex of the non-identified individuals was assumed to be male or female with probability \\(q[1]\\) and \\(q[2]=1-q[1]\\), respectively. This model corresponds to the finite mixture model introduced by Pledger, Pollock, and Norris (2003) in order to account for unknown classes of birds (heterogeneity). However, in our case, for parts of the individuals the class (sex) was known. In the example model, we constrain apparent survival to be linearly dependent on a covariate x with different slopes for males, females and juveniles using the logit link function. \\[logit(\\phi_{it}) = a0_{sex-age-class[it]} + a1_{sex-age-class[it]}x_i\\] Annual recapture probability was modelled for each year and age and sex class independently: \\[p_{it} = b0_{t,sex-age-class[it]}\\] Uniform prior distributions were used for all parameters with a parameter space limited to values between 0 and 1 (probabilities) and a normal distribution with a mean of 0 and a standard deviation of 1.5 for the intercept \\(a0\\), and a standard deviation of 5 was used for \\(a1\\). 25.4 The Stan code The trick for coding the CMR-mixture model in Stan is to formulate the model 3 times: 1. For the individuals with identified sex 2. For the males that were not identified 3. For the females that were not identified Then for the non-identified individuals a mixture model is formulated that assigns a probability of being a female or a male to each individual. data { int&lt;lower=2&gt; nocc; // number of capture events int&lt;lower=0&gt; nindi; // number of individuals with identified sex int&lt;lower=0&gt; nindni; // number of individuals with non-identified sex int&lt;lower=0,upper=2&gt; yi[nindi,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firsti[nindi]; // year of first capture int&lt;lower=0,upper=2&gt; yni[nindni,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firstni[nindni]; // year of first capture int&lt;lower=1, upper=2&gt; sex[nindi]; int&lt;lower=1, upper=2&gt; juvi[nindi, nocc]; int&lt;lower=1, upper=2&gt; juvni[nindni, nocc]; int&lt;lower=1&gt; year[nocc]; real x[nocc-1]; // a covariate } transformed data { int&lt;lower=0,upper=nocc+1&gt; lasti[nindi]; // last[i]: ind i last capture int&lt;lower=0,upper=nocc+1&gt; lastni[nindni]; // last[i]: ind i last capture lasti = rep_array(0,nindi); lastni = rep_array(0,nindni); for (i in 1:nindi) { for (k in firsti[i]:nocc) { if (yi[i,k] == 1) { if (k &gt; lasti[i]) lasti[i] = k; } } } for (ii in 1:nindni) { for (kk in firstni[ii]:nocc) { if (yni[ii,kk] == 1) { if (kk &gt; lastni[ii]) lastni[ii] = kk; } } } } parameters { real&lt;lower=0, upper=1&gt; theta[nindni]; // probability of being male for non-identified individuals real&lt;lower=0, upper=1&gt; b0[2,nocc-1]; // intercept of p real a0[2,2]; // intercept for phi real a1[2,2]; // coefficient for phi } transformed parameters { real&lt;lower=0,upper=1&gt;p_male[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p_female[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p[nindi,nocc]; // capture probability real&lt;lower=0,upper=1&gt;phi_male[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_male[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi_female[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_female[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi[nindi,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi[nindi,nocc+1]; // probability that an individual // is never recaptured after its // last capture { int k; int kk; for(ii in 1:nindi){ if (firsti[ii]&gt;1) { for (z in 1:(firsti[ii]-1)){ phi[ii,z] = 1; } } for(tt in firsti[ii]:(nocc-1)) { // linear predictor for phi: phi[ii,tt] = inv_logit(a0[sex[ii], juvi[ii,tt]] + a1[sex[ii], juvi[ii,tt]]*x[tt]); } } for(ii in 1:nindni){ if (firstni[ii]&gt;1) { for (z in 1:(firstni[ii]-1)){ phi_female[ii,z] = 1; phi_male[ii,z] = 1; } } for(tt in firstni[ii]:(nocc-1)) { // linear predictor for phi: phi_male[ii,tt] = inv_logit(a0[1, juvni[ii,tt]] + a1[1, juvni[ii,tt]]*x[tt]); phi_female[ii,tt] = inv_logit(a0[2, juvni[ii,tt]]+ a1[2, juvni[ii,tt]]*x[tt]); } } for(i in 1:nindi) { // linear predictor for p for identified individuals for(w in 1:firsti[i]){ p[i,w] = 1; } for(kkk in (firsti[i]+1):nocc) p[i,kkk] = b0[sex[i],year[kkk-1]]; chi[i,nocc+1] = 1.0; k = nocc; while (k &gt; firsti[i]) { chi[i,k] = (1 - phi[i,k-1]) + phi[i,k-1] * (1 - p[i,k]) * chi[i,k+1]; k = k - 1; } if (firsti[i]&gt;1) { for (u in 1:(firsti[i]-1)){ chi[i,u] = 0; } } chi[i,firsti[i]] = (1 - p[i,firsti[i]]) * chi[i,firsti[i]+1]; }// close definition of transformed parameters for identified individuals for(i in 1:nindni) { // linear predictor for p for non-identified individuals for(w in 1:firstni[i]){ p_male[i,w] = 1; p_female[i,w] = 1; } for(kkkk in (firstni[i]+1):nocc){ p_male[i,kkkk] = b0[1,year[kkkk-1]]; p_female[i,kkkk] = b0[2,year[kkkk-1]]; } chi_male[i,nocc+1] = 1.0; chi_female[i,nocc+1] = 1.0; k = nocc; while (k &gt; firstni[i]) { chi_male[i,k] = (1 - phi_male[i,k-1]) + phi_male[i,k-1] * (1 - p_male[i,k]) * chi_male[i,k+1]; chi_female[i,k] = (1 - phi_female[i,k-1]) + phi_female[i,k-1] * (1 - p_female[i,k]) * chi_female[i,k+1]; k = k - 1; } if (firstni[i]&gt;1) { for (u in 1:(firstni[i]-1)){ chi_male[i,u] = 0; chi_female[i,u] = 0; } } chi_male[i,firstni[i]] = (1 - p_male[i,firstni[i]]) * chi_male[i,firstni[i]+1]; chi_female[i,firstni[i]] = (1 - p_female[i,firstni[i]]) * chi_female[i,firstni[i]+1]; } // close definition of transformed parameters for non-identified individuals } // close block of transformed parameters exclusive parameter declarations } // close transformed parameters model { // priors theta ~ beta(1, 1); for (g in 1:(nocc-1)){ b0[1,g]~beta(1,1); b0[2,g]~beta(1,1); } a0[1,1]~normal(0,1.5); a0[1,2]~normal(0,1.5); a1[1,1]~normal(0,3); a1[1,2]~normal(0,3); a0[2,1]~normal(0,1.5); a0[2,2]~normal(a0[1,2],0.01); // for juveniles, we assume that the effect of the covariate is independet of sex a1[2,1]~normal(0,3); a1[2,2]~normal(a1[1,2],0.01); // likelihood for identified individuals for (i in 1:nindi) { if (lasti[i]&gt;0) { for (k in firsti[i]:lasti[i]) { if(k&gt;1) target+= (log(phi[i, k-1])); if (yi[i,k] == 1) target+=(log(p[i,k])); else target+=(log1m(p[i,k])); } } target+=(log(chi[i,lasti[i]+1])); } // likelihood for non-identified individuals for (i in 1:nindni) { real log_like_male = 0; real log_like_female = 0; if (lastni[i]&gt;0) { for (k in firstni[i]:lastni[i]) { if(k&gt;1){ log_like_male += (log(phi_male[i, k-1])); log_like_female += (log(phi_female[i, k-1])); } if (yni[i,k] == 1){ log_like_male+=(log(p_male[i,k])); log_like_female+=(log(p_female[i,k])); } else{ log_like_male+=(log1m(p_male[i,k])); log_like_female+=(log1m(p_female[i,k])); } } } log_like_male += (log(chi_male[i,lastni[i]+1])); log_like_female += (log(chi_female[i,lastni[i]+1])); target += log_mix(theta[i], log_like_male, log_like_female); } } 25.5 Call Stan from R, check convergence and look at results # Run STAN library(rstan) fit &lt;- stan(file = &quot;stanmodels/cmr_mixture_model.stan&quot;, data=datax, verbose = FALSE) # for above simulated data (25000 individuals x 15 time periods) # computing time is around 48 hours on an intel corei7 laptop # for larger data sets, we recommed moving the transformed parameters block # to the model block in order to avoid monitoring of p_male, p_female, # phi_male and phi_female producing memory problems # launch_shinystan(fit) # diagnostic plots summary(fit) ## mean se_mean sd 2.5% 25% ## b0[1,1] 0.60132367 0.0015709423 0.06173884 0.48042366 0.55922253 ## b0[1,2] 0.70098709 0.0012519948 0.04969428 0.60382019 0.66806698 ## b0[1,3] 0.50293513 0.0010904085 0.04517398 0.41491848 0.47220346 ## b0[1,4] 0.28118209 0.0008809447 0.03577334 0.21440931 0.25697691 ## b0[1,5] 0.34938289 0.0009901335 0.03647815 0.27819918 0.32351323 ## b0[1,6] 0.13158569 0.0006914740 0.02627423 0.08664129 0.11286629 ## b0[1,7] 0.61182981 0.0010463611 0.04129602 0.53187976 0.58387839 ## b0[1,8] 0.48535193 0.0010845951 0.04155762 0.40559440 0.45750793 ## b0[1,9] 0.52531291 0.0008790063 0.03704084 0.45247132 0.50064513 ## b0[1,10] 0.87174780 0.0007565552 0.03000936 0.80818138 0.85259573 ## b0[1,11] 0.80185454 0.0009425675 0.03518166 0.73173810 0.77865187 ## b0[1,12] 0.33152443 0.0008564381 0.03628505 0.26380840 0.30697293 ## b0[1,13] 0.42132288 0.0012174784 0.04140382 0.34062688 0.39305210 ## b0[1,14] 0.65180372 0.0015151039 0.05333953 0.55349105 0.61560493 ## b0[2,1] 0.34237039 0.0041467200 0.12925217 0.12002285 0.24717176 ## b0[2,2] 0.18534646 0.0023431250 0.07547704 0.05924694 0.12871584 ## b0[2,3] 0.61351083 0.0024140550 0.07679100 0.46647727 0.56242546 ## b0[2,4] 0.37140208 0.0024464965 0.06962399 0.24693888 0.32338093 ## b0[2,5] 0.19428215 0.0034618302 0.11214798 0.02800056 0.11146326 ## b0[2,6] 0.27371336 0.0026553769 0.09054020 0.11827243 0.20785316 ## b0[2,7] 0.18611173 0.0014387436 0.05328492 0.09122869 0.14789827 ## b0[2,8] 0.25648337 0.0018258589 0.05287800 0.16255769 0.21913271 ## b0[2,9] 0.20378754 0.0021367769 0.07380004 0.07777998 0.15215845 ## b0[2,10] 0.52679548 0.0024625568 0.08696008 0.36214334 0.46594844 ## b0[2,11] 0.47393354 0.0032593161 0.10555065 0.28843967 0.39781278 ## b0[2,12] 0.22289155 0.0017082729 0.05551514 0.12576797 0.18203335 ## b0[2,13] 0.26191486 0.0024159794 0.07016314 0.14106495 0.21234017 ## b0[2,14] 0.65111737 0.0055743944 0.18780555 0.29279480 0.50957591 ## a0[1,1] 0.95440670 0.0013771881 0.04808748 0.86301660 0.92146330 ## a0[1,2] 0.01529770 0.0469699511 1.46995922 -2.82218067 -0.95533706 ## a0[2,1] 0.16384995 0.0049928331 0.12634422 -0.06399631 0.07533962 ## a0[2,2] 0.01535679 0.0469634175 1.47006964 -2.81864060 -0.95515751 ## a1[1,1] 0.15937249 0.0028992587 0.08864790 -0.01288607 0.10017613 ## a1[1,2] 0.08055953 0.1007089857 3.02148727 -5.95525636 -1.96662599 ## a1[2,1] -0.83614134 0.0074143920 0.18655882 -1.21033848 -0.95698565 ## a1[2,2] 0.08071668 0.1006904255 3.02145647 -5.94617355 -1.96508733 ## 50% 75% 97.5% n_eff Rhat ## b0[1,1] 0.60206306 0.6431566 0.7206343 1544.5301 1.002331 ## b0[1,2] 0.70165494 0.7355204 0.7946280 1575.4617 1.001482 ## b0[1,3] 0.50367411 0.5330078 0.5898079 1716.3196 1.001183 ## b0[1,4] 0.27997512 0.3046483 0.3544592 1649.0040 1.000760 ## b0[1,5] 0.34936442 0.3751935 0.4191138 1357.3073 1.002072 ## b0[1,6] 0.12987449 0.1481661 0.1873982 1443.8040 1.003676 ## b0[1,7] 0.61203228 0.6397577 0.6933929 1557.5904 1.001458 ## b0[1,8] 0.48513822 0.5134314 0.5672066 1468.1355 1.002511 ## b0[1,9] 0.52534212 0.5501747 0.5994060 1775.7335 1.000824 ## b0[1,10] 0.87324112 0.8934047 0.9258033 1573.3747 1.000719 ## b0[1,11] 0.80300311 0.8261868 0.8675033 1393.1817 1.001172 ## b0[1,12] 0.33044476 0.3552199 0.4052902 1794.9956 1.000566 ## b0[1,13] 0.42116690 0.4492297 0.5026942 1156.5339 1.000289 ## b0[1,14] 0.64956850 0.6864706 0.7607107 1239.4056 1.004061 ## b0[2,1] 0.33493631 0.4251416 0.6150923 971.5524 1.004049 ## b0[2,2] 0.17981663 0.2358847 0.3446097 1037.6210 1.001474 ## b0[2,3] 0.61326419 0.6644156 0.7628427 1011.8737 1.005727 ## b0[2,4] 0.36837778 0.4158585 0.5190457 809.8949 1.003803 ## b0[2,5] 0.17910449 0.2591418 0.4533117 1049.4733 1.001499 ## b0[2,6] 0.26739172 0.3299594 0.4685139 1162.6006 1.001170 ## b0[2,7] 0.18254607 0.2198969 0.3003156 1371.6455 1.000878 ## b0[2,8] 0.25280556 0.2895585 0.3704113 838.7174 1.005624 ## b0[2,9] 0.19724053 0.2501298 0.3694806 1192.8747 1.003687 ## b0[2,10] 0.52587075 0.5845730 0.7061694 1247.0027 1.002851 ## b0[2,11] 0.46874445 0.5392302 0.7046892 1048.7425 0.999473 ## b0[2,12] 0.21961656 0.2580782 0.3397127 1056.1081 1.000907 ## b0[2,13] 0.25601959 0.3056204 0.4142888 843.3960 1.003130 ## b0[2,14] 0.65824835 0.7973674 0.9698829 1135.0669 1.003838 ## a0[1,1] 0.95368445 0.9862439 1.0515747 1219.2071 1.003898 ## a0[1,2] 0.01633534 0.9911055 2.9717839 979.4231 1.003726 ## a0[2,1] 0.15519648 0.2472483 0.4230776 640.3489 1.004625 ## a0[2,2] 0.01587281 0.9898084 2.9659552 979.8429 1.003744 ## a1[1,1] 0.15647489 0.2205720 0.3354845 934.8953 1.007190 ## a1[1,2] 0.06683287 2.1568781 6.0295208 900.1297 1.003701 ## a1[2,1] -0.83503982 -0.7075691 -0.4814539 633.1119 1.010568 ## a1[2,2] 0.06586905 2.1557247 6.0239735 900.4432 1.003704 "],["referenzen.html", "Referenzen", " Referenzen Aitkin, Murray, Brian Francis, John Hinde, and Ross Darnell. 2009. Statistical Modelling in r. Oxford: Oxford University Press. Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019. Retire Statistical Significance. Nature 567: 3057. Betancourt, M.~J. 2013. Generalizing the No-U-Turn Sampler to Riemannian Manifolds. ArXiv e-Prints, April. https://arxiv.org/abs/1304.1920. Betancourt, M.~J., and M. Girolami. 2013. Hamiltonian Monte Carlo for Hierarchical Models. ArXiv e-Prints. https://arxiv.org/abs/1312.0906. Brilleman, Samuel L., Eren M. Elci, Jacqueline Buros Novik, and Rory Wolfe. 2020. Bayesian Survival Analysis Using the Rstanarm r Package. http://arxiv.org/pdf/2002.09633v1. Efron, Bradley, and Trevor Hastie. 2016. Computer age statistical inference: Algorithms, evidence, and data science. Institute of Mathematical Statistics Monographs. Gelman, A., John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014a. Bayesian Data Analysis. Third. New York: CRC Press. Gelman, A, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014b. Bayesian Data Analysis. Third. New York: CRC Press. Gelman, A, and J Hill. 2007. Data Analysis Using Regression and Multilevel / Hierarchical Models. Cambridge: Cambridge Universtiy Press. Gelman, Andrew, and Sander Greenland. 2019. Are Confidence Intervals Better Termed Uncertainty Intervals? BMJ (Clinical Research Ed.) 366: l5381. https://doi.org/10.1136/bmj.l5381. Grüebler, Martin U, Fränzi Korner-Nievergelt, and Johann Von Hirschheydt. 2010. The Reproductive Benefits of Livestock Farming in Barn Swallows Hirundo Rustica: Quality of Nest Site or Foraging Habitat? Journal of Applied Ecology 47 (6): 134047. Harju, S. 2016. Book review:~Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan. The Journal of Wildlife Management 80: 771. Hastie, T, R Tibshirani, and J Friedman. 2009. The Elements of Statistical Learning, Data Mining, Inference, and Prediction. New York: Springer. Hoffman, Matthew D, and Andrew Gelman. 2014. The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research 15 (1): 1593623. Hoyle, Rick H. 2012. Handbook of Structural Equation Modeling. New York: The Guildford Press. Korner-Nievergelt, F, T Roth, Stefanie von Felten, J Guélat, B Almasi, and P Korner-Nievergelt. 2015. Bayesian Data Analysis in Ecolog Using Linear Models with R, BUGS, and Stan. New York: Elsevier. Manly, Bryan F J. 1994. Multivariate Statistical Methods, A Primer. London: 2nd ed. Chapman &amp; Hall. Mayfield, Harold F. 1975. Suggestions for Calculating Nest Success. Wilson Bulletin 87: 45666. McElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. New York: Max Planck Institute for Evolutionary Anthropology; CRC Press. Pledger, S., K. H. Pollock, and James L. Norris. 2003. Open Capture-Recapture Models with Heterogeneity: I. Cormack-Jolly-Seber Model. Biometrics 59: 78694. Royle, J. Andrew, and Robert M. Dorazio. 2008. Hierarchical Modelling and Inference in Ecology. London: Academic Press. Schano, Christian, Carole Niffenegger, Tobias Jonas, and Fränzi Korner-Nievergelt. 2021. Hatching phenology is lagging behind an advancing snowmelt pattern in a high-alpine bird. Scientific Reports 11 (1): 20130016. https://doi.org/10.1038/s41598-021-01497-8. Shipley, Bill. 2009. Confirmatory path analysis in a generalized multilevel context. Ecology 90: 36368. Thomson, D L, M J Conroy, D R Anderson, K P Burnham, E G Cooch, C M Francis, J.-D. Lebreton, et al. 2009. Standardising Terminology and notation for the Analysis of Demographic Processes in Marked Populations. In Modeling Demographic Processes in Marked Populations, edited by D L Thomson, E G Cooch, and M J Conroy, 10991106. Environmental and Ecological Statistics 3. Berlin: Springer. Zbinden, Niklaus, Marco Salvioni, Fränzi Korner-Nievergelt, and Verena Keller. 2018. Evidence for an Additive Effect of Hunting Mortality in an Alpine Black Grouse Lyrurus Tetrix Population. Wildlife Biology 2018: xxxxx. "]]
