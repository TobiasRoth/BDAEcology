[["index.html", "Bayesian Data Analysis in Ecology with R and Stan Preface Why this book? About this book How to contribute? Acknowledgments", " Bayesian Data Analysis in Ecology with R and Stan Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Pius Korner-Nievergelt 2021-02-19 Preface Why this book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (Korner-Nievergelt et al. 2015). You can order it here. People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. About this book We do not copy text from the book into the e-book. Therefore, we refer to the book (Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. While we show the R-code behind most of the analyses, we sometimes choose not to show all the code in the html version of the book. This is particularly the case for some of the illustrations. An intrested reader can always consult the public GitHub repository with the rmarkdown-files that were used to generate the book. How to contribute? It is open so that everybody with a GitHub account can make comments and suggestions for improvement. Readers can contribute in two ways. One way is to add an issue. The second way is to contribute content directly through the edit button at the top of the page (i.e. a symbol showing a pencil in a square). That button is linked to the rmarkdown source file of each page. You can correct typos or add new text and then submit a GitHub pull request. We try to respond to you as quickly as possible. We are looking forward to your contribution! Acknowledgments We thank Yihui Xie for providing bookdown which makes it much fun to write open books such as ours. "],["PART-I.html", "1 Introduction to PART I 1.1 Further reading", " 1 Introduction to PART I During our courses we are sometimes asked to give an introduction to some R-related stuff covering data analysis, presentation of results or rather specialist topics in ecology. In this part we present collected these introduction and try to keep them updated. This is also a commented collection of R-code that we documented for our own work. We hope this might be useful olso for other readers. 1.1 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],["basics.html", "2 Prerequisits: Basic statistical terms 2.1 Variables and observations 2.2 Displaying and summarizing variables 2.3 Inferential statistics 2.4 Bayes theorem and the common aim of frequentist and Bayesian methods 2.5 Classical frequentist tests and alternatives 2.6 Summary", " 2 Prerequisits: Basic statistical terms This chapter introduces some important terms useful for doing data analyses. It also introduces the essentials of the classical frequentist tests such as t- and Chisquare test. Even though we will not use nullhypotheses tests later (Amrhein, Greenland, and McShane 2019), we introduce them here because we need to understand 100 years of scientific literature. For each classical test, we provide a suggestion how to do it in a Bayesian way and we discuss some differences between the Bayesian and frequentist statistics. 2.1 Variables and observations Empirical research involves data collection. Data are collected by recording measurements of variables for observational units. An observational unit may be, for example, an individual, a plot, a time interval or a combination of those. The collection of all units ideally build a random sample of the entire population of units in that we are interested. The measurements (or observations) of the random sample are stored in a data table (sometimes also called data set, but a data set may include several data tables. A collection of data tables belonging to the same study or system is normally bundled and stored in a data base). A data table is a collection of variables (columns). Data tables normally are handled as objects of class data.frame in R. All measurements on a row in a data table belong to the same observational unit. The variables can be of different scales (Table 2.1). Table 2.1: Scales of measurements Scale Examples Properties Coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Elevational zones Identity and magnitude (values have an ordered relationship) ordered() Numeric Discrete: counts; continuous: body weight, wing length Identity, magnitude, and intervals or ratios intgeger() numeric() 2.2 Displaying and summarizing variables While nominal and ordinal variables are summarized by giving the absolute number or the proportion of observations for each category, numeric variables normally are summarized by a location and a scatter statistics, such as the mean and the standard deviation or the median and some quantiles. The distribution of a numeric variable is graphically displayed in a histogram (Fig. (histogram)). Figure 2.1: Histogram of the length of ell of statistics course participants. To draw a histogram, the variable is displayed on the x-axis and the \\(x_i\\)-values are assigned to classes. The edges of the classes are called breaks. They can be set with the argument breaks= within the function hist. The values given in the breaks= argument must at least span the values of the variable. If the argument breaks= is not specified, R searches for breaks-values that make the histogram look smooth. The number of observations falling in each class is given on the y-axis. The y-axis can be re-scaled so that the area of the histogram equals 1 by setting the argument density=TRUE. In that case, the values on the y-axis correspond to the density values of a probability distribution (Chapter 4). Location statistics are mean, median or mode. A common mean is the - arithmetic mean: \\(\\hat{\\mu} = \\bar{x} = \\frac{i=1}{n} x_i \\sum_{1}^{n}\\)(R function mean), where \\(n\\) is the sample size. The parameter \\(\\mu\\) is the (unknown) true mean of the entire population of which the \\(1,...,n\\) measurements are a random sample of. \\(\\bar{x}\\) is called the sample mean and used as an estimate for \\(\\mu\\). The \\(^\\) above any parameter indicates that the parameter value is obtained from a sample and, therefore, it may be different from the true value. The median is the 50% quantile. We find 50% of the measurements below and 50% above the median. If \\(x_1,..., x_n\\) are the ordered measurements of a variable, then the median is: - median \\(= x_{(n+1)/2}\\) for uneven \\(n\\), and median \\(= \\frac{1}{2}(x_{n/2} + x_{n/2+1})\\) for even \\(n\\) (R function median). The mode is the value that is occurring with highest frequency or that has the highest density. Scatter also is called spread, scale or variance. Variance parameters describe how far away from the location parameter single observations can be found, or how the measurements are scattered around their mean. The variance is defined as the average squared difference between the observations and the mean: variance \\(\\hat{\\sigma^2} = s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) The term \\((n-1)\\) is called the degrees of freedom. It is used in the denominator of the variance formula instead of \\(n\\) to prevent underestimating the variance. Because \\(\\bar{x}\\) is in average closer to \\(x_i\\) than the unknown true mean \\(\\mu\\) would be, the variance would be underestimated if \\(n\\) is used in the denominator. The maximum likelihood estimate (Chapter xxx.xx) of the variance corresponds to the variance formula using \\(n\\) instead of \\(n-1\\) in the denominator, see, e.g., Royle and Dorazio (2008). The variance is used to compare the degree of scatter among different groups. However, its values are difficult to interpret because of the squared unit. Therefore, the square root of the variance, the standard deviation is normally reported: standard deviation \\(\\hat{\\sigma} = s = \\sqrt{s^2}\\) (R Function sd) The standard deviation is approximately the average deviation of an observation from the sample mean. In the case of a normal distribution, about two thirds (68%) of the data are expected within one standard deviation around the mean. The variance and standard deviation each describe the scatter with a single value. Thus, we have to assume that the observations are scattered symmetrically around their mean in order to get a picture of the distribution of the measurements. When the measurements are spread asymmetrically (skewed distribution), then it may be more precise to describe the scatter with more than one value. Such statistics could be quantiles from the lower and upper tail of the data. Quantiles inform us about both location and spread of a distribution. The \\(p\\)th-quantile is the value with the property that a proportion \\(p\\) of all values are less than or equal to the value of the quantile. The median is the 50% quantile.The 25% quantile and the 75% quantile are also called the lower and upper quartiles, respectively. The range between the 25% and the 75% quartile is called the interquartile range. This range includes 50% of the distribution and is also used as a measure of scatter. The R function quantile extracts sample quantiles. The median, the quartiles, and the interquartile range can be graphically displayed using box and-whisker plots (boxplots in short, R function boxplot). The horizontal fat bars are the medians (Fig. (fig:boxplot)). The boxes mark the interquartile range. The whiskers reach out to the last observation within 1.5 times the interquartile range from the quartile. Circles mark observations beyond 1.5 times the interquartile range from the quartile. par(mar=c(5,4,1,1)) boxplot(ell~car, data=dat, las=1, ylab=&quot;Lenght of ell [cm]&quot;, col=&quot;tomato&quot;, xaxt=&quot;n&quot;) axis(1, at=c(1,2), labels=c(&quot;Not owing a car&quot;, &quot;Car owner&quot;)) n &lt;- table(dat$car) axis(1, at=c(1,2), labels=paste(&quot;n=&quot;, n, sep=&quot;&quot;), mgp=c(3,2, 0)) Figure 2.2: Boxplot of the length of ell of statistics course participants who are or ar not owner of a car. The boxplot is an appealing tool for comparing location, variance and distribution of measurements among groups. 2.2.1 Correlations A correlation measures the strength with which characteristics of two variables are associated with each other (co-occur). If both variables are numeric, we can visualize the correlation using a scatterplot. par(mar=c(5,4,1,1)) plot(temp~ell, data=dat, las=1, xlab=&quot;Lenght of ell [cm]&quot;, ylab=&quot;Comfort temperature [°C]&quot;, pch=16) Figure 2.3: Scatterplot of the length of ell and the comfort temperature of statistics course participants. The covariance between variable \\(x\\) and \\(y\\) is defined as: covariance \\(q = \\frac{1}{n-1}\\sum_{i=1}^{n}((x_i-\\bar{x})*(y_i-\\bar{y}))\\) (R function cov) As for the variance, also the units of the covariance are sqared and therefore covariance values are difficult to interpret. A standardized covariance is the Pearson correlation coefficient: Pearson correlation coefficient: \\(r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\) (R function cor) Means, variances, standard deviations, covariances and correlations are sensible for outliers. Single observations containing extreme values normally have a overproportional influence on these statistics. When outliers are present in the data, we may prefer a more robust correlation measure such as the Spearman correlation or Kendalls tau. Both are based on the ranks of the measurements instead of the measurements themselves. Spearman correlation coefficient: correlation between rank(x) and rank(y) (R function cor(x,y, method=\"spearman\")) Kendalls tau: \\(\\tau = 1-\\frac{4I}{(n(n-1))}\\), where \\(I\\) = number of pairs \\((i,k)\\) for which \\((x_i &lt; x_k)\\) &amp; \\((y_i &gt; y_k)\\) or viceversa. (R function cor(x,y, method=\"kendall\")) 2.2.2 Principal components analyses PCA The principal components analysis (PCA) is a multivariate correlation analysis. A multidimensional data set with \\(k\\) variables can be seen as a cloud of points (observations) in a \\(k\\)-dimensional space. Imagine, we could move around in the space and look at the cloud from different locations. From some locations, the data looks highly correlated, whereas from others, we cannot see the correlation. That is what PCA is doing. It is rotating the coordinate system (defined by the original variables) of the data cloud so that the correlations are no longer visible. The axes of the new coordinates system are linear combinations of the original variables. They are called principal components. There are as many principal coordinates as there are original variables, i.e. \\(k\\), \\(p_1, ..., p_k\\). The principal components meet further requirements: the first component explains most variance the second component explains most of the remaining variance and is perpendicular (= uncorrelated) to the first one third component explains most of the remaining variance and is perpendicular to the first two  For example, in a two-dimensional data set \\((x_1, x_2)\\) the principal components become \\(pc_{1i} = b_{11}x_{1i} + b_{12}x_{2i}\\) \\(pc_{2i} = b_{21}x_{1i} + b_{22}x_{2i}\\) with \\(b_{jk}\\) being loadings of principal component \\(j\\) and original variable \\(k\\). Fig. 2.4 shows the two principal components for a two-dimensional data set. They can be calculated using matrix algebra: principal components are eigenvectors of the covariance or correlation matrix. Figure 2.4: Principal components of a two dimensional data set based on the covariance matrix (green) and the correlation matrix (brown). The choice between correlation or covariance matrix is essential and important. The covariance matrix is an unstandardized correlation matrix. Therefore, the units, i.e., whether cm or m are used, influence the results of the PCA if it is based on the covariance matrix. When the PCA is based on the covariance matrix, the results will change, when we change the units of one variable, e.g., from cm to m. Basing the PCA on the covariance matrix only makes sense, when the variances are comparable among the variables, i.e., if all variables are measured in the same unit and we would like to weight each variable according to its variance. If this is not the case, the PCA must be based on the correlation matrix. pca &lt;- princomp(cbind(x1,x2)) # PCA based on covariance matrix pca &lt;- princomp(cbind(x1,x2), cor=TRUE) # PCA based on correlation matrix loadings(pca) ## ## Loadings: ## Comp.1 Comp.2 ## x1 0.707 0.707 ## x2 0.707 -0.707 ## ## Comp.1 Comp.2 ## SS loadings 1.0 1.0 ## Proportion Var 0.5 0.5 ## Cumulative Var 0.5 1.0 The loadings measure the correlation of each variable with the principal components. They inform about what aspects of the data each component is measuring. The signs of the loadings are arbitrary, thus we can multiplied them by -1 without changing the PCA. Sometimes this can be handy for describing the meaning of the principal component in a paper. For example, Zbinden et al. (2018) combined the number of hunting licenses, the duration of the hunting period and the number of black grouse cocks that were allowed to be hunted per hunter in a principal component in order to measure hunting pressure. All three variables had a negative loading in the first component, so that high values of the component meant low hunting pressure. Before the subsequent analyses, for which a measure of hunting pressure was of interest, the authors changed the signs of the loadings so that this component measured hunting pressure. The proportion of variance explained by each component is, beside the loadings, an important information. If the first few components explain the main part of the variance, it means that maybe not all \\(k\\) variables are necessary to describe the data, or, in other words, the original \\(k\\) variables contain a lot of redundant information. # extract the variance captured by each component summary(pca) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.2679406 0.6263598 ## Proportion of Variance 0.8038367 0.1961633 ## Cumulative Proportion 0.8038367 1.0000000 Ridge regression is similar to doing a PCA within a linear model while components with low variance are shrinked to a higher degree than components with a high variance. 2.3 Inferential statistics 2.3.1 Uncertainty there is never a yes-or-no answer there will always be uncertainty Amrhein (2017)[https://peerj.com/preprints/26857] The decision whether an effect is important or not cannot not be done based on data alone. For a decision we should carefully consider the consequences of each decision, the aims we would like to achieve and the data. Consequences, needs and wishes of different stakeholders can be formally combined with the information in data by using decision theoretical methods. In most data analyses, particularly in basic research and when working on case studies, we normally do not consider consequences of decisions. In these cases, our job is extracting the information of data so that this information later can be used by other scientists, stakeholders and politicians to make decisions. Therefore, statistics is describing pattern in data and quantifying the uncertainty of the described patterns that is due to the fact that the data is just a (small) random sample from the population we would like to know. Quantification of uncertainty is only possible if: 1. the mechanisms that generated the data are known 2. the observations are a random sample from the population of interest Most studies aim at understanding the mechanisms that generated the data, thus they are most likely not known. To overcome that problem, we construct models, e.g. statistical models, that are (strong) abstractions of the data generating process. And we report the model assumptions. All uncertainty measures are conditional on the model we used to analyze the data, i.e., they are only reliable, if the model we used somehow realistically describes the data generating process. Because most statistical models do not describe the data generating process well, the true uncertainty almost always is much higher than the one we report. In order to obtain a random sample from the population under study, a good study design is a prerequisite. To illustrate how inference about a big population is drawn from a small sample, we here use simulated data. The advantage of using simulated data is that the mechanism that generated the data is known. However, in the example, we use different models for simulation and analysis. Imagine there are 300000 PhD students on the world and we would like to know how many statistics courses they have taken before they started their PhD (Fig. 2.5). # simulate the virtual true population set.seed(235325) # set seed for random number generator # simulate fake data of the whole population # using an overdispersed Poisson distribution # There is no need to understand more of this model # at this moment of the course than that this model # produces integer numbers (counts). Poisson models # will be introduced later. statscourses &lt;- rpois(300000, rgamma(300000, 2, 3)) # draw a random sample from the population n &lt;- 12 # sample size y &lt;- sample(statscourses, 12, replace=FALSE) Figure 2.5: Histogram of the number of statistics courses 300000 virtual PhD students have taken before their PhD started. The rugs on the x-axis indicate a random sample of the 300000 students. The black vertical line indicates the mean of the 300000 students (true mean) and the blue line indicates the mean of the sample. We observe the sample mean, what do we know about the population mean? There are two different approaches to answer this question. 1) We could ask us, how much the sample mean would scatter, if we repeat the study many times? This approach is called the frequentist statistics. 2) We could ask us for any possible value, what is the probability that it is the true population mean? To do so, we use probability theory and that is called the Bayesian statistics. Both approaches use (essentially similar) models. Only the mathematical techniques to calculate uncertainty measures differ between the two approaches. In cases when beside the data no other information is used to construct the model, then the results are approximately identical (at least for large enough sample sizes). We illustrate what uncertainty intervals mean in Fig. 2.6. A frequentist 95% confidence interval (blue horizontal semgent in Fig. 2.6) is constructed such that, if you were to (hypothetically) repeat the experiment or sampling many many times, 95% of the intervals constructed would contain the true value of the parameter. From the Bayesian posterior distribution we could construct a 95% interval (e.g., by using the 2.5% and 97.5% quantiles). This interval has traditionally been called credible interval. It can be interpreted that we are 95% sure that the true mean is inside that interval. Both interpretations only are reliable if the model is a realistic abstraction of the data generating process (or if the model assumptions are realistic). Because both terms, confidence and credible interval, suggest that the interval indicates confidence or credibility but the intervals actually show uncertainty, it has been suggested to rename the interval into compatibility or uncertainty interval Gelman and Greenland (2019). Figure 2.6: Histogram of means of repeated samples from the true populations. The scatter of these means visualize the true uncertainty of the mean in this example. The blue vertical line indicates the mean of the original sample. The blue segment shows the 95% confidence interval (obtained by fequensist methods) and the violet line shows the posterior distribution of the mean (obtained by Bayesian methods). For both solutions, we assumed a Normal distribution for the data that is different from the true mechanism that generated the data (which was an overdispersed Poisson model. The star indicates the true mean. 2.3.2 Standard error The standard error SE is, beside the uncertainty interval, an alternative possibility to measure uncertainty. It measures an average deviation of the sample mean from the (unknown) true population mean. The frequentist method for obtaining the SE is based on the central limit theorem. According to the central limit theorem the sum of independent, not necessarily normally distributed random numbers are normally distributed when sample size is large enough (Chapter 4). Because the mean is a sum (divided by a constant, the sample size) it can be assumed that the distribution of many means of samples is normal. It can be mathematically shown that the standard error SE equals the standard deviation SD of the sample divided by the square root of the sample size: frequentist SE = SD/sqrt(n) = \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\) Bayesian SE: Using Bayesian methods, the SE is the SD of the posterior distribution. It is very important to keep the difference between SE and SD in mind! SD measures the scatter of the data, whereas SE measures the uncertainty of the mean (or of another estimated parameter, Fig. 2.7). SD is a descriptive statistics describing a characteristics of the data, whereas SE is an inferential statistics showing us how far away we possibly are from the true parameter value. When sample size increases, SE becomes smaller, whereas SD does not change (given the added observations are drawn at random from the same big population as the ones already in the sample). Figure 2.7: Illustration of the difference between SD and SE. The SD measures the scatter in the data (for the sample (green tickmarks) in green and for the true population (light grey histogram) in a thin black line. The SE measures the uncertainty of the sample mean (in blue for the sample mean and in bold black for the means of the repeated samples (dark grey histogram)). 2.4 Bayes theorem and the common aim of frequentist and Bayesian methods 2.4.1 Bayes theorem for discrete events The Bayes theorem describes the probability of event A conditional on event B (the probability of A after B has already occurred) from the probability of B conditional on A and the two probabilities of the events A and B: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) Imagine, event A is The person likes wine as a birthday present. and event B The person has no car.. The conditional probability of A given B is the probability that a person not owing a car likes wine. Answers from students whether they have a car and what they like as a birthday present are summarized in Table 2.2. Table 2.2: Cross table of the students birthday preference and car ownership. car/birthday flowers wine sum no car 6 9 15 car 1 6 7 sum 7 15 22 We can apply the Bayes theorem to obtain the probability that the student likes wine given it has no car, \\(P(A|B)\\). Lets assume that only the ones who prefer wine above flowers among the students go together for having a glass of wine at the bar after the statistics course. While they drink wine, the tell each other about their cars and they obtain the probability that a student has no car given it likes wine, \\(P(B|A) = 0.6\\). During the statistics class the teacher asked the students about their car ownership and birthday preference. Therefore, they know that \\(P(A) =\\) likes wine \\(= 0.68\\) and \\(P(B) =\\) no car \\(= 0.68\\). With these information, they can obtain the probability that a student likes wine given it has no car, even if not all students without cars went to the bar: \\(P(A|B) = \\frac{0.6*0.68}{0.68} = 0.6\\). 2.4.2 Bayes theorem for continuous parameters When we use the Bayes theorem for analyzing data, then the aim is to make probability statements for parameters. Such a probability statement could be the description of what we know about the population mean after having looked at the data. Because the mean of the population is measured at a continuous scale we use a probability density function to describe what we know about the mean. The Bayes theorem can be formulated for probability density functions denoted with \\(p(\\theta)\\), e.g. for a parameter \\(\\theta\\) (for example probability density functions see Chapter 4). What we are interested in is the probability of the parameter \\(\\theta\\) given the data, i.e., \\(p(\\theta|y)\\). This probability density function is called the posterior distribution of the parameter \\(\\theta\\). Here is the Bayes theorem formulated for obtaining the posterior distribution of a parameter from the data \\(y\\), the prior distribution of the parameter \\(p(\\theta)\\) and assuming a model for the data generating process. The data model is defined by the likelihood that specifies how the data \\(y\\) is distributed given the parameters \\(p(y|\\theta)\\): \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} = \\frac{p(y|\\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta) d\\theta}\\) The probability of the data \\(p(y)\\) is also called the scaling constant, because it is a constant. It is the integral of the likelihood over all possible values of the parameter(s) of the model. 2.4.3 Estimating a mean assuming that the variance is known For illustration, we first describe a simple (unrealistic) example for which it is almost possible to follow the mathematical steps for solving the Bayes theorem even for non-mathematicians. Even if we cannot follow all steps, this example will illustrate the principle how the Bayesian theorem works for continuous parameters. The example is unrealistic because we assume that the variance \\(\\sigma^2\\) in the data \\(y\\) is known. We construct a data model by assuming that \\(y\\) is normally distributed: \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known. The function \\(Norm\\) defines the probability density function of the normal distribution (Chapter 4). The parameter, for which we would like to get the posterior distribution is \\(\\theta\\), the mean. We specify a prior distribution for \\(\\theta\\). Because the normal distribution is a conjugate prior for a normal data model with known variance, we use the normal distribution. Conjugate priors have nice mathematical properties (see Chapter 9) and are therefore preferred when the posterior distribution is obtained algebraically. That is the prior: \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) With the above data, data model and prior, the posterior distribution of the mean \\(\\theta\\) is defined by: \\(p(\\theta|y) = Norm(\\mu_n, \\tau_n)\\), where \\(\\mu_n= \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}}\\) and \\(\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\) \\(\\bar{y}\\) is the arithmetic mean of the data. Because only this value is needed in order to obtain the posterior distribution, this value is called the sufficient statistics. From the mathematical formulas above and also from Fig. 2.8 we see that the mean of the posterior distribution is a weighted average between the prior mean and \\(\\bar{y}\\) with weights equal to the precisions (\\(\\frac{1}{\\tau_0^2}\\) and \\(\\frac{n}{\\sigma^2}\\)). Figure 2.8: Hypothetical example showing the result of applying the Bayes theorem for obtaining a posterior distribution of a continuous parameter. The likelhood is defined by the data and the model, the prior is expressing the knowledge about the parameter before looking at the data. Combining the two distributions using the Bayes theorem results in the posterior distribution. 2.4.4 Estimating the mean and the variance We now move to a more realistic example, which is estimating the mean and the variance of a sample of weights of Snowfinches Montifringilla nivalis (Fig. ??). To analyze those data, a model with two parameters (the mean and the variance or standard deviation) is needed. The data model (or likelihood) is specified as \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\). # weight (g) y &lt;- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5) n &lt;- length(y) Because there are two parameters, we need to specify a two-dimensional prior distribution. We looked up in A Gelman et al. (2014) that the conjugate prior distribution in our case is an Normal-Inverse-Chisquare distribution: \\(p(\\theta, \\sigma) = N-Inv-\\chi^2(\\mu_0, \\sigma_0^2/\\kappa_0; v_0, \\sigma_0^2)\\) From the same reference we looked up how the posterior distribution looks like in our case: \\(p(\\theta,\\sigma|y) = \\frac{p(y|\\theta, \\sigma)p(\\theta, \\sigma)}{p(y)} = N-Inv-\\chi^2(\\mu_n, \\sigma_n^2/\\kappa_n; v_n, \\sigma_n^2)\\), with \\(\\mu_n= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0 + \\frac{n}{\\kappa_0+n}\\bar{y}\\), \\(\\kappa_n = \\kappa_0+n\\), \\(v_n = v_0 +n\\), \\(v_n\\sigma_n^2=v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\) For this example, we need the arithmetic mean \\(\\bar{y}\\) and standard deviation \\(s^2\\) from the sample for obtaining the posterior distribution. Therefore, these two statistics are the sufficient statistics. The above formula look intimidating, but we never really do that calculations. We let Rdoing that for us in most cases by simulating many numbers from the posterior distribution. In the end, we can visualize the distribution of these many numbers to have a look at the posterior distribution. In Fig. 2.9 the two-dimensional \\((\\theta, \\sigma)\\) posterior distribution is visualized by using simulated values. The two dimensional distribution is called the joint posterior distribution. The mountain of dots in Fig. 2.9 visualize the Normal-Inverse-Chisquare that we calculated above. When all values of one parameter is displayed in a histogram ignoring the values of the other parameter, it is called the marginal posterior distribution. Algebraically, the marginal distribution is obtained by integrating one of the two parameters out over joint posterior distribution. This step is definitively way easier when simulated values from the posterior distribution are available! For the Snowfinch weights example, with a normal data model and a conjugate prior the marginal posterior distribution of the mean \\(\\theta\\) becomes a t-distribution and the one of \\(\\sigma\\) a inverse-Chisquare distribution. Another nice property of the posterior distribution in our example is that a horizontal section through the joint posterior distribution (at a fixed value of \\(\\sigma\\), i.e. a fixed variance) leads to a normal distribution. In other words, the density of the dots in the plot at a fixed value of \\(\\sigma\\) follows a normal density function. Figure 2.9: Visualization of the joint posterior distribution for the mean and standard deviation of Snowfinch weights. The lower left panel shows the two-dimensional joint posterior distribution, whereas the upper and right panel show the marginal posterior distributions of each parameter separately. The marginal posterior distributions of every parameter is what we normally report in a paper. They inform us about what we know about these parameters (of the big unmeasured population) after having looked at the data. In our example the the marginal distribution of the mean is a t-distribution (Chapter 4). Frequentist statistical methods also use a t-distribution to describe the uncertainty of a mean for the case when the variance is not known. Thus, frequentist methods came to the same solution using a completely different approach and different techniques. Doesnt that increase dramatically our trust in statistical methods? 2.5 Classical frequentist tests and alternatives 2.5.1 Nullhypothesis testing Null hypothesis testing is constructing a model that describes how the data would look like in case of what we expect to be would not be. Then, the data is compared to how the model thinks the data should look like. If the data does not look like the model thinks they should, we reject that model and accept that our expectation may be true. To decide whether the data looks like the model constructed from the null hypothesis thinks the data should look like the p-value is used. The p-value is the probability of observing the data or more extreme data given the null hypothesis is true. Small p-values indicate that it is rather unlikely to observe the data or more extreme data given the null hypothesis \\(H_0\\) is true. Null hypothesis testing is problematic. We discuss some of the problems after having introduces the most commonly used classical tests. 2.5.2 Comparison of a sample with a fixed values (one-sample t-test) A null hypothesis is used to construct a model that can generate (hypothetical) data. For example, a null hypothesis could be \\(H_0:\\)The mean of Snowfinch weights is exactly 40g. A normal distribution with a mean of \\(\\mu_0=40\\) and a variance equal to the estimated variance in the data \\(s^2\\) is then assumed to describe how we would expect the data to look like given the null hypothesis was true. From that model it is possible to calculate the distribution of hypothetical means of many different hypothetical samples of sample size \\(n\\). The result is a t-distribution (Fig. 2.10). In classical tests, the distribution is standardized so that its variance was one. Then the sample mean, or in classical tests a standardized difference between the mean and the hypothetical mean of the null hypothesis (here 40g), called test statistics \\(t = \\frac{\\bar{y}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\), is compared to that (standardized) t-distribution. If the test statistics falls well within the expected distribution the null hypothesis is accepted. Then, the data is well compatible with the null hypothesis. However, if the test statistics falls in the tails or outside the distribution, then the null hypothesis is rejected and we could write that the mean weight of Snowfinches is statistically significantly different from 40g. Unfortunately, we cannot infer about the probability of the null hypothesis and how relevant the result is. Figure 2.10: Illustration of a one-sample t-test. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the t-distribution that shows how hypothetical sample means are expected to be distributed if the big population of Snowfinches has a mean weight of 40g (i.e., if the null hypothesis was true). Orange area shows the area of the t-distribution that lays equal or farther away from 40g than the sample mean. The orange area is the p-value. We can use the r-function t.test to calculate the p-value of a one sample t-test. t.test(y, mu=40) ## ## One Sample t-test ## ## data: y ## t = 3.0951, df = 7, p-value = 0.01744 ## alternative hypothesis: true mean is not equal to 40 ## 95 percent confidence interval: ## 40.89979 46.72521 ## sample estimates: ## mean of x ## 43.8125 The output of the r-function t.test also includes the mean and the 95% confidence interval (or compatibility or uncertainty interval) of the mean. The CI could, alternatively, be obtained as the 2.5% and 97.5% quantiles of a t-distribution with a mean equal to the sample mean, degrees of freedom equal to the sample size minus one and a standard deviation equal to the standard error of the mean. # lower limit of 95% CI mean(y) + qt(0.025, df=length(y)-1)*sd(y)/sqrt(n) ## [1] 40.89979 # upper limit of 95% CI mean(y) + qt(0.975, df=length(y)-1)*sd(y)/sqrt(n) ## [1] 46.72521 When applying the Bayes theorem for obtaining the posterior distribution of the mean we end up with the same t-distribution as described above, in case we use flat prior distributions for the mean and the standard deviation. Thus, the two different approaches end up with the same result! par(mar=c(4.5, 5, 2, 2)) hist(y, col=&quot;blue&quot;, xlim=c(30,52), las=1, freq=FALSE, main=NA, ylim=c(0, 0.3)) abline(v=mean(y), lwd=2, col=&quot;lightblue&quot;) abline(v=40, lwd=2) lines(density(bsim@coef)) text(45, 0.3, &quot;posterior distribution\\nof the mean of y&quot;, cex=0.8, adj=c(0,1), xpd=NA) Figure 2.11: Illustration of the posterior distribution of the mean. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the posterior distribution that shows what we know about the mean after having looked at the data. The area under the posterior density function that is larger than 40 is the posterior probability of the hypothesis that the true mean Snwofinch weight is larger than 40g. The posterior probability of the hypothesis that the true mean Snowfinch weight is larger than 40g, \\(P(H:\\mu&gt;40) =\\), is equal to the proportion of simulated random values from the posterior distribution, saved in the vector bsim@coef, that are larger than 40. # Two ways of calculating the proportion of values # larger than a specific value within a vector of values round(sum(bsim@coef[,1]&gt;40)/nrow(bsim@coef),2) ## [1] 0.99 round(mean(bsim@coef[,1]&gt;40),2) ## [1] 0.99 # Note: logical values TRUE and FALSE become # the numeric values 1 and 0 within the functions sum() and mean() We, thus, can be pretty sure that the mean Snowfinch weight (in the big world population) is larger than 40g. Such a conclusion is not very informative, because it does not tell us how much larger we can expect the mean Snowfinch weight to be. Therefore, we prefer reporting a credible interval (or compatibility interval or uncertainty interval) that tells us what values for the mean Snowfinch weight are compatible with the data (given the data model we used realistically reflects the data generating process). Based on such an interval, we can conclude that we are pretty sure that the mean Snowfinch weight is between 40 and 48g. # 80% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.1, 0.9)) ## 10% 90% ## 42.07725 45.54080 # 95% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.025, 0.975)) ## 2.5% 97.5% ## 40.90717 46.69152 # 99% credible interval, compatibility interval, uncertainty interval quantile(bsim@coef[,1], probs=c(0.005, 0.995)) ## 0.5% 99.5% ## 39.66181 48.10269 2.5.3 Comparison of the locations between two groups (two-sample t-test) Many research questions aim at measuring differences between groups. For example, we could be curious to know how different in size car owner are from people not owning a car. A boxplot is a nice possibility to visualize the ell length measurements of two (or more) groups (Fig. ??). From the boxplot, we do not see how many observations are in the two samples. We can add that information to the plot. The boxplot visualizes the samples but it does not show what we know about the big (unmeasured) population mean. To show that, we need to add a compatibility interval (or uncertainty interval, credible interval, confidence interval, in brown in Fig. ??). When we added the two means with a compatibility interval, we see what we know about the two means, but we do still not see what we know about the difference between the two means. The uncertainties of the means do not show the uncertainty of the difference between the means. To do so, we need to extract the difference between the two means from a model that describes (abstractly) how the data has been generated. Such a model is a linear model that we will introduce in Chapter 10. The second parameter measures the differences in the means of the two groups. And from the simulated posterior distribution we can extract a 95% compatibility interval. Thus, we can conclude that the average ell length of car owners is with high probability between 0.5 cm smaller and 2.5 cm larger than the averag ell of people not having a car. mod &lt;- lm(ell~car, data=dat) mod ## ## Call: ## lm(formula = ell ~ car, data = dat) ## ## Coefficients: ## (Intercept) carY ## 43.267 1.019 bsim &lt;- sim(mod, n.sim=nsim) quantile(bsim@coef[,&quot;carY&quot;], prob=c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -0.501348 1.014478 2.494324 The corresponding two-sample t-test gives a p-value for the null hypothesis: The difference between the two means equals zero., a confidence interval for the difference and the two means. While the function lmgives the difference Y minus N, the function t.testgives the difference N minus Y. Luckily the two means are also given in the output, so we know which group mean is the larger one. t.test(ell~car, data=dat, var.equal=TRUE) ## ## Two Sample t-test ## ## data: ell by car ## t = -1.4317, df = 20, p-value = 0.1677 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.5038207 0.4657255 ## sample estimates: ## mean in group N mean in group Y ## 43.26667 44.28571 In both possibilities, we used to compare the to means, the Bayesian posterior distribution of the difference and the t-test or the confidence interval of the difference, we used a data model. We thus assumed that the observations are normally distributed. In some cases, such an assumption is not a reasonable assumption. Then the result is not reliable. In such cases, we can either search for a more realistic model or use non-parametric (also called distribution free) methods. Nowadays, we have almost infinite possibilities to construct data models (e.g. generalized linear models and beyond). Therefore, we normally start looking for a model that fits the data better. However, in former days, all these possiblities did not exist (or were not easily available for non-mathematicians). Therefore, we here introduce two of such non-parametric methods, the Wilcoxon-test (or Mann-Whitney-U-test) and the randomisation test. Some of the distribution free statistical methods are based on the rank instead of the value of the observations. The principle of the Wilcoxon-test is to rank the observations and sum the ranks per group. It is not completely true that the non-parametric methods do not have a model. The model of the Wilcoxon-test knows how the difference in the sum of the ranks between two groups is distributed given the mean of the two groups do not differ (null hypothesis). Therefore, it is possible to get a p-value, e.g. by the function wilcox.test. wilcox.test(ell~car, data=dat) ## ## Wilcoxon rank sum test with continuity correction ## ## data: ell by car ## W = 34.5, p-value = 0.2075 ## alternative hypothesis: true location shift is not equal to 0 The note in the output tells us that ranking is ambiguous, when some values are equal. Equal values are called ties when they should be ranked. The result of the Wilcoxon-test tells us how probable it is to observe the difference in the rank sum between the two sample or a more extreme difference given the means of the two groups are equal. That is at least something. A similar result is obtained by using a randomisation test. This test is not based on ranks but on the original values. The aim of the randomisation is to simulate a distribution of the difference in the arithmetic mean between the two groups assuming this difference would be zero. To do so, the observed values are randomly distributed among the two groups. Because of the random distribution among the two groups, we expect that, if we repeat that virtual experiment many times, the average difference between the group means would be zero (both virtual samples are drawn from the same big population). We can use a loop in R for repeating the random re-assignement to the two groups and, each time, extracting the difference between the group means. As a result, we have a vector of many (nsim) values that all are possible differences between group means given the two samples were drawn from the same population. The proportion of these values that have an equal or larger absolute value give the probability that the observed or a larger difference between the group means is observed given the null hypothesis would be true, thus that is a p-value. diffH0 &lt;- numeric(nsim) for(i in 1:nsim){ randomcars &lt;- sample(dat$car) rmod &lt;- lm(ell~randomcars, data=dat) diffH0[i] &lt;- coef(rmod)[&quot;randomcarsY&quot;] } mean(abs(diffH0)&gt;abs(coef(mod)[&quot;carY&quot;])) # p-value ## [1] 0.1858 Visualizing the possible differences between the group means given the null hypothesis was true shows that the observed difference is well within what is expected if the two groups would not differ in their means (Fig. 2.12). Figure 2.12: Histogram if differences between the means of randomly assigned groups (grey) and the difference between the means of the two observed groups (red) The randomization test results in a p-value and, we could also report the observed difference between the group means. However, it does not tell us, what values of the difference all would be compatible with the data. We do not get an uncertainty measurement for the difference. In order to get a compatibility interval without assuming a distribution for the data (thus non-parametric) we could bootstrap the samples. Bootstrapping is sampling observations from the data with replacement. For example, if we have a sample of 8 observations, we draw 8 times a random observation from the 8 observation. Each time, we assume that all 8 observations are available. Thus a bootstrapped sample could include some observations several times, whereas others are missing. In this way, we simulate the variance in the data that is due to the fact that our data do not contain the whole big population. Also bootstrapping can be programmed in R using a loop. diffboot &lt;- numeric(1000) for(i in 1:nsim){ ngroups &lt;- 1 while(ngroups==1){ bootrows &lt;- sample(1:nrow(dat), replace=TRUE) ngroups &lt;- length(unique(dat$car[bootrows])) } rmod &lt;- lm(ell~car, data=dat[bootrows,]) diffboot[i] &lt;- coef(rmod)[2] } quantile(diffboot, prob=c(0.025, 0.975)) ## 2.5% 97.5% ## -0.3395643 2.4273810 The resulting values for the difference between the two group means can be interpreted as the distribution of those differences, if we had repeated the study many times (Fig. 2.13). A 95% interval of the distribution corresponds to a 95% compatibility interval (or confidence interval or uncertainty interval). hist(diffboot); abline(v=coef(mod)[2], lwd=2, col=&quot;red&quot;) Figure 2.13: Histogram of the boostrapped differences between the group means (grey) and the observed difference. For both methods, randomisation test and bootstrapping, we have to assume that all observations are independent. Randomization and bootstrapping becomes complicated or even unfeasible when data are structured. 2.6 Summary Bayesian data analysis is applying the Bayes theorem for summarizing knowledge based on data, priors and the model assumptions. Frequentist statistics is quantifying uncertainty by hypothetical repetitions. "],["analyses-steps.html", "3 Data analysis step by step 3.1 Plausibility of Data 3.2 Relationships 3.3 Data Distribution 3.4 Preparation of Explanatory Variables 3.5 Data Structure 3.6 Define Prior Distributions 3.7 Fit the Model 3.8 Check Model 3.9 Model Uncertainty 3.10 Draw Conclusions Further reading", " 3 Data analysis step by step In this chapter we provide a checklist with some guidance for data analysis. However, do not expect the list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps 3.2 to 3.8 until we find a model that fit the data well and that is realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful model. There is a chance and danger at the same time: we may find interesting results that answer different questions than we asked originally. They may be very exciting and important, however they may be biased. We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we have to be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process. 3.1 Plausibility of Data Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries. See chapter ?? for useful R-code that can be used for data preparation and to make plausibility controls. 3.2 Relationships Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful. We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible. 3.3 Data Distribution What is the nature of the variable of interest (outcome, dependent variable)? At this stage, there is no use of formally comparing the distribution of the outcome variable to a statistical distribution, because the rawdata is not required to follow a specific distribution. The models assume that conditional on the explanatory variables and the model structure, the outcome variable follows a specific distribution. Therefore, checking how well the chosen distribution fits to the data is done after the model fit 3.8. This first choice is solely done based on the nature of the data. Normally, our first choice is one of the classical distributions for which robust software for model fitting is available. Here is a rough guideline for this first choice: continuous measurements \\(\\Longrightarrow\\) normal distribution &gt; exceptions: time-to-event data \\(\\Longrightarrow\\) see survival analysis count \\(\\Longrightarrow\\) Poisson or negative-binomial distribution count with upper bound (proportion) \\(\\Longrightarrow\\) binomial distribution binary \\(\\Longrightarrow\\) Bernoully distribution rate (count by a reference) \\(\\Longrightarrow\\) Poisson including an offset nominal \\(\\Longrightarrow\\) multinomial distribution Chapter 4 gives an overview of the distributions that are most relevant for ecologists. 3.4 Preparation of Explanatory Variables Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step: Is the variance (of the explanatory variable) big enough so that an effect of the variable can be measured? Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable (e.g., log, square-root). Does it show a bimodal distribution? Consider making the variable binary. Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a trans- formation (e.g., log) could linearize the relationship between x and y. Centering: Centering (\\(x.c = x-mean(x)\\)) is a transformation that produces a variable with a mean of 0. Centering is optional. We have two reasons to center a predictor variable. First, it helps the model fitting algorithm to better converge because it reduces correlations among model parameters. Second, with centered predictors, the intercept and main effects in the linear model are better interpretable (they are measured at the center of the data instead of at the covariate value of 0 which may be far off). Scaling: Scaling (\\(x.s = x/c\\), where \\(c\\) is a constant) is a transformation that changes the unit of the variable. Also scaling is optional. We have three reasons to scale an predictor variable. First, to make the effect sizes better understandable. For example, a population change from one year to the next may be very small and hard to interpret. When we give the change for a 10-year period, its ecological meaning is better understandable. Second, to make the estimate of the effect sizes comparable between variables, we may use \\(x.s = x/sd(x)\\). The resulting variable has a unit of one standard deviation. A standard deviation may be comparable between variables that oritinally are measured in different units (meters, seconds etc). Gelman and Hill (2007) (p. 55 f) propose to scale the variables by two times the standard deviation (\\(x.s = x/(2*sd(x))\\)) to make effect sizes comparable between numeric and binary variables. Third, scaling can be important for model convergence, especially when polynomials are included. Also, consider the use of orthogonal polynomials, see Chapter 4.2.9 in Korner-Nievergelt et al. (2015). Collinearity: Look at the correlation among the explanatory variables (pairs plot or correlation matrix). If the explanatory variables are correlated, go back to step 2. Also, Chapter 4.2.7 in Korner-Nievergelt et al. (2015) discusses collinearity. Are interactions and polynomial terms needed in the model? If not already done in step 2, think about the relationship between each explanatory variable and the dependent variable. Is it linear or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM). May the effect of one explanatory variable depend on the value of another explanatory variable (interaction)? 3.5 Data Structure After having taken into account all of the (fixed effect) terms from step 4: are the observations independent or grouped/structured? What random factors are needed in the model? Are the data obviously temporally or spatially correlated? Or, are other correlation structures present, such as phylogenetic relationships? Our strategy is to start with a rather simple model that may not account for all correlation structures that in fact are present in the data. We first, only include those that are known to be important a priory. Only when residual analyses reveals important additional correlation structures, we include them in the model. 3.6 Define Prior Distributions Decide whether we would like to use informative prior distributions or whether we would like use priors that only have a negligible effect on the results. When the results are later used for informing authorities or for making a decision (as usual in applied sciences), then we would like to base the results on all information available. Information from the literature is then used to construct informative prior distributions. In contrast to applied sciences, in basic research we often would like to show only the information in the data that should not be influenced by earlier results. Therefore, in basic research we look for priors that do not influence the results. 3.7 Fit the Model Fit the model. 3.8 Check Model We assess model fit by graphical analyses of the residuals (Chapter 6 in Korner-Nievergelt et al. (2015)), by predictive model checking (Section 10.1 in Korner-Nievergelt et al. (2015)), or by sensitivity analysis (Chapter 15 in Korner-Nievergelt et al. (2015)). For non-Gaussian models it is often easier to assess model fit using pos- terior predictive checks (Chapter 10 in Korner-Nievergelt et al. (2015)) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, we list in Chapter 16 of Korner-Nievergelt et al. (2015) some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases. 3.9 Model Uncertainty If, while working through steps 1 to 8, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we then turn to the methods presented in Chapter 11 (Korner-Nievergelt et al. (2015)) to draw inference from more than one model. If we have only one model, we proceed to 3.10. 3.10 Draw Conclusions Simulate values from the joint posterior distribution of the model parameters (sim or Stan). Use these samples to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities. Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],["distributions.html", "4 Probability distributions 4.1 Introduction 4.2 Normal distribution 4.3 Poisson distribution 4.4 Gamma distribution 4.5 F-distribution", " 4 Probability distributions 4.1 Introduction 4.2 Normal distribution normal distribution = Gaussian distribution \\(p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(\\theta -\\mu)^2) = Normal(\\mu, \\sigma)\\) \\(E(\\theta) = \\mu\\), \\(var(\\theta) = \\sigma^2\\), \\(mode(\\theta) = \\mu\\) The variance parameter can be specified to be a variance, a standard deviation or a precision. Depending on the software used (or author of the paper) different habits exist. 4.3 Poisson distribution xxx 4.4 Gamma distribution xxx 4.4.1 Cauchy distribution We sometimes use the Cauchy distiribution to specify the prior distirbution of the standart deviation and similar parameters in a model. An example for this is the regression model that we used to introduce Stan (Chapter 8.3). 4.4.2 t-distribution marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution \\(p(\\theta|v,\\mu,\\sigma) = \\frac{\\Gamma((v+1)/2)}{\\Gamma(v/2)\\sqrt{v\\pi}\\sigma}(1+\\frac{1}{v}(\\frac{\\theta-\\mu}{\\sigma})^2)^{-(v+1)/2}\\) \\(v\\) degrees of freedom \\(\\mu\\) location \\(\\sigma\\) scale 4.5 F-distribution Ratios of sample variances drawn from populations with equal variances follow an F-distribution. The density function of the F-distribution is even more complicated than the one of the t-distribution! We do not copy it here. Further, we have not yet met any Bayesian example where the F-distribution is used (that does not mean that there is no). It is used in frequentist analyses in order to compare variances, and, within the ANOVA, to compare means between groups. If two variances only differ because of natural variance in the data (nullhypothesis) then \\(\\frac{Var(X_1)}{Var(X_2)}\\sim F_{df_1,df_2}\\). Figure 4.1: Different density functions of the F statistics "],["addbasics.html", "5 Additional basic material 5.1 Chisquare test 5.2 3 methods for getting the posterior distribution 5.3 Analysis of variance ANOVA 5.4 Bayesian way of analysing correlations between categorical variables 5.5 Summary", " 5 Additional basic material 5.1 Chisquare test The chisquare test is used for two frequentist purposes. 1. Testing for correlations between two categorical variables. 2. Comparison of two distributions (goodness of fit test) When testing for correlations between two categorical variables, then the nullhypothesis is there is no correlation. The data can be displayed in cross-tables. # Example: correlation between birthday preference and car ownership load(&quot;RData/datacourse.RData&quot;) table(dat$birthday, dat$car) ## ## N Y ## flowers 6 1 ## wine 9 6 Given the nullhypothesis is true, we expect that the distribution of the data in each column of the cross-table is similar to the distribution of the row-sums. And, the distribution of the data in each row should be similar to the distribution of the column-sums. The chisquare test statistics \\(\\chi^2\\) measures the deviation of the data from this expected distribution of the data in the cross-table. For calculating the chisquare test statistics \\(\\chi^2\\), we first have to obtain for each cell in the cross-table the expected value \\(E_{ij}\\) = rowsum*colsum/total. \\(\\chi^2\\) measures the difference between the observed \\(O_{ij}\\) and expected \\(E_{ij}\\) values as: \\(\\chi^2=\\sum_{i=1}^{m}\\sum_{j=1}^{k}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\) where \\(m\\) is the number of rows and \\(k\\) is the number of columns. The \\(\\chi^2\\)-distribution has 1 parameter, the degrees of freedom \\(v\\) = \\((m-1)(k-1)\\). R is calculating the \\(\\chi^2\\) value for specific cross-tables, and it is also giving the p-values, i.e., the probability of obtaining the observed or a higher \\(\\chi^2\\) value given the nullhypothesis is true by comparing the observed \\(\\chi^2\\) with the corresponding chisquare distribution. chisq.test(table(dat$birthday, dat$car)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(dat$birthday, dat$car) ## X-squared = 0.51084, df = 1, p-value = 0.4748 The warning (that is suppressed in the rmarkdown version, but that you will see if you run the code on your own computer) is given, because in our example some cells have counts less than 5. In such cases, the Fishers exact test should be preferred. This test calculates the p-value analytically using probability theory, whereas the chisquare test relies on the assumption that the \\(\\chi^2\\) value follows a chisquare distribution. The latter assumption holds better for larger sample sizes. fisher.test(table(dat$birthday, dat$car)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(dat$birthday, dat$car) ## p-value = 0.3501 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3153576 213.8457248 ## sample estimates: ## odds ratio ## 3.778328 5.2 3 methods for getting the posterior distribution analytically approximation Monte Carlo simulation 5.2.1 Monte Carlo simulation (parametric bootstrap) Monte Carlo integration: numerical solution of \\(\\int_{-1}^{1.5} F(x) dx\\) sim is solving a mathematical problem by simulation How sim is simulating to get the marginal distribution of \\(\\mu\\): 5.2.2 Grid approximation \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}\\) For example, one coin flip (Bernoulli model) data: y=0 (a tail) likelihood: \\(p(y|\\theta)=\\theta^y(1-\\theta)^{(1-y)}\\) 5.2.3 Markov chain Monte Carlo simulations Markov chain Monte Carlo simulation (BUGS, Jags) Hamiltonian Monte Carlo (Stan) 5.3 Analysis of variance ANOVA The aim of an ANOVA is to compare means of groups. In a frequentist analysis, this is done by comparing the between-group with the within-group variance. The result of a Bayesian analysis is the joint posterior distribution of the group means. Figure 5.1: Number of stats courses students have taken before starting a PhD in relation to their feeling about statistics. In the frequentist ANOVA, the following three sum of squared distances (SS) are used to calculate the total, the between- and within-group variances: Total sum of squares = SST = \\(\\sum_1^n{(y_i-\\bar{y})^2}\\) Within-group SS = SSW = \\(\\sum_1^n{(y_i-\\bar{y_g})^2}\\): unexplained variance Between-group SS = SSB = \\(\\sum_1^g{n_g(\\bar{y_g}-\\bar{y})^2}\\): explained variance The between-group and within-group SS sum to the total sum of squares: SST=SSB+SSW. Attention: this equation is only true in any case for a simple one-way ANOVA (just one grouping factor). If the data are grouped according to more than one factor (such as in a two- or three-way ANOVA), then there is one single solution for the equation only when the data is completely balanced, i.e. when there are the same number of observations in all combinations of factor levels. For non-balanced data with more than one grouping factor, there are different ways of calculating the SSBs, and the result of the F-test described below depends on the order of the predictors in the model. Figure 5.2: Visualisation of the total, between-group and within-group sum of squares. Points are observations; long horizontal line is the overall mean; short horizontal lines are group specific means. In order to make SSB and SSW comparable, we have to divide them by their degrees of freedoms. For the within-group SS, SSW, the degrees of freedom is the number of obervations minus the number of groups (\\(g\\)), because \\(g\\) means have been estimated from the data. If the \\(g\\) means are fixed and \\(n-g\\) data points are known, then the last \\(g\\) data points are defined, i.e., they cannot be chosen freely. For the between-group SS, SSB, the degrees of freedom is the number of groups minus 1 (the minus 1 stands for the overall mean). MSB = SSB/df_between, MSW = SSW/df_within It can be shown (by mathematicians) that, given the nullhypothesis, the mean of all groups are equal \\(m_1 = m_2 = m_3\\), then the mean squared errors between groups (MSB) is expected to be equal to the mean squared errors within the groups (MSW). Therefore, the ration MSB/MSW is expected to follow an F-distribution given the nullhypothesis is true. MSB/MSW ~ F(df_between, df_within) The Bayesian analysis for comparing group means consists of calculating the posterior distribution for each group mean and then drawing inference from these posterior distributions. A Bayesian one-way ANOVA involves the following steps: 1. Decide for a data model: We, here, assume that the measurements are normally distributed around the group means. In this example here, we transform the outcome variable in order to better meet the normal assumption. Note: the frequentist ANOVA makes exactly the same assumptions. We can write the data model: \\(y_i\\sim Norm(\\mu_i,\\sigma)\\) with \\(mu_i= \\beta_0 + \\beta_1I(group=2) +\\beta_1I(group=3)\\), where the \\(I()\\)-function is an indicator function taking on 1 if the expression is true and 0 otherwise. This model has 4 parameters: \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\sigma\\). # fit a normal model with 3 different means mod &lt;- lm(log(nrcourses+1)~statsfeeling, data=dat) Choose a prior distribution for each model parameter: In this example, we choose flat prior distributions for each parameter. By using these priors, the result should not remarkably be affected by the prior distributions but almost only reflect the information in the data. We choose so-called improper prior distributions. These are completely flat distributions that give all parameter values the same probability. Such distributions are called improper because the area under the curve is not summing to 1 and therefore, they cannot be considered to be proper probability distributions. However, they can still be used to solve the Bayesian theorem. Solve the Bayes theorem: The solution of the Bayes theorem for the above priors and model is implemented in the function sim of the package arm. # calculate numerically the posterior distributions of the model # parameters using flat prior distributions nsim &lt;- 5000 set.seed(346346) bsim &lt;- sim(mod, n.sim=nsim) Display the joint posterior distributions of the group means # calculate group means from the model parameters newdat &lt;- data.frame(statsfeeling=levels(factor(dat$statsfeeling))) X &lt;- model.matrix(~statsfeeling, data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- X%*%bsim@coef[i,] hist(fitmat[1,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab=&quot;Group mean of log(number of courses +1)&quot;, las=1, ylim=c(0, 2.2)) hist(fitmat[2,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab=&quot;&quot;, las=1, add=TRUE, col=rgb(0,0,1,0.5)) hist(fitmat[3,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab=&quot;&quot;, las=1, add=TRUE, col=rgb(1,0,0,0.5)) legend(2,2, fill=c(&quot;white&quot;,rgb(0,0,1,0.5), rgb(1,0,0,0.5)), legend=levels(factor(dat$statsfeeling))) Figure 5.3: Posterior distributions of the mean number of stats courses PhD students visited before starting the PhD grouped according to their feelings about statistics. Based on the posterior distributions of the group means, we can extract derived quantities depending on our interest and questions. Here, for example, we could extract the posterior probability of the hypothesis that students with a positive feeling about statistics have a better education in statistics than those with a neutral or negative feeling about statistics. # P(mean(positive)&gt;mean(neutral)) mean(fitmat[3,]&gt;fitmat[2,]) ## [1] 0.8754 # P(mean(positive)&gt;mean(negative)) mean(fitmat[3,]&gt;fitmat[1,]) ## [1] 0.9798 5.4 Bayesian way of analysing correlations between categorical variables For a Bayesian analysis of cross-table data, a data model has to be found. There are several possibilities that could be used: a so-called log-linear model (Poisson model) for the counts in each cell of the cross-table. a binomial or a multinomial model for obtaining estimates of the proportions of data in each cell These models provide possibilities to explore the patterns in the data in more details than a chisquare test. # log-linear model mod &lt;- glm(count~birthday+car + birthday:car, data=datagg, family=poisson) bsim &lt;- sim(mod, n.sim=nsim) round(t(apply(bsim@coef, 2, quantile, prob=c(0.025, 0.5, 0.975))),2) ## 2.5% 50% 97.5% ## (Intercept) 0.99 1.79 2.58 ## birthdaywine -0.60 0.41 1.43 ## carY -3.89 -1.77 0.29 ## birthdaywine:carY -0.94 1.38 3.65 The interaction parameter measures the strength of the correlation. To quantitatively understand what a parameter value of 1.39 means, we have to look at the interpretation of all parameter values. We do that here quickly without a thorough explanation, because we already explained the Poisson model in chapter 8 of (Korner-Nievergelt et al. 2015). The intercept 1.79 corresponds to the logarithm of the count in the cell flowers and N (number of students who prefer flowers as a birthday present and who do not have a car), i.e., \\(exp(\\beta_0)\\) = 6. The exponent of the second parameter corresponds to the multiplicative difference between the counts in the cells flowers and N and wine and N, i.e., count in the cell wine and N = \\(exp(\\beta_0)exp(\\beta_1)\\) = exp(1.79)exp(0.41) = 9. The third parameter measures the multiplicative difference in the counts between the cells flowers and N and flowers and Y, i.e., count in the cell flowers and Y = \\(exp(\\beta_0)exp(\\beta_2)\\) = exp(1.79)exp(-1.79) = 1. Thus, the third parameter is the difference in the logarithm of the counts between the car owners and the car-free students for those who prefer flowers. The interaction parameter is the difference of this difference between the students who prefer wine and those who prefer flowers. This is difficult to intuitively understand. Here is another try to formulate it: The interaction parameter measures the difference in the logarithm of the counts in the cross-table between the row-differences between the columns. Maybe it becomes clear, when we extract the count in the cell wine and Y from the model parameters: \\(exp(\\beta_0)exp(\\beta_1)exp(\\beta_2)exp(\\beta_3)\\) = exp(1.79)exp(0.41)exp(-1.79)exp(1.39) = 6. Alternatively, we could estimate the proportions of flower and wine preferers within each group of car owners and car-free students using a binomial model. For an explanation of the binomial model, see chapter 8 of (Korner-Nievergelt et al. 2015). # binomial model tab &lt;- table(dat$car,dat$birthday) mod &lt;- glm(tab~rownames(tab), family=binomial) bsim &lt;- sim(mod, n.sim=nsim) Figure 5.4: Estimated proportion of students that prefer flowers over wine as a birthday present among the car-free students (N) and the car owners (Y). Given are the median of the posterior distribution (circle). The bar extends between the 2.5% and 97.5% quantiles of the posterior distribution. 5.5 Summary "],["furthertopics.html", "6 Further topics 6.1 Bioacoustic analyse 6.2 Python", " 6 Further topics This is a collection of short introductions or links with commented R code that cover other topics that might be usful for ecologists. 6.1 Bioacoustic analyse Bioacoustic analyses are nicely covered in a blog by Marcelo Araya-Salas. 6.2 Python Like R, python is a is a high-level programming language that is used by many ecologists. The reticulate package provides a comprehensive set of tools for interoperability between Python and R. "],["PART-II.html", "7 Introduction to PART II Further reading", " 7 Introduction to PART II Further reading A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. A Gelman et al. 2014) and Trevor Hastie (e.g. (Hastie, Tibshirani, and Friedman 2009, @Efron2016)) because both explain complicated things in a concise and understandable way. "],["stan.html", "8 MCMC using Stan 8.1 Background 8.2 Install rstan 8.3 Writing a Stan model 8.4 Run Stan from R Further reading", " 8 MCMC using Stan 8.1 Background Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. 8.2 Install rstan In this book we use the program Stan to draw random samples from the joint posterior distribution of the model parameters given a model, the data, prior distributions, and initial values. To do so, it uses the no-U-turn sampler, which is a type of Hamiltonian Monte Carlo simulation (Hoffman and Gelman 2014; Betancourt 2013), and optimization-based point estimation. These algorithms are more efficient than the ones implemented in BUGS programs and they can handle larger data sets. Stan works particularly well for hierar- chical models (Betancourt and Girolami 2013). Stan runs on Windows, Mac, and Linux and can be used via the R interface rstan. Stan is automatically installed when the R package rstan is installed. For installing rstan, it is advised to follow closely the system-specific instructions. 8.3 Writing a Stan model The statistical model is written in the Stan language and saved in a text file. The Stan language is rather strict, forcing the user to write unambiguous models. Stan is very well documented and the Stan Documentation contains a comprehensive Language Manual, a Wiki documentation and various tutorials. We here provide a normal regression with one predictor variable as a worked example. The entire Stan model is as following (saved as linreg.stan) data { int&lt;lower=0&gt; n; vector[n] y; vector[n] x; } parameters { vector[2] beta; real&lt;lower=0&gt; sigma; } model { //priors beta ~ normal(0,5); sigma ~ cauchy(0,5); // likelihood y ~ normal(beta[1] + beta[2] * x, sigma); } A Stan model consists of different named blocks. These blocks are (from first to last): data, transformed data, parameters, trans- formed parameters, model, and generated quantities. The blocks must appear in this order. The model block is mandatory; all other blocks are optional. In the data block, the type, dimension, and name of every variable has to be declared. Optionally, the range of possible values can be specified. For example, vector[N] y; means that y is a vector (type real) of length N, and int&lt;lower=0&gt; N; means that N is an integer with nonnegative values (the bounds, here 0, are included). Note that the restriction to a possible range of values is not strictly necessary but this will help specifying the correct model and it will improve speed. We also see that each line needs to be closed by a column sign. In the parameters block, all model parameters have to be defined. The coefficients of the linear predictor constitute a vector of length 2, vector[2] beta;. Alternatively, real beta[2]; could be used. The sigma parameter is a one-number parameter that has to be positive, therefore real&lt;lower=0&gt; sigma;. The model block contains the model specification. Stan functions can handle vectors and we do not have to loop over all observations as typical for BUGS . Here, we use a Cauchy distribution as a prior distribution for sigma. This distribution can have negative values, but because we defined the lower limit of sigma to be 0 in the parameters block, the prior distribution actually used in the model is a truncated Cauchy distribution (truncated at zero). In Chapter 9.2 we explain how to choose prior distributions. Further characteristics of the Stan language that are good to know include: The variance parameter for the normal distribution is specified as the standard deviation (like in R but different from BUGS, where the precision is used). If no prior is specified, Stan uses a uniform prior over the range of possible values as specified in the parameter block. Variable names must not contain periods, for example, x.z would not be allowed, but x_z is allowed. To comment out a line, use double forward-slashes //. 8.4 Run Stan from R We fit the model to simulated data. Stan needs a vector containing the names of the data objects. In our case, x, y, and N are objects that exist in the R console. The function stan() starts Stan and returns an object containing MCMCs for every model parameter. We have to specify the name of the file that contains the model specification, the data, the number of chains, and the number of iterations per chain we would like to have. The first half of the iterations of each chain is declared as the warm-up. During the warm-up, Stan is not simulating a Markov chain, because in every step the algorithm is adapted. After the warm-up the algorithm is fixed and Stan simulates Markov chains. library(rstan) # Simulate fake data n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # random numbers of the covariate simresid &lt;- rnorm(n, 0, sd=sigma) # residuals y &lt;- b0 + b1*x + simresid # calculate y, i.e. the data # Bundle data into a list datax &lt;- list(n=length(y), y=y, x=x) # Run STAN fit &lt;- stan(file = &quot;stanmodels/linreg.stan&quot;, data=datax, verbose = FALSE) ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.101 seconds (Warm-up) ## Chain 1: 0.102 seconds (Sampling) ## Chain 1: 0.203 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.098 seconds (Warm-up) ## Chain 2: 0.083 seconds (Sampling) ## Chain 2: 0.181 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.1 seconds (Warm-up) ## Chain 3: 0.093 seconds (Sampling) ## Chain 3: 0.193 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.101 seconds (Warm-up) ## Chain 4: 0.079 seconds (Sampling) ## Chain 4: 0.18 seconds (Total) ## Chain 4: Further reading Stan-Homepage: It contains the documentation for Stand a a lot of tutorials. "],["priors.html", "9 Prior distributions 9.1 Introduction 9.2 How to choose a prior 9.3 Prior sensitivity", " 9 Prior distributions 9.1 Introduction 9.2 How to choose a prior Tabelle von Fränzi (CourseIII_glm_glmmm/course2018/presentations_handouts/presentations) 9.3 Prior sensitivity xxx "],["lm.html", "10 Normal Linear Models 10.1 Linear Regression 10.2 Regression Variants: ANOVA, ANCOVA, and Multiple Regression 10.3 Pendenzen", " 10 Normal Linear Models 10.1 Linear Regression 10.1.1 Background Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression. Typical questions that can be answered using linear regression are: How does \\(y\\) change with changes in \\(x\\)? How is y predicted from \\(x\\)? An ordinary linear regression (i.e., one numeric \\(x\\) and one numeric \\(y\\) variable) can be represented by a scatterplot of \\(y\\) against \\(x\\). We search for the line that ts best and describe how the observations scatter around this regression line (see Fig. 10.1 for an example). The model formula of a simple linear regression with one continuous predictor variable \\(x_i\\) (the subscript \\(i\\) denotes the \\(i=1,\\dots,n\\) data points) is: \\[\\begin{align} \\mu_i &amp;=\\beta_0 + \\beta_1 x_i \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{10.1} \\end{align}\\] While the first part of Equation (10.1) describes the regression line, the second part describes the differences between predicted values \\(\\mu_i\\) and observations \\(y_i\\). In other words: the observation \\(y_i\\) stems from a normal distribution with mean \\(\\mu_i\\) and variance \\(\\sigma^2\\) . The mean of the normal distribution, \\(\\mu_i\\) , equals the sum of the intercept (\\(b_0\\) ) and the product of the slope (\\(b_1\\)) and the continuous predictor value, \\(x_i\\). The differences between observation \\(y_i\\) and the predicted values \\(\\mu_i\\) are the residuals (i.e., \\(\\epsilon_i=y_i-\\mu_i\\)). Equivalently to Equation (10.1), the regression could thus be written as: \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\ \\epsilon_i &amp;\\sim Norm(0, \\sigma^2) \\tag{10.2} \\end{align}\\] We prefer the notation in Equation (10.1) because, in this formula, the stochastic part (second row) is nicely separated from the deterministic part (first row) of the model, whereas, in the second notation (10.2) the rst row contains both stochastic and deterministic parts. Using matrix notation equation (10.1) can also be written in one row: \\[\\boldsymbol{y} \\sim Norm(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2\\boldsymbol{I})\\] where \\(\\boldsymbol{ I}\\) is the \\(n \\times n\\) identity matrix (it transforms the variance parameter to a \\(n \\times n\\) matrix with its diagonal elements equal \\(\\sigma^2\\) ; \\(n\\) is the sample size). The multiplication by \\(\\boldsymbol{ I}\\) is necessary because we use vector notation, \\(\\boldsymbol{y}\\) instead of \\(y_{i}\\) . Here, \\(\\boldsymbol{y}\\) is the vector of all observations, whereas \\(y_{i}\\) is a single observation, \\(i\\). When using vector notation, we can write the linear predictor of the model, \\(\\beta_0 + \\beta_1 x_i\\) , as a multiplication of the vector of the model coefcients \\[\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\] times the model matrix \\[\\boldsymbol{X} = \\begin{pmatrix} 1 &amp; x_1 \\\\ \\dots &amp; \\dots \\\\ 1 &amp; x_n \\end{pmatrix}\\] where \\(x_1 , \\dots, x_n\\) are the observed values for the predictor variable, \\(x\\). The rst column of \\(\\boldsymbol{X}\\) contains only ones because the values in this column are multiplied with the intercept, \\(\\beta_0\\) . To the intercept, the product of the second element of \\(\\boldsymbol{\\beta}\\), \\(\\beta_1\\) , with each element in the second column of \\(\\boldsymbol{X}\\) is added to obtain the predicted value for each observation, \\(\\boldsymbol{\\mu}\\): \\[\\begin{align} \\boldsymbol{\\beta X}= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} \\times \\begin{pmatrix} 1 &amp; x_1 \\\\ \\dots &amp; \\dots \\\\ 1 &amp; x_n \\end{pmatrix} = \\begin{pmatrix} \\beta_0 + \\beta_1x_1 \\\\ \\dots \\\\ \\beta_0 + \\beta_1x_n \\end{pmatrix}= \\begin{pmatrix} \\hat{y}_1 \\\\ \\dots \\\\ \\hat{y}_n \\end{pmatrix} = \\boldsymbol{\\mu} \\tag{10.3} \\end{align}\\] 10.1.2 Fitting a Linear Regression in R In Equation (10.1), the predicted (synonym: fitted) values \\(\\mu_i\\) are directly dened by the model coefcients, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) . Therefore, when we can estimate \\(\\beta_{0}\\), \\(\\beta_{1}\\) , and \\(\\sigma^2\\), the model is fully dened . The last parameter \\(\\sigma^2\\) describes how the observations scatter around the regression line and relies on the assumption that the residuals are normally distributed. The estimates for the model parameters of a linear regression are obtained by searching for the best tting regression line. To do so, we search for the regression line that minimizes the sum of the squared residuals. This model tting method is called the least-squares method, abbreviated as LS. It has a very simple solution using matrix algebra (see e.g., Aitkin et al. 2009). Note that we can apply LS techniques independent of whether we use a Bayesian or frequentist framework to draw inference. In Bayesian statistics, Equation (10.1) is called the data model, because it describes mathematically the process that has (or, better, that we think has) produced the data. This nomenclature also helps to distinguish data models from models for parameters such as prior distributions. The least-squares estimates for the model parameters of a linear regression are obtained in R using the function lm. For illustration, we rst simulate a data set and then t a linear regression to these simulated data. The advantage of simulating data is that the following analyses can be reproduced without having to read data into R. set.seed(34) # set a seed for the random number generator n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # sample values of the covariate mu &lt;- b0 + b1 * x y &lt;- rnorm(n, mu, sd = sigma) # Save data in table dat &lt;- tibble(x = x, y = y) Then, we t a linear regression to the data to obtain the results of the three parameters of the linear regression, that is intercept, slope and residual standard deviation. mod &lt;- lm(y ~ x, data = dat) coef(mod) ## (Intercept) x ## 2.0049517 0.6880415 summary(mod)$sigma ## [1] 5.04918 The object mod produced by lm contains the estimates for the intercept, \\(\\beta_0\\) , and the slope, \\(\\beta_1\\). The residual standard deviation \\(\\sigma^2\\) is extracted using the function summary. We can show the result of the linear regression as a line in a scatter plot with the covariate (x) on the x-axis and the observations (y) on the y-axis (Fig. 10.1). Figure 10.1: Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The &lt;U+FB01&gt;tted values lie where the orange dotted lines touch the blue regression line. Conclusions drawn from a model depend on the model assumptions. When model assumptions are violated, estimates usually are biased and inappropriate conclusions can be drawn. We devote Chapter 11 to the assessment of model assumptions, given its importance. 10.1.3 Drawing Conclusions To answer the question about how strongly \\(y\\) is related to \\(x\\), or to predict \\(y\\) from \\(x\\), and because we usually draw inference in a Bayesian framework, we are interested in the joint posterior distribution of \\(\\boldsymbol{\\beta}\\) (vector that contains \\(\\beta_{0}\\) and \\(\\beta_{1}\\) ) and \\(\\sigma^2\\) , the residual variance. The function sim does this for us. To somewhat demystify the sim function we briey explain what sim does. The principle is to rst draw a random value from the marginal posterior distribution of \\(\\sigma^2\\), and then to draw random values from the conditional posterior distribution for \\(\\boldsymbol{\\beta}\\) (A. Gelman et al. 2014). The conditional posterior distribution of the parameter vector \\(\\boldsymbol{\\beta}\\), \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})\\) is the posterior distribution of \\(\\boldsymbol{\\beta}\\) given a specic value for \\(\\sigma^2\\) . This conditional distribution can be analytically derived. With at prior distributions, it is a uni- or multivariate normal distribution \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})=Norm(\\boldsymbol{\\hat{\\beta}},V_\\beta,\\sigma^2)\\) with: \\[\\begin{align} \\boldsymbol{\\hat{\\beta}=(\\boldsymbol{X^TX})^{-1}X^Ty} \\tag{10.4} \\end{align}\\] and \\(V_\\beta = (\\boldsymbol{X^T X})^{-1}\\) . For models with the normal error distribution, the LS estimates for \\(\\boldsymbol{\\beta}\\) (given by Eq. (10.4)) equal the maximum likelihood (ML) estimates ==(see Chapter 5)==. The marginal posterior distribution of \\(\\sigma^2\\) is independent of specic values of \\(\\boldsymbol{\\beta}\\). It is, for at prior distributions, an inverse chi-square distribution \\(p(\\sigma^2|\\boldsymbol{y,X})=Inv-\\chi^2(n-k,\\sigma^2)\\), where \\(\\sigma^2 = \\frac{1}{n-k}(\\boldsymbol{y}-\\boldsymbol{X,\\hat{\\beta}})^T(\\boldsymbol{y}-\\boldsymbol{X,\\hat{\\beta}})\\), and \\(k\\) is the number of parameters. The marginal posterior distribution of \\(\\boldsymbol{\\beta}\\) can be obtained by integrating the conditional posterior distribution \\(p(\\boldsymbol{\\beta}|\\sigma^2,\\boldsymbol{y,X})=Norm(\\boldsymbol{\\hat{\\beta}},V_\\beta\\sigma^2)\\) over the distribution of \\(\\sigma^2\\) . This results in a uni- or multivariate \\(t\\)-distribution. However, it is not necessary to do this analytically. Using the function sim from the package, we can draw samples from \\(p(\\sigma^2|\\boldsymbol{y,X})\\) and describe the marginal posterior distributions of \\(\\boldsymbol{\\beta}\\) using the simulated values. library(arm) nsim &lt;- 1000 bsim &lt;- sim(mod, n.sim = nsim) The function sim simulates (in our example) 1000 values from the joint posterior distribution of the three model parameters \\(\\beta_0\\) , \\(\\beta_1\\), and \\(\\sigma\\). These simulated values are shown in Figure 10.2. Figure 10.2: Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters. The posterior distributions describe the range of plausible parameter values given the data and the model. They express our uncertainty about the model parameters; they show what we know about the model parameters after having looked at the data and given the model is realistic. The 2.5% and 97.5% quantiles of the marginal posterior distributions can be used as 95% credible intervals of the model parameters. The function coef extracts the simulated values for the beta coefcients, returning a matrix with nsim rows and the number of columns corresponding to the number of parameters. In our example, the rst column contains the simulated values from the posterior distribution of the intercept and the second column contains values from the posterior distribution of the slope. The 2 in the second argument of the apply-function (see Chapter ??) indicates that the quantile function is applied columnwise. apply(X = coef(bsim), MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975)) %&gt;% round(2) ## (Intercept) x ## 2.5% -2.95 0.44 ## 97.5% 7.17 0.92 We also can calculate a credible interval of the estimated residual standard deviation, \\(\\hat{\\sigma}\\). quantile(bsim@sigma, probs = c(0.025, 0.975)) %&gt;% round(1) ## 2.5% 97.5% ## 4.2 6.3 Using Bayesian methods allows us to get a posterior probability for specic hypotheses, such as The slope parameter is larger than 1 or The slope parameter is larger than 0.5. These probabilities are the proportion of simulated values from the posterior distribution that are larger than 1 and 0.5, respectively. sum(coef(bsim)[,2] &gt; 1) / nsim ## [1] 0.008 sum(coef(bsim)[,2] &gt; 0.5) / nsim ## [1] 0.936 From this, there is very little evidence in the data that the slope is larger than 1, but we are quite condent that the slope is larger than 0.5 (assuming that our model is realistic). We often want to show the effect of \\(x\\) on \\(y\\) graphically, with information about the uncertainty of the parameter estimates included in the graph. To draw such effect plots, we use the simulated values from the posterior distribution of the model parameters. From the deterministic part of the model, we know the regression line \\(\\mu = \\beta_0 + \\beta_1 x_i\\). The simulation from the joint posterior distribution of \\(\\beta_0\\) and \\(\\beta_1\\) gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We can draw these regression lines in an x-y plot (scatter plot) to show the uncertainty in the regression line estimation (Fig. 10.3, left). Note, that in this case it is not advisable to use ggplot because we draw many lines in one plot, which makes ggplot rather slow. par(mar = c(4, 4, 0, 0)) plot(x, y, pch = 16, las = 1, xlab = &quot;Covariate (x)&quot;, ylab = &quot;Dependend variable (y)&quot;) for(i in 1:nsim) { abline(coef(bsim)[i,1], coef(bsim)[i,2], col = rgb(0, 0, 0, 0.05)) } Figure 10.3: Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line. A more convenient way to show uncertainty is to draw the 95% credible interval, CrI, of the regression line. To this end, we rst dene new x-values for which we would like to have the tted values (about 100 points across the range of x will produce smooth-looking lines when connected by line segments). We save these new x-values within the new tibble newdat. Then, we create a new model matrix that contains these new x-values (newmodmat) using the function model.matrix. We then calculate the 1000 tted values for each element of the new x (one value for each of the 1000 simulated regressions, Fig. 10.3), using matrix multiplication (%*%). We save these values in the matrix tmat. Finally, we extract the 2.5% and 97.5% quantiles for each x-value from tmat, and draw the lines for the lower and upper limits of the credible interval (Fig. 10.4). # Calculate 95% credible interval newdat &lt;- tibble(x = seq(10, 30, by = 0.1)) newmodmat &lt;- model.matrix( ~ x, data = newdat) fitmat &lt;- matrix(ncol = nsim, nrow = nrow(newdat)) for(i in 1:nsim) {fitmat[,i] &lt;- newmodmat %*% coef(bsim)[i,]} newdat$CrI_lo &lt;- apply(fitmat, 1, quantile, probs = 0.025) newdat$CrI_up &lt;- apply(fitmat, 1, quantile, probs = 0.975) # Make plot regplot &lt;- ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_smooth(method = lm, se = FALSE) + geom_line(data = newdat, aes(x = x, y = CrI_lo), lty = 3) + geom_line(data = newdat, aes(x = x, y = CrI_up), lty = 3) + labs(x = &quot;Covariate (x)&quot;, y = &quot;Dependend variable (y)&quot;) regplot Figure 10.4: Regression with 95% credible interval of the posterior distribution of the &lt;U+FB01&gt;tted values. The interpretation of the 95% credible interval is straightforward: We are 95% sure that the true regression line is within the credible interval. As always, this interpretation is only true if the the model is correct. The larger the sample size, the narrower the interval, because each additional data point increases information about the true regression line. The credible interval measures uncertainty of the regression line, but it does not describe how new observations would scatter around the regression line. If we want to describe where future observations will be, we have to report the posterior predictive distribution. We can get a sample of random draws from the posterior predictive distribution \\(\\hat{y}|\\boldsymbol{\\beta},\\sigma^2,\\boldsymbol{X}\\sim Norm( \\boldsymbol{X \\beta, \\sigma^2})\\) using the simulated joint posterior distributions of the model parameters, thus taking the uncertainty of the parameter estimates into account. We draw a new \\(\\hat{y}\\)-value from \\(Norm( \\boldsymbol{X \\beta, \\sigma^2})\\) for each simulated set of model parameters. Then, we can visualize the 2.5% and 97.5% quantiles of this distribution for each new x-value. # increase number of simulation to procude smooth lines of the posterior # predictive distribution set.seed(34) nsim &lt;- 50000 bsim &lt;- sim(mod, n.sim=nsim) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- newmodmat%*%coef(bsim)[i,] # prepare matrix for simulated new data newy &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) # for each simulated tted value, simulate one new y-value for(i in 1:nsim) { newy[,i] &lt;- rnorm(nrow(newdat), mean = fitmat[,i], sd = bsim@sigma[i]) } # Calculate 2.5% and 97.5% quantiles newdat$pred_lo &lt;- apply(newy, 1, quantile, probs = 0.025) newdat$pred_up &lt;- apply(newy, 1, quantile, probs = 0.975) # Add the posterior predictive distribution to plot regplot + geom_line(data = newdat, aes(x = x, y = pred_lo), lty = 2) + geom_line(data = newdat, aes(x = x, y = pred_up), lty = 2) Figure 10.5: Regression line with 95% credible interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines. Future observations are expected to be within the interval dened by the broken lines in Fig. 10.5 with a probability of 0.95. Increasing sample size will not necessarily give a narrower predictive distribution because the predictive distribution also depends on the residual variance \\(\\sigma^2\\). The way we produced Fig. 10.5 is somewhat tedious compared to how easy we could have obtained the same gure using frequentist methods: predict(mod, newdata = newdat, interval = \"prediction\") would have produced the y-values for the lower and upper lines in Fig. 10.5 in one R-code line. However, once we have a simulated sample of the posterior predictive distribution, we have much more information than is contained in the frequentist prediction interval. For example, we could give an estimate for the proportion of observations greater than 20, given \\(x = 25\\). sum(newy[newdat$x == 25, ] &gt; 20) / nsim ## [1] 0.44504 Thus, we expect 44% of future observations with \\(x = 25\\) to be higher than 20. We can extract similar information for any relevant threshold value. Another reason to learn the more complicated R code we presented here, compared to the frequentist methods, is that, for more complicated models such as mixed models, the frequentist methods to obtain condence intervals of tted values are much more complicated than the Bayesian method just presented. The latter can be used with only slight adaptations for mixed models and also for generalized linear mixed models. 10.1.4 Frequentist Results The solution for \\(\\boldsymbol{\\beta}\\) is the Equation (10.3). Most statistical software, including R, return an estimated frequentist standard error for each \\(\\beta_k\\). We extract these standard errors together with the estimates for the model parameters using the summary function. summary(mod) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5777 -3.6280 -0.0532 3.9873 12.1374 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0050 2.5349 0.791 0.433 ## x 0.6880 0.1186 5.800 0.000000507 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.049 on 48 degrees of freedom ## Multiple R-squared: 0.412, Adjusted R-squared: 0.3998 ## F-statistic: 33.63 on 1 and 48 DF, p-value: 0.0000005067 The summary output rst gives a rough summary of the residual distribution. However, we will do more rigorous residual analyses in Chapter 11. The estimates of the model coefcients follow. The column Estimate contains the estimates for the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\) . The column Std. Error contains the estimated (frequentist) standard errors of the estimates. The last two columns contain the t-value and the p-value of the classical t-test for the null hypothesis that the coefcient equals zero. The last part of the summary output gives the parameter \\(\\sigma\\) of the model, named residual standard error and the residual degrees of freedom. We try to avoid the name residual standard error and use sigma instead, because \\(\\sigma\\) is not a measurement of uncertainty of a parameter estimate like the standard errors of the model coefcients are. \\(\\sigma\\) is a model parameter that describes how the observations scatter around the tted values, that is, it is a standard deviation. It is independent of sample size, whereas the standard errors of the estimates for the model parameters will decrease with increasing sample size. Such a standard error of the estimate of \\(\\sigma\\), however, is not given in the summary output. Note that, by using Bayesian methods, we could easily obtain the standard error of the estimated \\(\\sigma\\) by calculating the standard deviation of the posterior distribution of \\(\\sigma\\). The \\(R^2\\) and the adjusted \\(R^2\\) are explained in Section == Posterior Predictive Model Checking and Proportion of Explained Variance==. 10.2 Regression Variants: ANOVA, ANCOVA, and Multiple Regression 10.2.1 One-Way ANOVA The aim of analysis of variance (ANOVA) is to compare means of an outcome variable y between different groups (categorical variables). To do so in the frequentists framework, variances between and within the groups are compared (hence the name analysis of variance). If the variance between the group means is larger than expected by chance , we reject the null hypothesis of no differences between the groups. When doing an ANOVA in a Bayesian way, inference is based on the posterior distributions of the group means and the differences between the group means. One-way ANOVA means that we only have one explanatory variable (a factor). We illustrate the one-way ANOVA based on an example of simulated data (Fig. 10.6). We have measured weights of 30 virtual individuals for each of 3 groups. Possible research questions could be: How big are the differences between the group means? Are individuals from group 2 heavier than the ones from group 1? Which group mean is higher than 7.5 g? # settings for the simulation set.seed(626436) b0 &lt;- 12 # mean of group 1 (reference group) sigma &lt;- 2 # residual standard deviation b1 &lt;- 3 # difference between group 1 and group 2 b2 &lt;- -5 # difference between group 1 and group 3 n &lt;- 90 # sample size # generate data group &lt;- factor(rep(c(&quot;group 1&quot;,&quot;group 2&quot;, &quot;group 3&quot;), each=30)) simresid &lt;- rnorm(n, mean=0, sd=sigma) # simulate residuals y &lt;- b0 + as.numeric(group==&quot;group 2&quot;) * b1 + as.numeric(group==&quot;group 3&quot;) * b2 + simresid dat &lt;- tibble(y, group) # make figure dat %&gt;% ggplot(aes(x = group, y = y)) + geom_boxplot(fill = &quot;orange&quot;) + labs(y = &quot;Weight (g)&quot;, x = &quot;&quot;) + ylim(0, NA) Figure 10.6: Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box. An ANOVA is a linear regression with a categorical predictor variable instead of a continuous one. The categorical predictor variable with \\(k\\) levels is (as a default in R) transformed to \\(k-1\\) indicator variables. An indicator variable is a binary variable containing 0 and 1 where 1 indicates a specic level (a category of a nominal variable). Often, one indicator variable is constructed for every level except for the reference level. In our example, the categorical variable is group (\\(g\\)) with the three levels group 1, group 2, and group 3 (\\(k = 3\\)). Group 1 is taken as the reference level, and for each of the other two groups an indicator variable is constructed, \\(I(g_i = 2)\\) and \\(I(g_i = 3)\\). We can write the model as a formula: \\[\\begin{align} \\mu_i &amp;=\\beta_0 + \\beta_1 I(g_i=2) + \\beta_1 I(g_i=3) \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{10.5} \\end{align}\\] where \\(yi\\) is the \\(i\\)-th observation (weight measurement for individual i in our example), and \\(\\beta_{0,1,2}\\) are the model coefcients. The residual variance is \\(\\sigma^2\\). The model coefcients \\(\\beta_{0,1,2}\\) constitute the deterministic part of the model. From the model formula it follows that the group means, \\(m_g\\), are: \\[\\begin{align} m_1 &amp;=\\beta_0 \\\\ m_2 &amp;=\\beta_0 + \\beta_1 \\\\ m_3 &amp;=\\beta_0 + \\beta_2 \\\\ \\tag{10.6} \\end{align}\\] There are other possibilities to describe three group means with three parameters, for example: \\[\\begin{align} m_1 &amp;=\\beta_1 \\\\ m_2 &amp;=\\beta_2 \\\\ m_3 &amp;=\\beta_3 \\\\ \\tag{10.7} \\end{align}\\] In this case, the model formula would be: \\[\\begin{align} \\mu_i &amp;= \\beta_1 I(g_i=1) + \\beta_2 I(g_i=2) + \\beta_3 I(g_i=3) \\\\ y_i &amp;\\sim Norm(\\mu_i, \\sigma^2) \\tag{10.8} \\end{align}\\] The way the group means are described is called the parameterization of the model. Different statistical softwares use different parameterizations. The parameterization used by R by default is the one shown in Equation (10.5). R automatically takes the rst level as the reference (the rst level is the rst one alphabetically unless the user denes a different order for the levels). The mean of the rst group (i.e., of the rst factor level) is the intercept, \\(b_0\\) , of the model. The mean of another factor level is obtained by adding, to the intercept, the estimate of the corresponding parameter (which is the difference from the reference group mean). R calls this parameterization treatment contrasts. The parameterization of the model is dened by the model matrix. In the case of a one-way ANOVA, there are as many columns in the model matrix as there are factor levels (i.e., groups); thus there are k factor levels and k model coefcients. Recall from Equation (10.3) that for each observation, the entry in the \\(j\\)-th column of the model matrix is multiplied by the \\(j\\)-th element of the model coefcients and the \\(k\\) products are summed to obtain the tted values. For a data set with \\(n = 5\\) observations of which the rst two are from group 1, the third from group 2, and the last two from group 3, the model matrix used for the parameterization described in Equation (10.6) is \\[\\begin{align} \\boldsymbol{X}= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{align}\\] If parameterization of Equation (10.7) were used, \\[\\begin{align} \\boldsymbol{X}= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{align}\\] Other possibilities of model parameterizations, particularly for ordered factors, are introduced in Section 10.2.8. To obtain the parameter estimates for model parameterized according to Equation (10.6) we t the model in R: # fit the model mod &lt;- lm(y~group) # parameter estimates mod ## ## Call: ## lm(formula = y ~ group) ## ## Coefficients: ## (Intercept) groupgroup 2 groupgroup 3 ## 12.367 2.215 -5.430 summary(mod)$sigma ## [1] 1.684949 The Intercept is \\(\\beta_0\\). The other coefcients are named with the factor name (group) and the factor level (either group 2 or group 3). These are \\(\\beta_1\\) and \\(\\beta_2\\) , respectively. Before drawing conclusions from an R output we need to examine whether the model assumptions are met, that is, we need to do a residual analysis as described in Chapter 11. Different questions can be answered using the above ANOVA: What are the group means? What is the difference in the means between group 1 and group 2? What is the difference between the means of the heaviest and lightest group? In a Bayesian framework we can directly assess how strongly the data support the hypothesis that the mean of the group 2 is larger than the mean of group 1. We rst simulate from the posterior distribution of the model parameters. library(arm) nsim &lt;- 1000 bsim &lt;- sim(mod, n.sim=nsim) Then we obtain the posterior distributions for the group means according to the parameterization of the model formula (Equation (10.6)). m.g1 &lt;- coef(bsim)[,1] m.g2 &lt;- coef(bsim)[,1] + coef(bsim)[,2] m.g3 &lt;- coef(bsim)[,1] + coef(bsim)[,3] The histograms of the simulated values from the posterior distributions of the three means are given in Fig. 10.7. The three means are well separated and, based on our data, we are condent that the group means differ. From these simulated posterior distributions we obtain the means and use the 2.5% and 97.5% quantiles as limits of the 95% credible intervals (Fig. 10.7, right). # save simulated values from posterior distribution in tibble post &lt;- tibble(`group 1` = m.g1, `group 2` = m.g2, `group 3` = m.g3) %&gt;% gather(&quot;groups&quot;, &quot;Group means&quot;) # histograms per group leftplot &lt;- ggplot(post, aes(x = `Group means`, fill = groups)) + geom_histogram(aes(y=..density..), binwidth = 0.5, col = &quot;black&quot;) + labs(y = &quot;Density&quot;) + theme(legend.position = &quot;top&quot;, legend.title = element_blank()) # plot mean and 95%-CrI rightplot &lt;- post %&gt;% group_by(groups) %&gt;% dplyr::summarise( mean = mean(`Group means`), CrI_lo = quantile(`Group means`, probs = 0.025), CrI_up = quantile(`Group means`, probs = 0.975)) %&gt;% ggplot(aes(x = groups, y = mean)) + geom_point() + geom_errorbar(aes(ymin = CrI_lo, ymax = CrI_up), width = 0.1) + ylim(0, NA) + labs(y = &quot;Weight (g)&quot;, x =&quot;&quot;) multiplot(leftplot, rightplot, cols = 2) Figure 10.7: Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% credible intervals obtained from the simulated distributions (right). To obtain the posterior distribution of the difference between the means of group 1 and group 2, we simply calculate this difference for each draw from the joint posterior distribution of the group means. d.g1.2 &lt;- m.g1 - m.g2 mean(d.g1.2) ## [1] -2.209551 quantile(d.g1.2, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## -3.128721 -1.342693 The estimated difference is -2.2095511. We are 95% sure that the difference between the means of group 1 and 2 is between -3.1287208 and -1.3426929. How strongly do the data support the hypothesis that the mean of group 2 is larger than the mean of group 1? To answer this question we calculate the proportion of the draws from the joint posterior distribution for which the mean of group 2 is larger than the mean of group 1. sum(m.g2 &gt; m.g1) / nsim ## [1] 1 This means that in all of the 1000 simulations from the joint posterior distribution, the mean of group 2 was larger than the mean of group 1. Therefore, there is a very high probability (i.e., it is close to 1; because probabilities are never exactly 1, we write &gt;0.999) that the mean of group 2 is larger than the mean of group 1. 10.2.2 Frequentist Results from a One-Way ANOVA 10.2.3 Two-Way ANOVA 10.2.4 Frequentist Results from a Two-Way ANOVA 10.2.5 Multiple Comparisons and Post Hoc Tests 10.2.6 Analysis of Covariance 10.2.7 Multiple Regression and Collinearity 10.2.8 Ordered Factors and Contrasts 10.2.9 Quadratic and Higher Polynomial Terms 10.3 Pendenzen Manchmal schreiben wir \\(\\sigma\\) und manchmal \\(\\sigma^2\\). Bin mir nicht sicher, ob die Unterscheidung jedes Mal richtig ist. Das sollten wir noch kontrollieren und irgendwo im Text auch auf den Unterschied hinweisen. "],["residualanalysis.html", "11 Assessing Model Assumptions 11.1 Model Assumptions 11.2 Independent and Identically Distributed 11.3 The QQ-Plot 11.4 Temporal Autocorrelation 11.5 Spatial Autocorrelation 11.6 Heteroscedasticity", " 11 Assessing Model Assumptions 11.1 Model Assumptions Every statistical model makes assumptions. We try to build models that reect the data-generating process as realistically as possible. However, a model never is the truth. Yet, all inferences drawn from a model, such as estimates of effect size or derived quantities with credible intervals, are based on the assumption that the model is true. However, if a model captures the datagenerating process poorly, for example, because it misses important structures (predictors, interactions, polynomials), inferences drawn from the model are probably biased and results become unreliable. In a (hypothetical) model that captures all important structures of the data generating process, the stochastic part, the difference between the observation and the tted value (the residuals), should only show random variation. Analyzing residuals is a very important part of the data analysis process. Residual analysis can be very exciting, because the residuals show what remains unexplained by the present model. Residuals can sometimes show surprising patterns and, thereby, provide deeper insight into the system. However, at this step of the analysis it is important not to forget the original research questions that motivated the study. Because these questions have been asked without knowledge of the data, they protect against data dredging. Of course, residual analysis may raise interesting new questions. Nonetheless, these new questions have emerged from patterns in the data, which might just be random, not systematic, patterns. The search for a model with good t should be guided by thinking about the process that generated the data, not by trial and error (i.e., do not try all possible variable combinations until the residuals look good; that is data dredging). All changes done to the model should be scientically justied. Usually, model complexity increases, rather than decreases, during the analysis. 11.2 Independent and Identically Distributed Usually, we model an outcome variable as independent and identically distributed (iid) given the model parameters. This means that all observations with the same predictor values behave like independent random numbers from the identical distribution. As a consequence, residuals should look iid. Independent means that: The residuals do not correlate with other variables (those that are included in the model as well as any other variable not included in the model). The residuals are not grouped (i.e., the means of any set of residuals should all be equal). The residuals are not autocorrelated (i.e., no temporal or spatial autocorrelation exist; Sections 11.4 and 11.5). Identically distributed means that: All residuals come from the same distribution. In the case of a linear model with normal error distribution (Chapter 10) the residuals are assumed to come from the same normal distribution. Particularly: The residual variance is homogeneous (homoscedasticity), that is, it does not depend on any predictor variable, and it does not change with the tted value. The mean of the residuals is zero over the whole range of predictor values. When numeric predictors (covariates) are present, this implies that the relationship between x and y can be adequately described by a straight line. Residual analysis is mainly done graphically. R makes it very easy to plot residuals to look at the different aspects just listed. As a rst example, we use the coal tit example from Chapter 10: Hier fehlt noch ein Teil aus dem BUCH. 11.3 The QQ-Plot xxx 11.4 Temporal Autocorrelation 11.5 Spatial Autocorrelation 11.6 Heteroscedasticity "],["PART-III.html", "12 Introduction to PART III 12.1 Model notations", " 12 Introduction to PART III This part is a collection of more complicated ecological models to analyse data that may not be analysed with the traditional linear models that we covered in PART I of this book. 12.1 Model notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Betancourt et al. (2016) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. "],["zeroinflated-poisson-lmm.html", "13 Zero-inflated Poisson Mixed Model 13.1 Introduction 13.2 Example data 13.3 Model", " 13 Zero-inflated Poisson Mixed Model 13.1 Introduction Usually we describe the outcome variable with a single distribution, such as the normal distribution in the case of linear (mixed) models, and Poisson or binomial distributions in the case of generalized linear (mixed) models. In life sciences, however, quite often the data are actually generated by more than one process. In such cases the distribution of the data could be the result of two or more different distributions. If we do not account for these different processes our inferences are likely to be biased. In this chapter, we introduce a mixture model that explicitly include two processes that generated the data. The zero-inflated Poisson model is a mixture of a binomial and a Poisson distribution. We belief that two (or more)-level models are very useful tools in life sciences because they can help uncover the different processes that generate the data we observe. 13.2 Example data We used the blackstork data from the blmeco-package. They contain the breeding success of Black-stork in Latvia. The data was collected and kindly provided by Maris Stradz. The data contains the number of nestlings of more then 300 Black-stork nests in different years. Counting animals or plants is a typical example of data that contain a lot of zero counts. For example, the number of nestlings produced by a breeding pair is often zero because the whole nest was depredated or because a catastrophic event occurred such as a flood. However, when the nest succeeds, the number of nestlings varies among the successful nests depending on how many eggs the female has laid, how much food the parents could bring to the nest, or other factors that affect the survival of a nestling in an intact nest. Thus the factors that determine how many zero counts there are in the data differ from the factors that determine how many nestlings there are, if a nest survives. Count data that are produced by two different processesone produces the zero counts and the other the variance in the count for the ones that were not zero in the first processare called zero-inflated data. Histograms of zero-inflated data look bimodal, with one peak at zero (Figure 13.1). Figure 13.1: Histogram of the number of nestlings counted in black stork nests Ciconia nigra in Latvia (n = 1130 observations of 279 nests). 13.3 Model The Poisson distribution does not fit well to such data, because the data contain more zero counts than expected under the Poisson distribution. Mullahy (1986) and Lambert (1992) formulated two different types of models that combine the two processes in one model and therefore account for the zero excess in the data and allow the analysis of the two processes separately. The hurdle model (Mullahy, 1986) combines a left-truncated count data model (Poisson or negative binomial distribution that only describes the distribution of data larger than zero) with a zero-hurdle model that describes the distribution of the data that are either zero or nonzero. In other words, the hurdle model divides the data into two data subsets, the zero counts and the nonzero counts, and fits two separate models to each subset of the data. To account for this division of the data, the two models assume left truncation (all measurements below 1 are missing in the data) and right censoring (all measurements larger than 1 have the value 1), respectively, in their error distributions. A hurdle model can be fitted in R using the function hurdle from the package pscl (Jackman, 2008). See the tutorial by Zeileis et al. (2008) for an introduction. In contrast to the hurdle model, the zero-inflated models (Mullahy, 1986; Lambert, 1992) combine a Bernoulli model (zero vs. nonzero) with a conditional Poisson model; conditional on the Bernoulli process being nonzero. Thus this model allows for a mixture of zero counts: some zero counts are zero because the outcome of the Bernoulli process was zero (these zero counts are sometimes called structural zero values), and others are zero because their outcome from the Poisson process was zero. The function `zeroinfl from the package pscl fits zero-inflated models (Zeileis et al., 2008). The zero-inflated model may seem to reflect the true process that has generated the data closer than the hurdle model. However, sometimes the fit of zero-inflated models is impeded because of high correlation of the model parameters between the zero model and the count model. In such cases, a hurdle model may cause less troubles. Both functions (hurdle and zeroinfl) from the package pscl do not allow the inclusion of random factors. The functions MCMCglmm from the package MCMCglmm (Hadfield, 2010) and glmmadmb from the package glmmADMB (http://glmmadmb.r-forge.r-project.org/) provide the possibility to account for zero-inflation with a GLMM. However, these functions are not very flexible in the types of zero-inflated models they can fit; for example, glmmadmb only includes a constant proportion of zero values. A zero-inflation model using BUGS is described in Ke ry and Schaub (2012). Here we use Stan to fit a zero- inflated model. Once we understand the basic model code, it is easy to add predictors and/or random effects to both the zero and the count model. The example data contain numbers of nestlings in black stork Ciconia nigra nests in Latvia collected by Maris Stradz and collaborators at 279 nests be- tween 1979 and 2010. Black storks build solid and large aeries on branches of large trees. The same aerie is used for up to 17 years until it collapses. The black stork population in Latvia has drastically declined over the last decades. Here, we use the nestling data as presented in Figure 14-2 to describe whether the number of black stork nestlings produced in Latvia decreased over time. We use a zero-inflated Poisson model to separately estimate temporal trends for nest survival and the number of nestlings in successful nests. Since the same nests have been measured repeatedly over 1 to 17 years, we add nest ID as a random factor to both models, the Bernoulli and the Poisson model. After the first model fit, we saw that the between-nest variance in the number of nest- lings for the successful nests was close to zero. Therefore, we decide to delete the random effect from the Poisson model. Here is our final model: zit is a latent (unobserved) variable that takes the values 0 or 1 for each nest i during year t. It indicates a structural zero, that is, if zit 14 1 the number of nestlings yit always is zero, because the expected value in the Poisson model lit(1 zit) becomes zero. If zit 14 0, the expected value in the Poisson model becomes lit. To fit this model in Stan, we first write the Stan model code and save it in a separated text-file with name zeroinfl.stan. Here is a handy package: https://cran.r-project.org/web/packages/GLMMadaptive/vignettes/ZeroInflated_and_TwoPart_Models.html "],["dailynestsurv.html", "14 Daily nest survival 14.1 Background 14.2 Models for estimating daily nest survival 14.3 Known fate model 14.4 The Stan model 14.5 Prepare data and run Stan 14.6 Check convergence 14.7 Look at results 14.8 Known fate model for irregular nest controls Further reading", " 14 Daily nest survival 14.1 Background Analyses of nest survival is important for understanding the mechanisms of population dynamics. The life-span of a nest could be used as a measure of nest survival. However, this measure very often is biased towards nests that survived longer because these nests are detected by the ornithologists with higher probability (Mayfield 1975). In order not to overestimate nest survival, daily nest survival conditional on survival to the previous day can be estimated. 14.2 Models for estimating daily nest survival What model is best used depends on the type of data available. Data may look: Regular (e.g. daily) nest controls, all nests monitored from their first egg onward Regular nest controls, nests found during the course of the study at different stages and nestling ages Irregular nest controls, all nests monitored from their first egg onward Irregular nest controls, nests found during the course of the study at different stages and nestling ages Table 14.1: Models useful for estimating daily nest survival. Data numbers correspond to the descriptions above. Model Data Software, R-code Binomial or Bernoulli model 1, (3) glm, glmer, Cox proportional hazard model 1,2,3,4 brm, soon: stan_cox Known fate model 1, 2 Stan code below Known fate model 3, 4 Stan code below 14.3 Known fate model A natural model that allows estimating daily nest survival is the known-fate survival model. It is a Markov model that models the state of a nest \\(i\\) at day \\(t\\) (whether it is alive, \\(y_{it}=1\\) or not \\(y_{it}=0\\)) as a Bernoulli variable dependent on the state of the nest the day before. \\[ y_{it} \\sim Bernoulli(y_{it-1}S_{it})\\] The daily nest survival \\(S_{it}\\) can be linearly related to predictor variables that are measured on the nest or on the day level. \\[logit(S_{it}) = \\textbf{X} \\beta\\] It is also possible to add random effects if needed. 14.4 The Stan model The following Stan model code is saved as daily_nest_survival.stan. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; last[Nnests]; // day of last observation (alive or dead) int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last int&lt;lower=0&gt; y[Nnests, maxage]; // indicator of alive nests real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date } parameters { vector[3] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(last[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t]); } } // priors b[1]~normal(0,5); b[2]~normal(0,3); b[3]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):last[i]){ y[i,t]~bernoulli(y[i,t-1]*S[i,t-1]); } } } 14.5 Prepare data and run Stan Data is from (???). load(&quot;RData/nest_surv_data.rda&quot;) str(datax) ## List of 7 ## $ y : int [1:156, 1:31] 1 NA 1 NA 1 NA NA 1 1 1 ... ## $ Nnests: int 156 ## $ last : int [1:156] 26 30 31 27 31 30 31 31 31 31 ... ## $ first : int [1:156] 1 14 1 3 1 24 18 1 1 1 ... ## $ cover : num [1:156] -0.943 -0.215 0.149 0.149 -0.215 ... ## $ age : num [1:31] -1.65 -1.54 -1.43 -1.32 -1.21 ... ## $ maxage: int 31 datax$y[is.na(datax$y)] &lt;- 0 # Stan does not allow for NA&#39;s in the outcome # Run STAN library(rstan) mod &lt;- stan(file = &quot;stanmodels/daily_nest_survival.stan&quot;, data=datax, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) 14.6 Check convergence We love exploring the performance of the Markov chains by using the function launch_shinystan from the package shinystan. 14.7 Look at results It looks like cover does not affect daily nest survival, but daily nest survival decreases with the age of the nestlings. #launch_shinystan(mod) print(mod) ## Inference for Stan model: daily_nest_survival. ## 5 chains, each with iter=2500; warmup=1250; thin=1; ## post-warmup draws per chain=1250, total post-warmup draws=6250. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b[1] 4.04 0.00 0.15 3.76 3.94 4.03 4.13 4.34 4020 1 ## b[2] 0.00 0.00 0.13 -0.24 -0.09 -0.01 0.08 0.25 4710 1 ## b[3] -0.69 0.00 0.16 -1.01 -0.80 -0.69 -0.59 -0.40 4326 1 ## lp__ -298.94 0.02 1.26 -302.30 -299.51 -298.60 -298.05 -297.53 2781 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Feb 10 17:17:35 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # effect plot bsim &lt;- as.data.frame(mod) nsim &lt;- nrow(bsim) newdat &lt;- data.frame(age=seq(1, datax$maxage, length=100)) newdat$age.z &lt;- (newdat$age-mean(1:datax$maxage))/sd((1:datax$maxage)) Xmat &lt;- model.matrix(~age.z, data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- plogis(Xmat%*%as.numeric(bsim[i,c(1,3)])) newdat$fit &lt;- apply(fitmat, 1, median) newdat$lwr &lt;- apply(fitmat, 1, quantile, prob=0.025) newdat$upr &lt;- apply(fitmat, 1, quantile, prob=0.975) plot(newdat$age, newdat$fit, ylim=c(0.8,1), type=&quot;l&quot;, las=1, ylab=&quot;Daily nest survival&quot;, xlab=&quot;Age [d]&quot;) lines(newdat$age, newdat$lwr, lty=3) lines(newdat$age, newdat$upr, lty=3) Figure 14.1: Estimated daily nest survival probability in relation to nest age. Dotted lines are 95% uncertainty intervals of the regression line. 14.8 Known fate model for irregular nest controls When nest are controlled only irregularly, it may happen that a nest is found predated or dead after a longer break in controlling. In such cases, we know that the nest was predated or it died due to other causes some when between the last control when the nest was still alive and when it was found dead. In such cases, we need to tell the model that the nest could have died any time during the interval when we were not controlling. To do so, we create a variable that indicates the time (e.g. day since first egg) when the nest was last seen alive (lastlive). A second variable indicates the time of the last check which is either the equal to lastlive when the nest survived until the last check, or it is larger than lastlive when the nest failure has been recorded. A last variable, gap, measures the time interval in which the nest failure occurred. A gap of zero means that the nest was still alive at the last control, a gapof 1 means that the nest failure occurred during the first day after lastlive, a gap of 2 means that the nest failure either occurred at the first or second day after lastlive. # time when nest was last observed alive lastlive &lt;- apply(datax$y, 1, function(x) max(c(1:length(x))[x==1])) # time when nest was last checked (alive or dead) lastcheck &lt;- lastlive+1 # here, we turn the above data into a format that can be used for # irregular nest controls. WOULD BE NICE TO HAVE A REAL DATA EXAMPLE! # when nest was observed alive at the last check, then lastcheck equals lastlive lastcheck[lastlive==datax$last] &lt;- datax$last[lastlive==datax$last] datax1 &lt;- list(Nnests=datax$Nnests, lastlive = lastlive, lastcheck= lastcheck, first=datax$first, cover=datax$cover, age=datax$age, maxage=datax$maxage) # time between last seen alive and first seen dead (= lastcheck) datax1$gap &lt;- datax1$lastcheck-datax1$lastlive In the Stan model code, we specify the likelihood for each gap separately. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; lastlive[Nnests]; // day of last observation (alive) int&lt;lower=0&gt; lastcheck[Nnests]; // day of observed death or, if alive, last day of study int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date int&lt;lower=0&gt; gap[Nnests]; // obsdead - lastlive } parameters { vector[3] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(lastcheck[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t]); } } // priors b[1]~normal(0,1.5); b[2]~normal(0,3); b[3]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):lastlive[i]){ 1~bernoulli(S[i,t-1]); } if(gap[i]==1){ target += log(1-S[i,lastlive[i]]); // } if(gap[i]==2){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1])); // } if(gap[i]==3){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1]) + prod(S[i,lastlive[i]:(lastlive[i]+1)])*(1-S[i,lastlive[i]+2])); // } if(gap[i]==4){ target += log((1-S[i,lastlive[i]]) + S[i,lastlive[i]]*(1-S[i,lastlive[i]+1]) + prod(S[i,lastlive[i]:(lastlive[i]+1)])*(1-S[i,lastlive[i]+2]) + prod(S[i,lastlive[i]:(lastlive[i]+2)])*(1-S[i,lastlive[i]+3])); // } } } # Run STAN mod1 &lt;- stan(file = &quot;stanmodels/daily_nest_survival_irreg.stan&quot;, data=datax1, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) Further reading Helpful links: https://deepai.org/publication/bayesian-survival-analysis-using-the-rstanarm-r-package (Brilleman et al. 2020) https://www.hammerlab.org/2017/06/26/introducing-survivalstan/ "],["cjs-with-mix.html", "15 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 15.1 Introduction 15.2 Data description 15.3 Model description 15.4 The Stan code 15.5 Call Stan from R, check convergence and look at results", " 15 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 15.1 Introduction In some species the identification of the sex is not possible for all individuals without sampling DNA. For example, morphological dimorphism is absent or so weak that parts of the individuals cannot be assigned to one of the sexes. Particularly in ornithological long-term capture recapture data sets that typically are obtained by voluntary bird ringers who do normaly not have the possibilities to analyse DNA, often the sex identification is missing in parts of the individuals. For estimating survival, it would nevertheless be valuable to include data of all individuals, use the information on sex-specific effects on survival wherever possible but account for the fact that of parts of the individuals the sex is not known. We here explain how a Cormack-Jolly-Seber model can be integrated with a mixture model in oder to allow for a combined analyses of individuals with and without sex identified. An introduction to the Cormack-Jolly-Seber model we gave in Chapter 14.5 of the book Korner-Nievergelt et al. (2015). We here expand this model by a mixture structure that allows including individuals with a missing categorical predictor variable, such as sex. 15.2 Data description ## simulate data # true parameter values theta &lt;- 0.6 # proportion of males nocc &lt;- 15 # number of years in the data set b0 &lt;- matrix(NA, ncol=nocc-1, nrow=2) b0[1,] &lt;- rbeta((nocc-1), 3, 4) # capture probability of males b0[2,] &lt;- rbeta((nocc-1), 2, 4) # capture probability of females a0 &lt;- matrix(NA, ncol=2, nrow=2) a1 &lt;- matrix(NA, ncol=2, nrow=2) a0[1,1]&lt;- qlogis(0.7) # average annual survival for adult males a0[1,2]&lt;- qlogis(0.3) # average annual survival for juveniles a0[2,1] &lt;- qlogis(0.55) # average annual survival for adult females a0[2,2] &lt;- a0[1,2] a1[1,1] &lt;- 0 a1[1,2] &lt;- -0.5 a1[2,1] &lt;- -0.8 a1[2,2] &lt;- a1[1,2] nindi &lt;- 1000 # number of individuals with identified sex nindni &lt;- 1500 # number of individuals with non-identified sex nind &lt;- nindi + nindni # total number of individuals y &lt;- matrix(ncol=nocc, nrow=nind) z &lt;- matrix(ncol=nocc, nrow=nind) first &lt;- sample(1:(nocc-1), nind, replace=TRUE) sex &lt;- sample(c(1,2), nind, prob=c(theta, 1-theta), replace=TRUE) juvfirst &lt;- sample(c(0,1), nind, prob=c(0.5, 0.5), replace=TRUE) juv &lt;- matrix(0, nrow=nind, ncol=nocc) for(i in 1:nind) juv[i,first[i]] &lt;- juv[i] x &lt;- runif(nocc-1, -2, 2) # a time dependent covariate covariate p &lt;- b0 # recapture probability phi &lt;- array(NA, dim=c(2, 2, nocc-1)) # for ad males phi[1,1,] &lt;- plogis(a0[1,1]+a1[1,1]*x) # for ad females phi[2,1,] &lt;- plogis(a0[2,1]+a1[2,1]*x) # for juvs phi[1,2,] &lt;- phi[2,2,] &lt;- plogis(a0[2,2]+a1[2,2]*x) for(i in 1:nind){ z[i,first[i]] &lt;- 1 y[i, first[i]] &lt;- 1 for(t in (first[i]+1):nocc){ z[i, t] &lt;- rbinom(1, size=1, prob=z[i,t-1]*phi[sex[i],juv[i,t-1]+1, t-1]) y[i, t] &lt;- rbinom(1, size=1, prob=z[i,t]*p[sex[i],t-1]) } } y[is.na(y)] &lt;- 0 The mark-recapture data set consists of capture histories of 2500 individuals over 15 time periods. For each time period \\(t\\) and individual \\(i\\) the capture history matrix \\(y\\) contains \\(y_{it}=1\\) if the individual \\(i\\) is captured during time period \\(t\\), or \\(y_{it}=0\\) if the individual \\(i\\) is not captured during time period \\(t\\). The marking time period varies between individuals from 1 to 14. At the marking time period, the age of the individuals was classified either as juvenile or as adult. Juveniles turn into adults after one time period, thus age is known for all individuals during all time periods after marking. For 1000 individuals of the 2500 individuals, the sex is identified, whereas for 1500 individuals, the sex is unknown. The example data contain one covariate \\(x\\) that takes on one value for each time period. # bundle the data for Stan i &lt;- 1:nindi ni &lt;- (nindi+1):nind datax &lt;- list(yi=y[i,], nindi=nindi, sex=sex[i], nocc=nocc, yni=y[ni,], nindni=nindni, firsti=first[i], firstni=first[ni], juvi=juv[i,]+1, juvni=juv[ni,]+1, year=1:nocc, x=x) 15.3 Model description The observations \\(y_{it}\\), an indicator of whether individual i was recaptured during time period \\(t\\) is modelled conditional on the latent true state of the individual birds \\(z_{it}\\) (0 = dead or permanently emigrated, 1 = alive and at the study site) as a Bernoulli variable. The probability \\(P(y_{it} = 1)\\) is the product of the probability that an alive individual is recaptured, \\(p_{it}\\), and the state of the bird \\(z_{it}\\) (alive = 1, dead = 0). Thus, a dead bird cannot be recaptured, whereas for a bird alive during time period \\(t\\), the recapture probability equals \\(p_{it}\\): \\[y_{it} \\sim Bernoulli(z_{it}p_{it})\\] The latent state variable \\(z_{it}\\) is a Markovian variable with the state at time \\(t\\) being dependent on the state at time \\(t-1\\) and the apparent survival probability \\[\\phi_{it}\\]: \\[z_{it} \\sim Bernoulli(z_{it-1}\\phi_{it})\\] We use the term apparent survival in order to indicate that the parameter \\(\\phi\\) is a product of site fidelity and survival. Thus, individuals that permanently emigrated from the study area cannot be distinguished from dead individuals. In both models, the parameters \\(\\phi\\) and \\(p\\) were modelled as sex-specific. However, for parts of the individuals, sex could not be identified, i.e. sex was missing. Ignoring these missing values would most likely lead to a bias because they were not missing at random. The probability that sex can be identified is increasing with age and most likely differs between sexes. Therefore, we included a mixture model for the sex: \\[Sex_i \\sim Categorical(q_i)\\] where \\(q_i\\) is a vector of length 2, containing the probability of being a male and a female, respectively. In this way, the sex of the non-identified individuals was assumed to be male or female with probability \\(q[1]\\) and \\(q[2]=1-q[1]\\), respectively. This model corresponds to the finite mixture model introduced by Pledger, Pollock, and Norris (2003) in order to account for unknown classes of birds (heterogeneity). However, in our case, for parts of the individuals the class (sex) was known. In the example model, we constrain apparent survival to be linearly dependent on a covariate x with different slopes for males, females and juveniles using the logit link function. \\[logit(\\phi_{it}) = a0_{sex-age-class[it]} + a1_{sex-age-class[it]}x_i\\] Annual recapture probability was modelled for each year and age and sex class independently: \\[p_{it} = b0_{t,sex-age-class[it]}\\] Uniform prior distributions were used for all parameters with a parameter space limited to values between 0 and 1 (probabilities) and a normal distribution with a mean of 0 and a standard deviation of 1.5 for the intercept \\(a0\\), and a standard deviation of 5 was used for \\(a1\\). 15.4 The Stan code The trick for coding the CMR-mixture model in Stan is to formulate the model 3 times: 1. For the individuals with identified sex 2. For the males that were not identified 3. For the females that were not identified Then for the non-identified individuals a mixture model is formulated that assigns a probability of being a female or a male to each individual. data { int&lt;lower=2&gt; nocc; // number of capture events int&lt;lower=0&gt; nindi; // number of individuals with identified sex int&lt;lower=0&gt; nindni; // number of individuals with non-identified sex int&lt;lower=0,upper=2&gt; yi[nindi,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firsti[nindi]; // year of first capture int&lt;lower=0,upper=2&gt; yni[nindni,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firstni[nindni]; // year of first capture int&lt;lower=1, upper=2&gt; sex[nindi]; int&lt;lower=1, upper=2&gt; juvi[nindi, nocc]; int&lt;lower=1, upper=2&gt; juvni[nindni, nocc]; int&lt;lower=1&gt; year[nocc]; real x[nocc-1]; // a covariate } transformed data { int&lt;lower=0,upper=nocc+1&gt; lasti[nindi]; // last[i]: ind i last capture int&lt;lower=0,upper=nocc+1&gt; lastni[nindni]; // last[i]: ind i last capture lasti = rep_array(0,nindi); lastni = rep_array(0,nindni); for (i in 1:nindi) { for (k in firsti[i]:nocc) { if (yi[i,k] == 1) { if (k &gt; lasti[i]) lasti[i] = k; } } } for (ii in 1:nindni) { for (kk in firstni[ii]:nocc) { if (yni[ii,kk] == 1) { if (kk &gt; lastni[ii]) lastni[ii] = kk; } } } } parameters { real&lt;lower=0, upper=1&gt; theta[nindni]; // probability of being male for non-identified individuals real&lt;lower=0, upper=1&gt; b0[2,nocc-1]; // intercept of p real a0[2,2]; // intercept for phi real a1[2,2]; // coefficient for phi } transformed parameters { real&lt;lower=0,upper=1&gt;p_male[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p_female[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p[nindi,nocc]; // capture probability real&lt;lower=0,upper=1&gt;phi_male[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_male[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi_female[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_female[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi[nindi,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi[nindi,nocc+1]; // probability that an individual // is never recaptured after its // last capture { int k; int kk; for(ii in 1:nindi){ if (firsti[ii]&gt;1) { for (z in 1:(firsti[ii]-1)){ phi[ii,z] = 1; } } for(tt in firsti[ii]:(nocc-1)) { // linear predictor for phi: phi[ii,tt] = inv_logit(a0[sex[ii], juvi[ii,tt]] + a1[sex[ii], juvi[ii,tt]]*x[tt]); } } for(ii in 1:nindni){ if (firstni[ii]&gt;1) { for (z in 1:(firstni[ii]-1)){ phi_female[ii,z] = 1; phi_male[ii,z] = 1; } } for(tt in firstni[ii]:(nocc-1)) { // linear predictor for phi: phi_male[ii,tt] = inv_logit(a0[1, juvni[ii,tt]] + a1[1, juvni[ii,tt]]*x[tt]); phi_female[ii,tt] = inv_logit(a0[2, juvni[ii,tt]]+ a1[2, juvni[ii,tt]]*x[tt]); } } for(i in 1:nindi) { // linear predictor for p for identified individuals for(w in 1:firsti[i]){ p[i,w] = 1; } for(kkk in (firsti[i]+1):nocc) p[i,kkk] = b0[sex[i],year[kkk-1]]; chi[i,nocc+1] = 1.0; k = nocc; while (k &gt; firsti[i]) { chi[i,k] = (1 - phi[i,k-1]) + phi[i,k-1] * (1 - p[i,k]) * chi[i,k+1]; k = k - 1; } if (firsti[i]&gt;1) { for (u in 1:(firsti[i]-1)){ chi[i,u] = 0; } } chi[i,firsti[i]] = (1 - p[i,firsti[i]]) * chi[i,firsti[i]+1]; }// close definition of transformed parameters for identified individuals for(i in 1:nindni) { // linear predictor for p for non-identified individuals for(w in 1:firstni[i]){ p_male[i,w] = 1; p_female[i,w] = 1; } for(kkkk in (firstni[i]+1):nocc){ p_male[i,kkkk] = b0[1,year[kkkk-1]]; p_female[i,kkkk] = b0[2,year[kkkk-1]]; } chi_male[i,nocc+1] = 1.0; chi_female[i,nocc+1] = 1.0; k = nocc; while (k &gt; firstni[i]) { chi_male[i,k] = (1 - phi_male[i,k-1]) + phi_male[i,k-1] * (1 - p_male[i,k]) * chi_male[i,k+1]; chi_female[i,k] = (1 - phi_female[i,k-1]) + phi_female[i,k-1] * (1 - p_female[i,k]) * chi_female[i,k+1]; k = k - 1; } if (firstni[i]&gt;1) { for (u in 1:(firstni[i]-1)){ chi_male[i,u] = 0; chi_female[i,u] = 0; } } chi_male[i,firstni[i]] = (1 - p_male[i,firstni[i]]) * chi_male[i,firstni[i]+1]; chi_female[i,firstni[i]] = (1 - p_female[i,firstni[i]]) * chi_female[i,firstni[i]+1]; } // close definition of transformed parameters for non-identified individuals } // close block of transformed parameters exclusive parameter declarations } // close transformed parameters model { // priors theta ~ beta(1, 1); for (g in 1:(nocc-1)){ b0[1,g]~beta(1,1); b0[2,g]~beta(1,1); } a0[1,1]~normal(0,1.5); a0[1,2]~normal(0,1.5); a1[1,1]~normal(0,3); a1[1,2]~normal(0,3); a0[2,1]~normal(0,1.5); a0[2,2]~normal(a0[1,2],0.01); // for juveniles, we assume that the effect of the covariate is independet of sex a1[2,1]~normal(0,3); a1[2,2]~normal(a1[1,2],0.01); // likelihood for identified individuals for (i in 1:nindi) { if (lasti[i]&gt;0) { for (k in firsti[i]:lasti[i]) { if(k&gt;1) target+= (log(phi[i, k-1])); if (yi[i,k] == 1) target+=(log(p[i,k])); else target+=(log1m(p[i,k])); } } target+=(log(chi[i,lasti[i]+1])); } // likelihood for non-identified individuals for (i in 1:nindni) { real log_like_male = 0; real log_like_female = 0; if (lastni[i]&gt;0) { for (k in firstni[i]:lastni[i]) { if(k&gt;1){ log_like_male += (log(phi_male[i, k-1])); log_like_female += (log(phi_female[i, k-1])); } if (yni[i,k] == 1){ log_like_male+=(log(p_male[i,k])); log_like_female+=(log(p_female[i,k])); } else{ log_like_male+=(log1m(p_male[i,k])); log_like_female+=(log1m(p_female[i,k])); } } } log_like_male += (log(chi_male[i,lastni[i]+1])); log_like_female += (log(chi_female[i,lastni[i]+1])); target += log_mix(theta[i], log_like_male, log_like_female); } } 15.5 Call Stan from R, check convergence and look at results # Run STAN library(rstan) fit &lt;- stan(file = &quot;stanmodels/cmr_mixture_model.stan&quot;, data=datax, verbose = FALSE) # for above simulated data (25000 individuals x 15 time periods) # computing time is around 48 hours on an intel corei7 laptop # for larger data sets, we recommed moving the transformed parameters block # to the model block in order to avoid monitoring of p_male, p_female, # phi_male and phi_female producing memory problems # launch_shinystan(fit) # diagnostic plots summary(fit) ## mean se_mean sd 2.5% 25% ## b0[1,1] 0.60132367 0.0015709423 0.06173884 0.48042366 0.55922253 ## b0[1,2] 0.70098709 0.0012519948 0.04969428 0.60382019 0.66806698 ## b0[1,3] 0.50293513 0.0010904085 0.04517398 0.41491848 0.47220346 ## b0[1,4] 0.28118209 0.0008809447 0.03577334 0.21440931 0.25697691 ## b0[1,5] 0.34938289 0.0009901335 0.03647815 0.27819918 0.32351323 ## b0[1,6] 0.13158569 0.0006914740 0.02627423 0.08664129 0.11286629 ## b0[1,7] 0.61182981 0.0010463611 0.04129602 0.53187976 0.58387839 ## b0[1,8] 0.48535193 0.0010845951 0.04155762 0.40559440 0.45750793 ## b0[1,9] 0.52531291 0.0008790063 0.03704084 0.45247132 0.50064513 ## b0[1,10] 0.87174780 0.0007565552 0.03000936 0.80818138 0.85259573 ## b0[1,11] 0.80185454 0.0009425675 0.03518166 0.73173810 0.77865187 ## b0[1,12] 0.33152443 0.0008564381 0.03628505 0.26380840 0.30697293 ## b0[1,13] 0.42132288 0.0012174784 0.04140382 0.34062688 0.39305210 ## b0[1,14] 0.65180372 0.0015151039 0.05333953 0.55349105 0.61560493 ## b0[2,1] 0.34237039 0.0041467200 0.12925217 0.12002285 0.24717176 ## b0[2,2] 0.18534646 0.0023431250 0.07547704 0.05924694 0.12871584 ## b0[2,3] 0.61351083 0.0024140550 0.07679100 0.46647727 0.56242546 ## b0[2,4] 0.37140208 0.0024464965 0.06962399 0.24693888 0.32338093 ## b0[2,5] 0.19428215 0.0034618302 0.11214798 0.02800056 0.11146326 ## b0[2,6] 0.27371336 0.0026553769 0.09054020 0.11827243 0.20785316 ## b0[2,7] 0.18611173 0.0014387436 0.05328492 0.09122869 0.14789827 ## b0[2,8] 0.25648337 0.0018258589 0.05287800 0.16255769 0.21913271 ## b0[2,9] 0.20378754 0.0021367769 0.07380004 0.07777998 0.15215845 ## b0[2,10] 0.52679548 0.0024625568 0.08696008 0.36214334 0.46594844 ## b0[2,11] 0.47393354 0.0032593161 0.10555065 0.28843967 0.39781278 ## b0[2,12] 0.22289155 0.0017082729 0.05551514 0.12576797 0.18203335 ## b0[2,13] 0.26191486 0.0024159794 0.07016314 0.14106495 0.21234017 ## b0[2,14] 0.65111737 0.0055743944 0.18780555 0.29279480 0.50957591 ## a0[1,1] 0.95440670 0.0013771881 0.04808748 0.86301660 0.92146330 ## a0[1,2] 0.01529770 0.0469699511 1.46995922 -2.82218067 -0.95533706 ## a0[2,1] 0.16384995 0.0049928331 0.12634422 -0.06399631 0.07533962 ## a0[2,2] 0.01535679 0.0469634175 1.47006964 -2.81864060 -0.95515751 ## a1[1,1] 0.15937249 0.0028992587 0.08864790 -0.01288607 0.10017613 ## a1[1,2] 0.08055953 0.1007089857 3.02148727 -5.95525636 -1.96662599 ## a1[2,1] -0.83614134 0.0074143920 0.18655882 -1.21033848 -0.95698565 ## a1[2,2] 0.08071668 0.1006904255 3.02145647 -5.94617355 -1.96508733 ## 50% 75% 97.5% n_eff Rhat ## b0[1,1] 0.60206306 0.6431566 0.7206343 1544.5301 1.002331 ## b0[1,2] 0.70165494 0.7355204 0.7946280 1575.4617 1.001482 ## b0[1,3] 0.50367411 0.5330078 0.5898079 1716.3196 1.001183 ## b0[1,4] 0.27997512 0.3046483 0.3544592 1649.0040 1.000760 ## b0[1,5] 0.34936442 0.3751935 0.4191138 1357.3073 1.002072 ## b0[1,6] 0.12987449 0.1481661 0.1873982 1443.8040 1.003676 ## b0[1,7] 0.61203228 0.6397577 0.6933929 1557.5904 1.001458 ## b0[1,8] 0.48513822 0.5134314 0.5672066 1468.1355 1.002511 ## b0[1,9] 0.52534212 0.5501747 0.5994060 1775.7335 1.000824 ## b0[1,10] 0.87324112 0.8934047 0.9258033 1573.3747 1.000719 ## b0[1,11] 0.80300311 0.8261868 0.8675033 1393.1817 1.001172 ## b0[1,12] 0.33044476 0.3552199 0.4052902 1794.9956 1.000566 ## b0[1,13] 0.42116690 0.4492297 0.5026942 1156.5339 1.000289 ## b0[1,14] 0.64956850 0.6864706 0.7607107 1239.4056 1.004061 ## b0[2,1] 0.33493631 0.4251416 0.6150923 971.5524 1.004049 ## b0[2,2] 0.17981663 0.2358847 0.3446097 1037.6210 1.001474 ## b0[2,3] 0.61326419 0.6644156 0.7628427 1011.8737 1.005727 ## b0[2,4] 0.36837778 0.4158585 0.5190457 809.8949 1.003803 ## b0[2,5] 0.17910449 0.2591418 0.4533117 1049.4733 1.001499 ## b0[2,6] 0.26739172 0.3299594 0.4685139 1162.6006 1.001170 ## b0[2,7] 0.18254607 0.2198969 0.3003156 1371.6455 1.000878 ## b0[2,8] 0.25280556 0.2895585 0.3704113 838.7174 1.005624 ## b0[2,9] 0.19724053 0.2501298 0.3694806 1192.8747 1.003687 ## b0[2,10] 0.52587075 0.5845730 0.7061694 1247.0027 1.002851 ## b0[2,11] 0.46874445 0.5392302 0.7046892 1048.7425 0.999473 ## b0[2,12] 0.21961656 0.2580782 0.3397127 1056.1081 1.000907 ## b0[2,13] 0.25601959 0.3056204 0.4142888 843.3960 1.003130 ## b0[2,14] 0.65824835 0.7973674 0.9698829 1135.0669 1.003838 ## a0[1,1] 0.95368445 0.9862439 1.0515747 1219.2071 1.003898 ## a0[1,2] 0.01633534 0.9911055 2.9717839 979.4231 1.003726 ## a0[2,1] 0.15519648 0.2472483 0.4230776 640.3489 1.004625 ## a0[2,2] 0.01587281 0.9898084 2.9659552 979.8429 1.003744 ## a1[1,1] 0.15647489 0.2205720 0.3354845 934.8953 1.007190 ## a1[1,2] 0.06683287 2.1568781 6.0295208 900.1297 1.003701 ## a1[2,1] -0.83503982 -0.7075691 -0.4814539 633.1119 1.010568 ## a1[2,2] 0.06586905 2.1557247 6.0239735 900.4432 1.003704 "],["referenzen.html", "Referenzen", " Referenzen "]]
