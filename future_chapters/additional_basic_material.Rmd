
# Additional basic material {#addbasics}

THIS CHAPTER IS UNDER CONSTRUCTION!!! 

## Correlations among categorical variables

### Chisquare test

When testing for correlations between two categorical variables, then the nullhypothesis is "there is no correlation". The data can be displayed in cross-tables. 

```{r, echo=TRUE}
# Example: correlation between birthday preference and car ownership
load("RData/datacourse.RData")
table(dat$birthday, dat$car)
```

Given the nullhypothesis was true, we expect that the distribution of the data in each column of the cross-table is similar to the distribution of the row-sums. And, the distribution of the data in each row should be similar to the distribution of the column-sums. The chisquare test statistics $\chi^2$ measures the deviation of the data from this expected distribution of the data in the cross-table. 

For calculating the chisquare test statistics $\chi^2$, we first have to obtain for each cell in the cross-table the expected value $E_{ij}$ = rowsum*colsum/total.

$\chi^2$ measures the difference between the observed $O_{ij}$ and expected $E_{ij}$ values as:  
$\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(O_{ij}-E_{ij})^2}{E_{ij}}$ where $m$ is the number of rows and $k$ is the number of columns. 
The $\chi^2$-distribution has 1 parameter, the degrees of freedom $v$ = $(m-1)(k-1)$.

```{r chisqdist, echo=FALSE, fig.cap="Two examples of Chisquare distributions.", fig.height=4, fig.width=7}
par(mar=c(4.5, 4.5, 0,0))
x <- seq(0, 10, length=100)
y <- dchisq(x, 2)
plot(x,y, type="l", lwd=2, las=1, xlab="Chisquare-value", ylab="Density")
text(5, 0.08, "df=2")
y <- dchisq(x, 6)
lines(x, y, lwd=2, col="blue")
text(6, 0.15, "df=6", col="blue")

```

R is calculating the $\chi^2$ value for specific cross-tables, and it is also giving the p-values, i.e., the probability of obtaining the observed or a higher $\chi^2$ value given the nullhypothesis was true by comparing the observed $\chi^2$ with the corresponding chisquare distribution.

```{r, echo=TRUE}
chisq.test(table(dat$birthday, dat$car))
```

The warning (that is suppressed in the rmarkdown version, but that you will see if you run the code on your own computer) is given, because in our example some cells have counts less than 5. In such cases, the Fisher's exact test should be preferred. This test calculates the p-value analytically using probability theory, whereas the chisquare test relies on the assumption that the  $\chi^2$ value follows a chisquare distribution. The latter assumption holds better for larger sample sizes.

```{r, echo=TRUE}
fisher.test(table(dat$birthday, dat$car))
```

 
### Correlations among categorical variables using Bayesian methods

For a Bayesian analysis of cross-table data, a data model has to be found. There are several possibilities that could be used:  

* a so-called log-linear model (Poisson model) for the counts in each cell of the cross-table.  
* a binomial or a multinomial model for obtaining estimates of the proportions of data in each cell

These models provide possibilities to explore the patterns in the data in more details than a chisquare test. 

```{r, echo=TRUE}
# We arrange the data into a cross-table in a data-frame
# format. That is, the counts in each cell of the 
# cross-table become a variable and the row and column names
# are also given in separate variables 
datagg <- aggregate(dat$name_fictive, list(birthday=dat$birthday, car=dat$car), 
                    length, drop=FALSE)
datagg$x[is.na(datagg$x)] <- 0
names(datagg) <- c("birthday", "car", "count")
datagg
```


```{r, echo=TRUE}
# log-linear model
library(arm)
nsim <- 5000

mod <- glm(count~birthday+car + birthday:car, 
           data=datagg, family=poisson)
bsim <- sim(mod, n.sim=nsim)
round(t(apply(bsim@coef, 2, quantile, 
              prob=c(0.025, 0.5, 0.975))),2)
```

The interaction parameter measures the strength of the correlation. To quantitatively understand what a parameter value of `r round(coef(mod)[4],2)` means, we have to look at the interpretation of all parameter values. We do that here quickly without a thorough explanation, because we already explained the Poisson model in chapter 8 of [@KornerNievergelt2015].

The intercept `r round(coef(mod)[1],2)` corresponds to the logarithm of the count in the cell "flowers" and "N" (number of students who prefer flowers as a birthday present and who do not have a car), i.e., $exp(\beta_0)$ = `r round(exp(coef(mod)[1]),2)`. The exponent of the second parameter corresponds to the multiplicative difference between the counts in the cells "flowers and N" and "wine and N", i.e., count in the cell "wine and N" = $exp(\beta_0)exp(\beta_1)$ = exp(`r round(coef(mod)[1],2)`)exp(`r round(coef(mod)[2],2)`) = `r round(exp(coef(mod)[1])*exp(coef(mod)[2]), 2)`. The third parameter measures the multiplicative difference in the counts between the cells "flowers and N" and "flowers and Y", i.e., count in the cell "flowers and Y" = $exp(\beta_0)exp(\beta_2)$ = exp(`r round(coef(mod)[1],2)`)exp(`r round(coef(mod)[3],2)`) = `r round(exp(coef(mod)[1])*exp(coef(mod)[3]), 2)`. Thus, the third parameter is the difference in the logarithm of the counts between the car owners and the car-free students for those who prefer flowers. The interaction parameter is the difference of this difference between the students who prefer wine and those who prefer flowers. This is difficult to intuitively understand. Here is another try to formulate it: The interaction parameter measures the difference in the logarithm of the counts in the cross-table between the row-differences between the columns. Maybe it becomes clear, when we extract the count in the cell "wine and Y" from the model parameters: $exp(\beta_0)exp(\beta_1)exp(\beta_2)exp(\beta_3)$ = exp(`r round(coef(mod)[1],2)`)exp(`r round(coef(mod)[2],2)`)exp(`r round(coef(mod)[3],2)`)exp(`r round(coef(mod)[4],2)`) = `r round(exp(coef(mod)[1])*exp(coef(mod)[2])*exp(coef(mod)[3])*exp(coef(mod)[4]), 2)`.  


Alternatively, we could estimate the proportions of students prefering flower and wine within each group of car owners and car-free students using a binomial model. For an explanation of the binomial model, see chapter 8 of [@KornerNievergelt2015].

```{r, echo=TRUE}
# binomial model
tab <- table(dat$car,dat$birthday)
mod <- glm(tab~rownames(tab),  family=binomial)
bsim <- sim(mod, n.sim=nsim)
```

```{r, echo=FALSE, fig.cap="Estimated proportion of students that prefer flowers over wine as a birthday present among the car-free students (N) and the car owners (Y). Given are the median of the posterior distribution (circle). The bar extends between the 2.5% and 97.5% quantiles of the posterior distribution."}
par(cex=1.2, mar=c(5,4.5,0,0))
plot(1:3, seq(0, 1, length=3), type="n", xaxt="n", xlab=NA, ylab="Proportion flower", 
     las=1,xlim=c(0.5, 2.5))
axis(1, at=1:2, labels=levels(dat$car))
fitmat <- plogis(cbind(bsim@coef[,1], bsim@coef[,1]+bsim@coef[,2]))
segments(1:2, apply(fitmat, 2, quantile, prob=0.025), 
         1:2, apply(fitmat, 2, quantile, prob=0.975), lwd=3, lend="butt")
points(1:2, apply(fitmat, 2, quantile, prob=0.5), pch=21, bg="white")

```







## 3 methods for getting the posterior distribution

* analytically
* approximation
* Monte Carlo simulation

### Monte Carlo simulation (parametric bootstrap)  
  
Monte Carlo integration: numerical solution of $\int_{-1}^{1.5} F(x) dx$ 
```{r, fig.cap='',echo=FALSE}
# a function that is hard to integrate
myfunction <- function(x) 2/x * sin(x) + 0.5*cos(4*x) + sin(2*abs(x-0.2)) + asin(x/3)
x <- seq(-1.5, 2, length=100)
par(mar=c(4.5,4.5,0,1))
plot(x, myfunction(x), type="l", las=1, ylim=c(0, 3.5), ylab="F(x)")
rect(-1, 0, 1.5, 3.3)

ranx <- runif(nsim, -1, 1.5)
rany <- runif(nsim, 0, 3.3)
colkey <- c("orange", "blue")[as.numeric(myfunction(ranx)<rany)+1]
points(ranx, rany, col=colkey, pch=16, cex=0.5)
lines(x, myfunction(x), lwd=2)
```


sim is solving a mathematical problem by simulation
How sim is simulating to get the marginal distribution of $\mu$:

```{r, fig.cap='',echo=FALSE}
y <- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5)
n <- length(y)

mod <- lm(y~1)
bsim <- sim(mod, n.sim=20000)
xrange <- range(bsim@coef)
yrange <- range(bsim@sigma)

layout(matrix(c(2,0,1,3), ncol=2, byrow=TRUE), heights=c(1,2),  
       width=c(2,1))
par(mar=c(5.1, 4.1, 0.2, 0.2), cex.lab=0.8, cex.axis=0.8) 
plot(bsim@coef, bsim@sigma, xlab=expression(theta), ylab=expression(sigma), las=1, cex=0.5, xlim=xrange,  ylim=yrange, col=rgb(0,0,0,0.1), pch=16)
abline(h=6.5)
text(33, 10, paste(expression(p(theta|y, sigma))), adj=c(0,1), cex=0.8)
text(33, 8, "= normal distribution", adj=c(0,1), cex=0.8)
text(33, 12, "step 2", adj=c(0,1))
par(mar=c(0, 4.1, 1, 0.2)) 
histtheta <- hist(bsim@coef, plot=FALSE)
plot(histtheta$breaks, seq(0, max(histtheta$density)+0.01, length= 
                             length(histtheta$breaks)), type="n", xaxt="n", xlim = xrange, 
     las=1, ylab = "Density", yaxs="i")
rect(histtheta$breaks[1:(length(histtheta$breaks)-1)], 0, 
     histtheta$breaks[2:(length(histtheta$breaks))], histtheta$density, 
     col=grey(0.5))
mtext(paste(expression(p(theta|y)), "= t-distribution"), side=3, adj=0, cex=0.8)
par(mar=c(5.1, 0, 0.2, 0.2)) 
histsigma<- hist(bsim@sigma, plot=FALSE)
plot(seq(0, max(histsigma$density)+0.01, length=length(histsigma$breaks)), 
     histsigma$breaks, type="n", yaxt="n", ylim=yrange, 
     xlab="Density", xaxs="i",
     xaxt="n")
axis(1, at=seq(0, 0.4, by=0.1), labels=c(NA, 0.1, 0.3, 0.3, 0.4))
rect(0, histsigma$breaks[2:(length(histsigma$breaks))], histsigma$density, 
     histsigma$breaks[1:(length(histsigma$breaks)-1)],col=grey(0.5))
text(0.04, 18, "p(sigma|y) = \nInv-Chisq-dist.", adj=c(0,1), cex=0.8)
abline(h=6.5)
text(0.02, 9, "step 1", adj=c(0,1))
```




### Grid approximation
  
$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}$ 
  
For example, one coin flip (Bernoulli model) 
  
data: y=0  (a tail)  
likelihood: $p(y|\theta)=\theta^y(1-\theta)^{(1-y)}$


```{r,  fig.cap='',echo=FALSE}
# analytical solution
par(mar=c(4.5, 5, 0.2, 0.2)) 
xa <- seq(0, 1, length=100)
posta <- dbeta(xa, 2, 3)
x<- seq(0.05, 0.95, length=10)
breaksx<- seq(0, 1, length=11)
bandwitdth <- 0.1

y <- 0
prior <- dbeta(x, 2, 2)
lik <- x^y*(1-x)^(1-y)
plot(x,prior, type="b", col="blue", ylim=c(0,2), xlim=c(0,1), lwd=2, 
     xlab=expression(theta), ylab="Density", las=1)
lines(xa, posta, lwd=2, col=grey(0.5), lty=3)
rect(breaksx[1:10], rep(0, 10), breaksx[2:11], prior, border="blue")
text(0.65, 2, "prior", col="blue", adj=c(0,1), cex=0.9)
lines(x, lik, type="b", lwd=2, col="orange", cex=0.9)
text(0.65, 1.8, "likelihood", col="orange", adj=c(0,1), cex=0.9)
text(0.65, 1.6, "posterior (approx.)",adj=c(0,1), cex=0.9)
text(0.75, 1.4, "posterior (anal.)", col=grey(0.5), adj=c(0,1), cex=0.9)

post0 <- lik*prior
post <- post0/(sum(0.1*post0))
lines(x, post, type="b", col=1, lwd=2)

```


### Markov chain Monte Carlo simulations

* Markov chain Monte Carlo simulation (BUGS, Jags)
* Hamiltonian Monte Carlo (Stan)

```{r, echo=FALSE}
par(mar=c(3,3,3,3), cex=2)
plot(c(0,1), c(0,1), type="n", axes=FALSE, xlab=NA, ylab=NA)
text(0,0.8, "likelihood\ndata\npriors\ninitial values", adj=c(0,1))
arrows(0.4, 0.7, 0.6, 0.7, length=0.2, lwd=5)
text(0.63,0.75, "(joint) posterior\ndistribution", adj=c(0,1))
```



## Analysis of variance ANOVA
The aim of an ANOVA is to compare means of groups. In a frequentist analysis, this is done by comparing the between-group with the within-group variance. The result of a Bayesian analysis is the joint posterior distribution of the group means.

```{r, echo=FALSE, fig.cap="Number of stats courses students have taken before starting a PhD in relation to their feeling about statistics."}
y <- log(dat$nrcourses+1)
group <- dat$statsfeeling
boxplot(y~group, las=1, names=levels(group), ylab="log(Nr stats courses+1)",
    boxwex=0.5, col="orange", cex.lab=1.4, ylim=c(-0.2, max(y)), cex.axis=1.4)
npg <- table(group)
text(1:3, rep(-0.15, 3), npg, cex=1.4, xpd=NA)
text(0.6, -0.15, "n =", cex=1.4, xpd=NA)

```

In the frequentist ANOVA, the following three sum of squared distances (SS) are used to calculate the total, the between- and within-group variances:  
Total sum of squares =  SST = $\sum_1^n{(y_i-\bar{y})^2}$  
Within-group SS = SSW = $\sum_1^n{(y_i-\bar{y_g})^2}$: unexplained variance  
Between-group SS = SSB = $\sum_1^g{n_g(\bar{y_g}-\bar{y})^2}$: explained variance  

The between-group and within-group SS sum to the total sum of squares: SST=SSB+SSW. Attention: this equation is only true in any case for a simple one-way ANOVA (just one grouping factor). If the data are grouped according to more than one factor (such as in a two- or three-way ANOVA), then there is one single solution for the equation only when the data is completely balanced, i.e. when there are the same number of observations in all combinations of factor levels. For non-balanced data with more than one grouping factor, there are different ways of calculating the SSBs, and the result of the F-test described below depends on the order of the predictors in the model.   

```{r, echo=FALSE, fig.cap="Visualisation of the total, between-group and within-group sum of squares. Points are observations; long horizontal line is the overall mean; short horizontal lines are group specific means."}
n<-length(y)
par(mfrow=c(1,3), mar=c(4,3,3,0.5), oma=c(0,2,0,0))
# total sum of squares
plot(1:n, y, las=1, cex.lab=1.2, pch=16, ylab=NA, 
     xlab="Observation ID", main="SST", cex.main=1.5)
abline(h=mean(y))
segments(1:n, mean(y), 1:n, y, col="orange", lwd=1)
mtext("Log(Nr stats courses +1)", side=2, outer=TRUE)
text(n*1.1, mean(y), "=", xpd=NA, cex=2)

# sum of squares between groups
mpg <- tapply(y, group, mean)
plot(1:n, y, las=1, cex.lab=1.2, pch=16, ylab=NA, 
     xlab="Observation ID", main="SSB", yaxt="n", cex.main=1.5)
segments(c(1, cumsum(npg)[1:2]+1), mpg, cumsum(npg), mpg)
abline(h=mean(y))
segments(1:n, rep(mpg, times=npg), 1:n, mean(y), col="orange", 
         lwd=1)
text(n*1.1, mean(y), "+", xpd=NA, cex=2)

# sum of squares within groups
plot(1:n, y, las=1, cex.lab=1.2, pch=16, ylab=NA, 
     xlab="Observation ID", main="SSW", yaxt="n", cex.main=1.5)
segments(c(1, cumsum(npg)[1:2]+1), mpg, cumsum(npg), mpg)
segments(1:n, rep(mpg, times=npg), 1:n, y, col="orange", lwd=1)
```


In order to make SSB and SSW comparable, we have to divide them by their degrees of freedoms. For the within-group SS, SSW, the degrees of freedom is the number of obervations minus the number of groups ($g$), because $g$ means have been estimated from the data. If the $g$ means are fixed and $n-g$ data points are known, then the last $g$ data points are defined, i.e., they cannot be chosen freely. For the between-group SS, SSB, the degrees of freedom is the number of groups  minus 1 (the minus 1 stands for the overall mean).   

* MSB = SSB/df_between, MSW = SSW/df_within  

It can be shown (by mathematicians) that, given the nullhypothesis, the mean of all groups are equal $m_1 = m_2 = m_3$, then the mean squared errors between groups (MSB) is expected to be equal to the mean squared errors within the groups (MSW). Therefore, the ration MSB/MSW is  expected to follow an F-distribution given the nullhypothesis is true.

* MSB/MSW ~ F(df_between, df_within)


The Bayesian analysis for comparing group means consists of calculating the posterior distribution for each group mean and then drawing inference from these posterior distributions. 
A Bayesian one-way ANOVA involves the following steps:  
1. Decide for a data model: We, here, assume that the measurements are normally distributed around the group means. In this example here, we transform the outcome variable in order to better meet the normal assumption. Note: the frequentist ANOVA makes exactly the same assumptions. We can write the data model: $y_i\sim Norm(\mu_i,\sigma)$ with $mu_i= \beta_0 + \beta_1I(group=2) +\beta_1I(group=3)$, where the $I()$-function is an indicator function taking on 1 if the expression is true and 0 otherwise. This model has 4 parameters: $\beta_0$,  $\beta_1$, $\beta_2$ and $\sigma$. 

```{r, echo=TRUE}
# fit a normal model with 3 different means
mod <- lm(log(nrcourses+1)~statsfeeling, data=dat)
```

2. Choose a prior distribution for each model parameter: In this example, we choose flat prior distributions for each parameter. By using these priors, the result should not remarkably be affected by the prior distributions but almost only reflect the information in the data. We choose so-called improper prior distributions. These are completely flat distributions that give all parameter values the same probability. Such distributions are called improper because the area under the curve is not summing to 1 and therefore, they cannot be considered to be proper probability distributions. However, they can still be used to solve the Bayesian theorem.  

3. Solve the Bayes theorem: The solution of the Bayes theorem for the above priors and model is implemented in the function sim of the package arm. 

```{r, echo=TRUE}
# calculate numerically the posterior distributions of the model 
# parameters using flat prior distributions
nsim <- 5000
set.seed(346346)
bsim <- sim(mod, n.sim=nsim)
```

4. Display the joint posterior distributions of the group means  


```{r, echo=TRUE, fig.cap="Posterior distributions of the mean number of stats courses PhD students visited before starting the PhD grouped according to their feelings about statistics."}
# calculate group means from the model parameters
newdat <- data.frame(statsfeeling=levels(factor(dat$statsfeeling)))
X <- model.matrix(~statsfeeling, data=newdat)
fitmat <- matrix(ncol=nsim, nrow=nrow(newdat))
for(i in 1:nsim) fitmat[,i] <- X%*%bsim@coef[i,]
hist(fitmat[1,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab="Group mean of log(number of courses +1)", las=1, ylim=c(0, 2.2))
hist(fitmat[2,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab="", las=1, add=TRUE, col=rgb(0,0,1,0.5))
hist(fitmat[3,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab="", las=1, add=TRUE, col=rgb(1,0,0,0.5))
legend(2,2, fill=c("white",rgb(0,0,1,0.5), rgb(1,0,0,0.5)), legend=levels(factor(dat$statsfeeling)))
```

Based on the posterior distributions of the group means, we can extract derived quantities depending on our interest and questions. Here, for example, we could extract the posterior probability of the hypothesis that students with a positive feeling about statistics have a better education in statistics than those with a neutral or negative feeling about statistics.  

```{r, echo=TRUE}
# P(mean(positive)>mean(neutral))
mean(fitmat[3,]>fitmat[2,])

# P(mean(positive)>mean(negative))
mean(fitmat[3,]>fitmat[1,])

```




## Summary

