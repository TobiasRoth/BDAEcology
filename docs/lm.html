<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan</title>
  <meta name="description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.jpg" />
  <meta property="og:description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="github-repo" content="TobiasRoth/BDAEcology" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan" />
  
  <meta name="twitter:description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="twitter:image" content="/images/cover.jpg" />

<meta name="author" content="Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Pius Korner-Nievergelt" />


<meta name="date" content="2021-08-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="priors.html"/>
<link rel="next" href="residualanalysis.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="Settings/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book"><i class="fa fa-check"></i>Why this book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-contribute"><i class="fa fa-check"></i>How to contribute?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="part"><span><b>I BASIC STATISTICS FOR ECOLOGISTS</b></span></li>
<li class="chapter" data-level="1" data-path="PART-I.html"><a href="PART-I.html"><i class="fa fa-check"></i><b>1</b> Introduction to PART I</a>
<ul>
<li class="chapter" data-level="1.1" data-path="PART-I.html"><a href="PART-I.html#further-reading"><i class="fa fa-check"></i><b>1.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>2</b> Prerequisits: Basic statistical terms</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basics.html"><a href="basics.html#variables-and-observations"><i class="fa fa-check"></i><b>2.1</b> Variables and observations</a></li>
<li class="chapter" data-level="2.2" data-path="basics.html"><a href="basics.html#displaying-and-summarizing-variables"><i class="fa fa-check"></i><b>2.2</b> Displaying and summarizing variables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="basics.html"><a href="basics.html#correlations"><i class="fa fa-check"></i><b>2.2.1</b> Correlations</a></li>
<li class="chapter" data-level="2.2.2" data-path="basics.html"><a href="basics.html#principal-components-analyses-pca"><i class="fa fa-check"></i><b>2.2.2</b> Principal components analyses PCA</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="basics.html"><a href="basics.html#inferential-statistics"><i class="fa fa-check"></i><b>2.3</b> Inferential statistics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="basics.html"><a href="basics.html#uncertainty"><i class="fa fa-check"></i><b>2.3.1</b> Uncertainty</a></li>
<li class="chapter" data-level="2.3.2" data-path="basics.html"><a href="basics.html#standard-error"><i class="fa fa-check"></i><b>2.3.2</b> Standard error</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="basics.html"><a href="basics.html#bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods"><i class="fa fa-check"></i><b>2.4</b> Bayes theorem and the common aim of frequentist and Bayesian methods</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="basics.html"><a href="basics.html#bayes-theorem-for-discrete-events"><i class="fa fa-check"></i><b>2.4.1</b> Bayes theorem for discrete events</a></li>
<li class="chapter" data-level="2.4.2" data-path="basics.html"><a href="basics.html#bayes-theorem-for-continuous-parameters"><i class="fa fa-check"></i><b>2.4.2</b> Bayes theorem for continuous parameters</a></li>
<li class="chapter" data-level="2.4.3" data-path="basics.html"><a href="basics.html#estimating-a-mean-assuming-that-the-variance-is-known"><i class="fa fa-check"></i><b>2.4.3</b> Estimating a mean assuming that the variance is known</a></li>
<li class="chapter" data-level="2.4.4" data-path="basics.html"><a href="basics.html#estimating-the-mean-and-the-variance"><i class="fa fa-check"></i><b>2.4.4</b> Estimating the mean and the variance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basics.html"><a href="basics.html#classical-frequentist-tests-and-alternatives"><i class="fa fa-check"></i><b>2.5</b> Classical frequentist tests and alternatives</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="basics.html"><a href="basics.html#nullhypothesis-testing"><i class="fa fa-check"></i><b>2.5.1</b> Nullhypothesis testing</a></li>
<li class="chapter" data-level="2.5.2" data-path="basics.html"><a href="basics.html#comparison-of-a-sample-with-a-fixed-values-one-sample-t-test"><i class="fa fa-check"></i><b>2.5.2</b> Comparison of a sample with a fixed values (one-sample t-test)</a></li>
<li class="chapter" data-level="2.5.3" data-path="basics.html"><a href="basics.html#comparison-of-the-locations-between-two-groups-two-sample-t-test"><i class="fa fa-check"></i><b>2.5.3</b> Comparison of the locations between two groups (two-sample t-test)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="basics.html"><a href="basics.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyses-steps.html"><a href="analyses-steps.html"><i class="fa fa-check"></i><b>3</b> Data analysis step by step</a>
<ul>
<li class="chapter" data-level="3.1" data-path="analyses-steps.html"><a href="analyses-steps.html#step1"><i class="fa fa-check"></i><b>3.1</b> Plausibility of Data</a></li>
<li class="chapter" data-level="3.2" data-path="analyses-steps.html"><a href="analyses-steps.html#step2"><i class="fa fa-check"></i><b>3.2</b> Relationships</a></li>
<li class="chapter" data-level="3.3" data-path="analyses-steps.html"><a href="analyses-steps.html#step3"><i class="fa fa-check"></i><b>3.3</b> Data Distribution</a></li>
<li class="chapter" data-level="3.4" data-path="analyses-steps.html"><a href="analyses-steps.html#step4"><i class="fa fa-check"></i><b>3.4</b> Preparation of Explanatory Variables</a></li>
<li class="chapter" data-level="3.5" data-path="analyses-steps.html"><a href="analyses-steps.html#step5"><i class="fa fa-check"></i><b>3.5</b> Data Structure</a></li>
<li class="chapter" data-level="3.6" data-path="analyses-steps.html"><a href="analyses-steps.html#step6"><i class="fa fa-check"></i><b>3.6</b> Define Prior Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="analyses-steps.html"><a href="analyses-steps.html#step7"><i class="fa fa-check"></i><b>3.7</b> Fit the Model</a></li>
<li class="chapter" data-level="3.8" data-path="analyses-steps.html"><a href="analyses-steps.html#step8"><i class="fa fa-check"></i><b>3.8</b> Check Model</a></li>
<li class="chapter" data-level="3.9" data-path="analyses-steps.html"><a href="analyses-steps.html#step9"><i class="fa fa-check"></i><b>3.9</b> Model Uncertainty</a></li>
<li class="chapter" data-level="3.10" data-path="analyses-steps.html"><a href="analyses-steps.html#step10"><i class="fa fa-check"></i><b>3.10</b> Draw Conclusions</a></li>
<li class="chapter" data-level="" data-path="analyses-steps.html"><a href="analyses-steps.html#further-reading-1"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>4</b> Probability distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distributions.html"><a href="distributions.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Normal distribution</a></li>
<li class="chapter" data-level="4.3" data-path="distributions.html"><a href="distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>4.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="4.4" data-path="distributions.html"><a href="distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>4.4</b> Gamma distribution</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="distributions.html"><a href="distributions.html#cauchydistri"><i class="fa fa-check"></i><b>4.4.1</b> Cauchy distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="distributions.html"><a href="distributions.html#t-distribution"><i class="fa fa-check"></i><b>4.4.2</b> t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="distributions.html"><a href="distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.5</b> F-distribution</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="rfunctions.html"><a href="rfunctions.html"><i class="fa fa-check"></i><b>5</b> Important R-functions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="rfunctions.html"><a href="rfunctions.html#data-preparation"><i class="fa fa-check"></i><b>5.1</b> Data preparation</a></li>
<li class="chapter" data-level="5.2" data-path="rfunctions.html"><a href="rfunctions.html#figures"><i class="fa fa-check"></i><b>5.2</b> Figures</a></li>
<li class="chapter" data-level="5.3" data-path="rfunctions.html"><a href="rfunctions.html#summary-1"><i class="fa fa-check"></i><b>5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html"><i class="fa fa-check"></i><b>6</b> Reproducible research</a>
<ul>
<li class="chapter" data-level="6.1" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html#summary-2"><i class="fa fa-check"></i><b>6.1</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="furthertopics.html"><a href="furthertopics.html"><i class="fa fa-check"></i><b>7</b> Further topics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="furthertopics.html"><a href="furthertopics.html#bioacoustic-analyse"><i class="fa fa-check"></i><b>7.1</b> Bioacoustic analyse</a></li>
<li class="chapter" data-level="7.2" data-path="furthertopics.html"><a href="furthertopics.html#python"><i class="fa fa-check"></i><b>7.2</b> Python</a></li>
</ul></li>
<li class="part"><span><b>II BAYESIAN DATA ANALYSIS</b></span></li>
<li class="chapter" data-level="8" data-path="PART-II.html"><a href="PART-II.html"><i class="fa fa-check"></i><b>8</b> Introduction to PART II</a>
<ul>
<li class="chapter" data-level="" data-path="PART-II.html"><a href="PART-II.html#further-reading-2"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-paradigm.html"><a href="bayesian-paradigm.html"><i class="fa fa-check"></i><b>9</b> The Bayesian paradigm</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian-paradigm.html"><a href="bayesian-paradigm.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-paradigm.html"><a href="bayesian-paradigm.html#summary-3"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="priors.html"><a href="priors.html"><i class="fa fa-check"></i><b>10</b> Prior distributions</a>
<ul>
<li class="chapter" data-level="10.1" data-path="priors.html"><a href="priors.html#introduction-2"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="priors.html"><a href="priors.html#choosepriors"><i class="fa fa-check"></i><b>10.2</b> How to choose a prior</a></li>
<li class="chapter" data-level="10.3" data-path="priors.html"><a href="priors.html#prior-sensitivity"><i class="fa fa-check"></i><b>10.3</b> Prior sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>11</b> Normal Linear Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lm.html"><a href="lm.html#linear-regression"><i class="fa fa-check"></i><b>11.1</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="lm.html"><a href="lm.html#background"><i class="fa fa-check"></i><b>11.1.1</b> Background</a></li>
<li class="chapter" data-level="11.1.2" data-path="lm.html"><a href="lm.html#fitting-a-linear-regression-in-r"><i class="fa fa-check"></i><b>11.1.2</b> Fitting a Linear Regression in R</a></li>
<li class="chapter" data-level="11.1.3" data-path="lm.html"><a href="lm.html#drawing-conclusions"><i class="fa fa-check"></i><b>11.1.3</b> Drawing Conclusions</a></li>
<li class="chapter" data-level="11.1.4" data-path="lm.html"><a href="lm.html#frequentist-results"><i class="fa fa-check"></i><b>11.1.4</b> Frequentist Results</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="lm.html"><a href="lm.html#regression-variants-anova-ancova-and-multiple-regression"><i class="fa fa-check"></i><b>11.2</b> Regression Variants: ANOVA, ANCOVA, and Multiple Regression</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="lm.html"><a href="lm.html#one-way-anova"><i class="fa fa-check"></i><b>11.2.1</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="11.2.2" data-path="lm.html"><a href="lm.html#frequentist-results-from-a-one-way-anova"><i class="fa fa-check"></i><b>11.2.2</b> Frequentist Results from a One-Way ANOVA</a></li>
<li class="chapter" data-level="11.2.3" data-path="lm.html"><a href="lm.html#two-way-anova"><i class="fa fa-check"></i><b>11.2.3</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="11.2.4" data-path="lm.html"><a href="lm.html#frequentist-results-from-a-two-way-anova"><i class="fa fa-check"></i><b>11.2.4</b> Frequentist Results from a Two-Way ANOVA</a></li>
<li class="chapter" data-level="11.2.5" data-path="lm.html"><a href="lm.html#multiple-comparisons-and-post-hoc-tests"><i class="fa fa-check"></i><b>11.2.5</b> Multiple Comparisons and Post Hoc Tests</a></li>
<li class="chapter" data-level="11.2.6" data-path="lm.html"><a href="lm.html#analysis-of-covariance"><i class="fa fa-check"></i><b>11.2.6</b> Analysis of Covariance</a></li>
<li class="chapter" data-level="11.2.7" data-path="lm.html"><a href="lm.html#multiple-regression-and-collinearity"><i class="fa fa-check"></i><b>11.2.7</b> Multiple Regression and Collinearity</a></li>
<li class="chapter" data-level="11.2.8" data-path="lm.html"><a href="lm.html#orderedfactors"><i class="fa fa-check"></i><b>11.2.8</b> Ordered Factors and Contrasts</a></li>
<li class="chapter" data-level="11.2.9" data-path="lm.html"><a href="lm.html#quadratic-and-higher-polynomial-terms"><i class="fa fa-check"></i><b>11.2.9</b> Quadratic and Higher Polynomial Terms</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="lm.html"><a href="lm.html#pendenzen"><i class="fa fa-check"></i><b>11.3</b> Pendenzen</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="residualanalysis.html"><a href="residualanalysis.html"><i class="fa fa-check"></i><b>12</b> Assessing Model Assumptions</a>
<ul>
<li class="chapter" data-level="12.1" data-path="residualanalysis.html"><a href="residualanalysis.html#model-assumptions"><i class="fa fa-check"></i><b>12.1</b> Model Assumptions</a></li>
<li class="chapter" data-level="12.2" data-path="residualanalysis.html"><a href="residualanalysis.html#independent-and-identically-distributed"><i class="fa fa-check"></i><b>12.2</b> Independent and Identically Distributed</a></li>
<li class="chapter" data-level="12.3" data-path="residualanalysis.html"><a href="residualanalysis.html#qqplot"><i class="fa fa-check"></i><b>12.3</b> The QQ-Plot</a></li>
<li class="chapter" data-level="12.4" data-path="residualanalysis.html"><a href="residualanalysis.html#tempautocorrelation"><i class="fa fa-check"></i><b>12.4</b> Temporal Autocorrelation</a></li>
<li class="chapter" data-level="12.5" data-path="residualanalysis.html"><a href="residualanalysis.html#spatialautocorrelation"><i class="fa fa-check"></i><b>12.5</b> Spatial Autocorrelation</a></li>
<li class="chapter" data-level="12.6" data-path="residualanalysis.html"><a href="residualanalysis.html#Heteroscedasticity"><i class="fa fa-check"></i><b>12.6</b> Heteroscedasticity</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lmer.html"><a href="lmer.html"><i class="fa fa-check"></i><b>13</b> Linear mixed effect models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="lmer.html"><a href="lmer.html#background-1"><i class="fa fa-check"></i><b>13.1</b> Background</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="lmer.html"><a href="lmer.html#why-mixed-effects-models"><i class="fa fa-check"></i><b>13.1.1</b> Why Mixed Effects Models?</a></li>
<li class="chapter" data-level="13.1.2" data-path="lmer.html"><a href="lmer.html#random-factors-and-partial-pooling"><i class="fa fa-check"></i><b>13.1.2</b> Random Factors and Partial Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>14</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="glm.html"><a href="glm.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="glm.html"><a href="glm.html#summary-4"><i class="fa fa-check"></i><b>14.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="glmm.html"><a href="glmm.html"><i class="fa fa-check"></i><b>15</b> Generalized linear mixed models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="glmm.html"><a href="glmm.html#introduction-4"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="glmm.html"><a href="glmm.html#summary-5"><i class="fa fa-check"></i><b>15.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>16</b> Posterior predictive model checking</a>
<ul>
<li class="chapter" data-level="16.1" data-path="model-checking.html"><a href="model-checking.html#introduction-5"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="model-checking.html"><a href="model-checking.html#summary-6"><i class="fa fa-check"></i><b>16.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>17</b> Model comparison and multimodel inference</a>
<ul>
<li class="chapter" data-level="17.1" data-path="model-comparison.html"><a href="model-comparison.html#introduction-6"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="model-comparison.html"><a href="model-comparison.html#summary-7"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="stan.html"><a href="stan.html"><i class="fa fa-check"></i><b>18</b> MCMC using Stan</a>
<ul>
<li class="chapter" data-level="18.1" data-path="stan.html"><a href="stan.html#background-2"><i class="fa fa-check"></i><b>18.1</b> Background</a></li>
<li class="chapter" data-level="18.2" data-path="stan.html"><a href="stan.html#install-rstan"><i class="fa fa-check"></i><b>18.2</b> Install <code>rstan</code></a></li>
<li class="chapter" data-level="18.3" data-path="stan.html"><a href="stan.html#firststanmod"><i class="fa fa-check"></i><b>18.3</b> Writing a Stan model</a></li>
<li class="chapter" data-level="18.4" data-path="stan.html"><a href="stan.html#run-stan-from-r"><i class="fa fa-check"></i><b>18.4</b> Run Stan from R</a></li>
<li class="chapter" data-level="" data-path="stan.html"><a href="stan.html#further-reading-3"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>19</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ridge-regression.html"><a href="ridge-regression.html#introduction-7"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="SEM.html"><a href="SEM.html"><i class="fa fa-check"></i><b>20</b> Structural equation models</a>
<ul>
<li class="chapter" data-level="20.1" data-path="SEM.html"><a href="SEM.html#introduction-8"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="spatial-glmm.html"><a href="spatial-glmm.html"><i class="fa fa-check"></i><b>21</b> Modeling spatial data using GLMM</a>
<ul>
<li class="chapter" data-level="21.1" data-path="spatial-glmm.html"><a href="spatial-glmm.html#introduction-9"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="spatial-glmm.html"><a href="spatial-glmm.html#summary-8"><i class="fa fa-check"></i><b>21.2</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>III ECOLOGICAL MODELS</b></span></li>
<li class="chapter" data-level="22" data-path="PART-III.html"><a href="PART-III.html"><i class="fa fa-check"></i><b>22</b> Introduction to PART III</a>
<ul>
<li class="chapter" data-level="22.1" data-path="PART-III.html"><a href="PART-III.html#model-notations"><i class="fa fa-check"></i><b>22.1</b> Model notations</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html"><i class="fa fa-check"></i><b>23</b> Zero-inflated Poisson Mixed Model</a>
<ul>
<li class="chapter" data-level="23.1" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#introduction-10"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#example-data"><i class="fa fa-check"></i><b>23.2</b> Example data</a></li>
<li class="chapter" data-level="23.3" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#model"><i class="fa fa-check"></i><b>23.3</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="dailynestsurv.html"><a href="dailynestsurv.html"><i class="fa fa-check"></i><b>24</b> Daily nest survival</a>
<ul>
<li class="chapter" data-level="24.1" data-path="dailynestsurv.html"><a href="dailynestsurv.html#background-3"><i class="fa fa-check"></i><b>24.1</b> Background</a></li>
<li class="chapter" data-level="24.2" data-path="dailynestsurv.html"><a href="dailynestsurv.html#models-for-estimating-daily-nest-survival"><i class="fa fa-check"></i><b>24.2</b> Models for estimating daily nest survival</a></li>
<li class="chapter" data-level="24.3" data-path="dailynestsurv.html"><a href="dailynestsurv.html#known-fate-model"><i class="fa fa-check"></i><b>24.3</b> Known fate model</a></li>
<li class="chapter" data-level="24.4" data-path="dailynestsurv.html"><a href="dailynestsurv.html#dailynestsurvstan"><i class="fa fa-check"></i><b>24.4</b> The Stan model</a></li>
<li class="chapter" data-level="24.5" data-path="dailynestsurv.html"><a href="dailynestsurv.html#prepare-data-and-run-stan"><i class="fa fa-check"></i><b>24.5</b> Prepare data and run Stan</a></li>
<li class="chapter" data-level="24.6" data-path="dailynestsurv.html"><a href="dailynestsurv.html#check-convergence"><i class="fa fa-check"></i><b>24.6</b> Check convergence</a></li>
<li class="chapter" data-level="24.7" data-path="dailynestsurv.html"><a href="dailynestsurv.html#look-at-results"><i class="fa fa-check"></i><b>24.7</b> Look at results</a></li>
<li class="chapter" data-level="24.8" data-path="dailynestsurv.html"><a href="dailynestsurv.html#known-fate-model-for-irregular-nest-controls"><i class="fa fa-check"></i><b>24.8</b> Known fate model for irregular nest controls</a></li>
<li class="chapter" data-level="" data-path="dailynestsurv.html"><a href="dailynestsurv.html#further-reading-4"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cjs-with-mix.html"><a href="cjs-with-mix.html"><i class="fa fa-check"></i><b>25</b> Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals</a>
<ul>
<li class="chapter" data-level="25.1" data-path="cjs-with-mix.html"><a href="cjs-with-mix.html#introduction-11"><i class="fa fa-check"></i><b>25.1</b> Introduction</a></li>
<li class="chapter" data-level="25.2" data-path="cjs-with-mix.html"><a href="cjs-with-mix.html#data-description"><i class="fa fa-check"></i><b>25.2</b> Data description</a></li>
<li class="chapter" data-level="25.3" data-path="cjs-with-mix.html"><a href="cjs-with-mix.html#model-description"><i class="fa fa-check"></i><b>25.3</b> Model description</a></li>
<li class="chapter" data-level="25.4" data-path="cjs-with-mix.html"><a href="cjs-with-mix.html#the-stan-code"><i class="fa fa-check"></i><b>25.4</b> The Stan code</a></li>
<li class="chapter" data-level="25.5" data-path="cjs-with-mix.html"><a href="cjs-with-mix.html#call-stan-from-r-check-convergence-and-look-at-results"><i class="fa fa-check"></i><b>25.5</b> Call Stan from R, check convergence and look at results</a></li>
</ul></li>
<li class="part"><span><b>IV APPENDICES</b></span></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis in Ecology with R and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lm" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Normal Linear Models</h1>
<div id="linear-regression" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> Linear Regression</h2>
<div id="background" class="section level3" number="11.1.1">
<h3><span class="header-section-number">11.1.1</span> Background</h3>
<p>Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression.</p>
<p>Typical questions that can be answered using linear regression are: How does <span class="math inline">\(y\)</span> change with changes in <span class="math inline">\(x\)</span>? How is y predicted from <span class="math inline">\(x\)</span>? An ordinary linear regression (i.e., one numeric <span class="math inline">\(x\)</span> and one numeric <span class="math inline">\(y\)</span> variable) can be represented by a scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. We search for the line that ﬁts best and describe how the observations scatter around this regression line (see Fig. <a href="lm.html#fig:figlm">11.1</a> for an example). The model formula of a simple linear regression with one continuous predictor variable <span class="math inline">\(x_i\)</span> (the subscript <span class="math inline">\(i\)</span> denotes the <span class="math inline">\(i=1,\dots,n\)</span> data points) is:</p>
<p><span class="math display" id="eq:lm">\[\begin{align} 
  \mu_i &amp;=\beta_0 + \beta_1 x_i \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.1}
\end{align}\]</span></p>
<p>While the first part of Equation <a href="lm.html#eq:lm">(11.1)</a> describes the regression line, the second part describes the differences between predicted values <span class="math inline">\(\mu_i\)</span> and observations <span class="math inline">\(y_i\)</span>. In other words: the observation <span class="math inline">\(y_i\)</span> stems from a normal distribution with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span> . The mean of the normal distribution, <span class="math inline">\(\mu_i\)</span> , equals the sum of the intercept (<span class="math inline">\(b_0\)</span> ) and the product of the slope (<span class="math inline">\(b_1\)</span>) and the continuous predictor value, <span class="math inline">\(x_i\)</span>.</p>
<p>The differences between observation <span class="math inline">\(y_i\)</span> and the predicted values <span class="math inline">\(\mu_i\)</span> are the residuals (i.e., <span class="math inline">\(\epsilon_i=y_i-\mu_i\)</span>). Equivalently to Equation <a href="lm.html#eq:lm">(11.1)</a>, the regression could thus be written as:</p>
<p><span class="math display" id="eq:lmalternativ">\[\begin{align} 
  y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i\\
  \epsilon_i &amp;\sim Norm(0, \sigma^2)
  \tag{11.2}
\end{align}\]</span></p>
<p>We prefer the notation in Equation <a href="lm.html#eq:lm">(11.1)</a> because, in this formula, the stochastic part (second row) is nicely separated from the deterministic part (first row) of the model, whereas, in the second notation <a href="lm.html#eq:lmalternativ">(11.2)</a> the ﬁrst row contains both stochastic and deterministic parts.</p>
<p>Using matrix notation equation <a href="lm.html#eq:lm">(11.1)</a> can also be written in one row:</p>
<p><span class="math display">\[\boldsymbol{y} \sim 
  Norm(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2\boldsymbol{I})\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{ I}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix (it transforms the variance parameter to a <span class="math inline">\(n \times n\)</span> matrix with its diagonal elements equal <span class="math inline">\(\sigma^2\)</span> ; <span class="math inline">\(n\)</span> is the sample size). The multiplication by <span class="math inline">\(\boldsymbol{ I}\)</span> is necessary because we use vector notation, <span class="math inline">\(\boldsymbol{y}\)</span> instead of <span class="math inline">\(y_{i}\)</span> . Here, <span class="math inline">\(\boldsymbol{y}\)</span> is the vector of all observations, whereas <span class="math inline">\(y_{i}\)</span> is a single observation, <span class="math inline">\(i\)</span>. When using vector notation, we can write the linear predictor of the model, <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> , as a multiplication of the vector of the model coefﬁcients</p>
<p><span class="math display">\[\boldsymbol{\beta} =
  \begin{pmatrix} 
    \beta_0 \\ 
    \beta_1 
  \end{pmatrix}\]</span></p>
<p>times the model matrix</p>
<p><span class="math display">\[\boldsymbol{X} = 
  \begin{pmatrix} 
      1     &amp; x_1   \\ 
      \dots &amp; \dots \\ 
      1     &amp; x_n
  \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(x_1 , \dots, x_n\)</span> are the observed values for the predictor variable, <span class="math inline">\(x\)</span>. The ﬁrst column of <span class="math inline">\(\boldsymbol{X}\)</span> contains only ones because the values in this column are multiplied with the intercept, <span class="math inline">\(\beta_0\)</span> . To the intercept, the product of the second element of <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\beta_1\)</span> , with each element in the second column of <span class="math inline">\(\boldsymbol{X}\)</span> is added to obtain the predicted value for each observation, <span class="math inline">\(\boldsymbol{\mu}\)</span>:</p>
<p><span class="math display" id="eq:lmmatrix">\[\begin{align} 
\boldsymbol{\beta X}=
\begin{pmatrix} 
    \beta_0 \\ 
    \beta_1 
\end{pmatrix}
\times
\begin{pmatrix} 
      1     &amp; x_1   \\ 
      \dots &amp; \dots \\ 
      1     &amp; x_n
  \end{pmatrix} =
  \begin{pmatrix} 
    \beta_0 + \beta_1x_1 \\ 
    \dots \\
    \beta_0 + \beta_1x_n
\end{pmatrix}=
\begin{pmatrix} 
    \hat{y}_1 \\
  \dots \\
    \hat{y}_n 
\end{pmatrix} =
\boldsymbol{\mu}
\tag{11.3}
\end{align}\]</span></p>
</div>
<div id="fitting-a-linear-regression-in-r" class="section level3" number="11.1.2">
<h3><span class="header-section-number">11.1.2</span> Fitting a Linear Regression in R</h3>
<p>In Equation <a href="lm.html#eq:lm">(11.1)</a>, the predicted (synonym: fitted) values <span class="math inline">\(\mu_i\)</span> are directly deﬁned by the model coefﬁcients, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> . Therefore, when we can estimate <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span> , and <span class="math inline">\(\sigma^2\)</span>, the model is fully deﬁned . The last parameter <span class="math inline">\(\sigma^2\)</span> describes how the observations scatter around the regression line and relies on the assumption that the residuals are normally distributed. The estimates for the model parameters of a linear regression are obtained by searching for the best ﬁtting regression line. To do so, we search for the regression line that minimizes the sum of the squared residuals. This model ﬁtting method is called the least-squares method, abbreviated as LS. It has a very simple solution using matrix algebra <span class="citation">(see e.g., <a href="referenzen.html#ref-Aitkin.2009" role="doc-biblioref">Aitkin et al. 2009</a>)</span>.</p>
<p>Note that we can apply LS techniques independent of whether we use a Bayesian or frequentist framework to draw inference. In Bayesian statistics, Equation <a href="lm.html#eq:lm">(11.1)</a> is called the data model, because it describes mathematically the process that has (or, better, that we think has) produced the data. This nomenclature also helps to distinguish data models from models for parameters such as prior distributions.</p>
<p>The least-squares estimates for the model parameters of a linear regression are obtained in R using the function <code>lm</code>. For illustration, we ﬁrst simulate a data set and then ﬁt a linear regression to these simulated data. The advantage of simulating data is that the following analyses can be reproduced without having to read data into R.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="lm.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">34</span>)            <span class="co"># set a seed for the random number generator</span></span>
<span id="cb40-2"><a href="lm.html#cb40-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span>                 <span class="co"># sample size</span></span>
<span id="cb40-3"><a href="lm.html#cb40-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">5</span>              <span class="co"># standard deviation of the residuals</span></span>
<span id="cb40-4"><a href="lm.html#cb40-4" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">2</span>                 <span class="co"># intercept</span></span>
<span id="cb40-5"><a href="lm.html#cb40-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fl">0.7</span>               <span class="co"># slope</span></span>
<span id="cb40-6"><a href="lm.html#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="lm.html#cb40-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">10</span>, <span class="dv">30</span>)   <span class="co"># sample values of the covariate</span></span>
<span id="cb40-8"><a href="lm.html#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="lm.html#cb40-9" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x</span>
<span id="cb40-10"><a href="lm.html#cb40-10" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, mu, <span class="at">sd =</span> sigma)</span>
<span id="cb40-11"><a href="lm.html#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="lm.html#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Save data in table</span></span>
<span id="cb40-13"><a href="lm.html#cb40-13" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span></code></pre></div>
<p>Then, we ﬁt a linear regression to the data to obtain the results of the three parameters of the linear regression, that is intercept, slope and residual standard deviation.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="lm.html#cb41-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span>  <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dat)</span>
<span id="cb41-2"><a href="lm.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span></code></pre></div>
<pre><code>## (Intercept)           x 
##   2.0049517   0.6880415</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="lm.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 5.04918</code></pre>
<p>The object “mod” produced by <code>lm</code> contains the estimates for the intercept, <span class="math inline">\(\beta_0\)</span> , and the slope, <span class="math inline">\(\beta_1\)</span>. The residual standard deviation <span class="math inline">\(\sigma^2\)</span> is extracted using the function <code>summary</code>. We can show the result of the linear regression as a line in a scatter plot with the covariate (<code>x</code>) on the x-axis and the observations (<code>y</code>) on the y-axis (Fig. <a href="lm.html#fig:figlm">11.1</a>).</p>
<div class="figure"><span id="fig:figlm"></span>
<img src="2.03-lm_files/figure-html/figlm-1.png" alt="Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The ﬁtted values lie where the orange dotted lines touch the blue regression line." width="672" />
<p class="caption">
Figure 11.1: Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The ﬁtted values lie where the orange dotted lines touch the blue regression line.
</p>
</div>
<p>Conclusions drawn from a model depend on the model assumptions. When model assumptions are violated, estimates usually are biased and inappropriate conclusions can be drawn. We devote Chapter <a href="residualanalysis.html#residualanalysis">12</a> to the assessment of model assumptions, given its importance.</p>
</div>
<div id="drawing-conclusions" class="section level3" number="11.1.3">
<h3><span class="header-section-number">11.1.3</span> Drawing Conclusions</h3>
<p>To answer the question about how strongly <span class="math inline">\(y\)</span> is related to <span class="math inline">\(x\)</span>, or to predict <span class="math inline">\(y\)</span> from <span class="math inline">\(x\)</span>, and because we usually draw inference in a Bayesian framework, we are interested in the joint posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> (vector that contains <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> ) and <span class="math inline">\(\sigma^2\)</span> , the residual variance. The function <code>sim</code> does this for us. To somewhat demystify the <code>sim</code> function we brieﬂy explain what <code>sim</code> does. The principle is to ﬁrst draw a random value from the marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span>, and then to draw random values from the conditional posterior distribution for <span class="math inline">\(\boldsymbol{\beta}\)</span> <span class="citation">(<a href="referenzen.html#ref-Gelman.2014" role="doc-biblioref">A. Gelman et al. 2014a</a>)</span>.</p>
<p>The conditional posterior distribution of the parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})\)</span> is the posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> given a speciﬁc value for <span class="math inline">\(\sigma^2\)</span> . This conditional distribution can be analytically derived. With ﬂat prior distributions, it is a uni- or multivariate normal distribution <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})=Norm(\boldsymbol{\hat{\beta}},V_\beta,\sigma^2)\)</span> with:</p>
<p><span class="math display" id="eq:sim">\[\begin{align}
  \boldsymbol{\hat{\beta}=(\boldsymbol{X^TX})^{-1}X^Ty}
  \tag{11.4}
\end{align}\]</span></p>
<p>and <span class="math inline">\(V_\beta = (\boldsymbol{X^T X})^{-1}\)</span> . For models with the normal error distribution, the LS estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span> (given by Eq. <a href="lm.html#eq:sim">(11.4)</a>) equal the maximum likelihood (ML) estimates ==(see Chapter 5)==.</p>
<p>The marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span> is independent of speciﬁc values of <span class="math inline">\(\boldsymbol{\beta}\)</span>. It is, for ﬂat prior distributions, an inverse chi-square distribution <span class="math inline">\(p(\sigma^2|\boldsymbol{y,X})=Inv-\chi^2(n-k,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2 = \frac{1}{n-k}(\boldsymbol{y}-\boldsymbol{X,\hat{\beta}})^T(\boldsymbol{y}-\boldsymbol{X,\hat{\beta}})\)</span>, and <span class="math inline">\(k\)</span> is the number of parameters. The marginal posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> can be obtained by integrating the conditional posterior distribution <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})=Norm(\boldsymbol{\hat{\beta}},V_\beta\sigma^2)\)</span> over the distribution of <span class="math inline">\(\sigma^2\)</span> . This results in a uni- or multivariate <span class="math inline">\(t\)</span>-distribution.</p>
<p>However, it is not necessary to do this analytically. Using the function <code>sim</code> from the package, we can draw samples from <span class="math inline">\(p(\sigma^2|\boldsymbol{y,X})\)</span> and describe the marginal posterior distributions of <span class="math inline">\(\boldsymbol{\beta}\)</span> using the simulated values.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="lm.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb45-2"><a href="lm.html#cb45-2" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb45-3"><a href="lm.html#cb45-3" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim =</span> nsim)</span></code></pre></div>
<p>The function <code>sim</code> simulates (in our example) 1000 values from the joint posterior distribution of the three model parameters <span class="math inline">\(\beta_0\)</span> , <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span>. These simulated values are shown in Figure <a href="lm.html#fig:simfirstexample">11.2</a>.</p>
<div class="figure"><span id="fig:simfirstexample"></span>
<img src="2.03-lm_files/figure-html/simfirstexample-1.png" alt="Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters." width="768" />
<p class="caption">
Figure 11.2: Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters.
</p>
</div>
<p>The posterior distributions describe the range of plausible parameter values given the data and the model. They express our uncertainty about the model parameters; they show what we know about the model parameters after having looked at the data and given the model is realistic.</p>
<p>The 2.5% and 97.5% quantiles of the marginal posterior distributions can be used as 95% credible intervals of the model parameters. The function <code>coef</code> extracts the simulated values for the beta coefﬁcients, returning a matrix with <em>nsim</em> rows and the number of columns corresponding to the number of parameters. In our example, the ﬁrst column contains the simulated values from the posterior distribution of the intercept and the second column contains values from the posterior distribution of the slope. The “2” in the second argument of the apply-function (see Chapter <a href="#rmisc"><strong>??</strong></a>) indicates that the <code>quantile</code> function is applied columnwise.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="lm.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(<span class="at">X =</span> <span class="fu">coef</span>(bsim), <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> quantile, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb46-2"><a href="lm.html#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       (Intercept)    x
## 2.5%        -2.95 0.44
## 97.5%        7.17 0.92</code></pre>
<p>We also can calculate a credible interval of the estimated residual standard deviation, <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="lm.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>sigma, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb48-2"><a href="lm.html#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   4.2   6.3</code></pre>
<p>Using Bayesian methods allows us to get a posterior probability for speciﬁc hypotheses, such as “The slope parameter is larger than 1” or “The slope parameter is larger than 0.5.” These probabilities are the proportion of simulated values from the posterior distribution that are larger than 1 and 0.5, respectively.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="lm.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="dv">1</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.008</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="lm.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fl">0.5</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.936</code></pre>
<p>From this, there is very little evidence in the data that the slope is larger than 1, but we are quite conﬁdent that the slope is larger than 0.5 (assuming that our model is realistic).</p>
<p>We often want to show the effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> graphically, with information about the uncertainty of the parameter estimates included in the graph. To draw such effect plots, we use the simulated values from the posterior distribution of the model parameters. From the deterministic part of the model, we know the regression line <span class="math inline">\(\mu = \beta_0 + \beta_1 x_i\)</span>. The simulation from the joint posterior distribution of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We can draw these regression lines in an x-y plot (scatter plot) to show the uncertainty in the regression line estimation (Fig. <a href="lm.html#fig:figlmer1">11.3</a>, left). Note, that in this case it is not advisable to use <code>ggplot</code> because we draw many lines in one plot, which makes <code>ggplot</code> rather slow.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="lm.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb54-2"><a href="lm.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">las =</span> <span class="dv">1</span>, </span>
<span id="cb54-3"><a href="lm.html#cb54-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Covariate (x)&quot;</span>, </span>
<span id="cb54-4"><a href="lm.html#cb54-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Dependend variable (y)&quot;</span>)</span>
<span id="cb54-5"><a href="lm.html#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {</span>
<span id="cb54-6"><a href="lm.html#cb54-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="fu">coef</span>(bsim)[i,<span class="dv">1</span>], <span class="fu">coef</span>(bsim)[i,<span class="dv">2</span>], <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.05</span>))</span>
<span id="cb54-7"><a href="lm.html#cb54-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:figlmer1"></span>
<img src="2.03-lm_files/figure-html/figlmer1-1.png" alt="Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line." width="672" />
<p class="caption">
Figure 11.3: Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line.
</p>
</div>
<p>A more convenient way to show uncertainty is to draw the 95% credible interval, CrI, of the regression line. To this end, we ﬁrst deﬁne new x-values for which we would like to have the ﬁtted values (about 100 points across the range of x will produce smooth-looking lines when connected by line segments). We save these new x-values within the new tibble <code>newdat</code>. Then, we create a new model matrix that contains these new x-values (<code>newmodmat</code>) using the function <code>model.matrix</code>. We then calculate the 1000 ﬁtted values for each element of the new x (one value for each of the 1000 simulated regressions, Fig. <a href="lm.html#fig:figlmer1">11.3</a>), using matrix multiplication (%*%). We save these values in the matrix “ﬁtmat.” Finally, we extract the 2.5% and 97.5% quantiles for each x-value from ﬁtmat, and draw the lines for the lower and upper limits of the credible interval (Fig. <a href="lm.html#fig:figlmer2">11.4</a>).</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="lm.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate 95% credible interval</span></span>
<span id="cb55-2"><a href="lm.html#cb55-2" aria-hidden="true" tabindex="-1"></a>newdat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="dv">30</span>, <span class="at">by =</span> <span class="fl">0.1</span>))</span>
<span id="cb55-3"><a href="lm.html#cb55-3" aria-hidden="true" tabindex="-1"></a>newmodmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>( <span class="sc">~</span> x, <span class="at">data =</span> newdat)</span>
<span id="cb55-4"><a href="lm.html#cb55-4" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> nsim, <span class="at">nrow =</span> <span class="fu">nrow</span>(newdat))</span>
<span id="cb55-5"><a href="lm.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {fitmat[,i] <span class="ot">&lt;-</span> newmodmat <span class="sc">%*%</span> <span class="fu">coef</span>(bsim)[i,]}</span>
<span id="cb55-6"><a href="lm.html#cb55-6" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>CrI_lo <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.025</span>)</span>
<span id="cb55-7"><a href="lm.html#cb55-7" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>CrI_up <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.975</span>)</span>
<span id="cb55-8"><a href="lm.html#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="lm.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make plot</span></span>
<span id="cb55-10"><a href="lm.html#cb55-10" aria-hidden="true" tabindex="-1"></a>regplot <span class="ot">&lt;-</span> </span>
<span id="cb55-11"><a href="lm.html#cb55-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb55-12"><a href="lm.html#cb55-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb55-13"><a href="lm.html#cb55-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb55-14"><a href="lm.html#cb55-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> CrI_lo), <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb55-15"><a href="lm.html#cb55-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> CrI_up), <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb55-16"><a href="lm.html#cb55-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Covariate (x)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Dependend variable (y)&quot;</span>)</span>
<span id="cb55-17"><a href="lm.html#cb55-17" aria-hidden="true" tabindex="-1"></a>regplot</span></code></pre></div>
<div class="figure"><span id="fig:figlmer2"></span>
<img src="2.03-lm_files/figure-html/figlmer2-1.png" alt="Regression with 95% credible interval of the posterior distribution of the ﬁtted values." width="672" />
<p class="caption">
Figure 11.4: Regression with 95% credible interval of the posterior distribution of the ﬁtted values.
</p>
</div>
<p>The interpretation of the 95% credible interval is straightforward: We are 95% sure that the true regression line is within the credible interval. As always, this interpretation is only true if the the model is correct. The larger the sample size, the narrower the interval, because each additional data point increases information about the true regression line.</p>
<p>The credible interval measures uncertainty of the regression line, but it does not describe how new observations would scatter around the regression line. If we want to describe where future observations will be, we have to report the posterior predictive distribution. We can get a sample of random draws from the posterior predictive distribution <span class="math inline">\(\hat{y}|\boldsymbol{\beta},\sigma^2,\boldsymbol{X}\sim Norm( \boldsymbol{X \beta, \sigma^2})\)</span> using the simulated joint posterior distributions of the model parameters, thus taking the uncertainty of the parameter estimates into account. We draw a new <span class="math inline">\(\hat{y}\)</span>-value from <span class="math inline">\(Norm( \boldsymbol{X \beta, \sigma^2})\)</span> for each simulated set of model parameters. Then, we can visualize the 2.5% and 97.5% quantiles of this distribution for each new x-value.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="lm.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># increase number of simulation to procude smooth lines of the posterior</span></span>
<span id="cb56-2"><a href="lm.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictive distribution</span></span>
<span id="cb56-3"><a href="lm.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">34</span>)</span>
<span id="cb56-4"><a href="lm.html#cb56-4" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb56-5"><a href="lm.html#cb56-5" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span>
<span id="cb56-6"><a href="lm.html#cb56-6" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat))</span>
<span id="cb56-7"><a href="lm.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) fitmat[,i] <span class="ot">&lt;-</span> newmodmat<span class="sc">%*%</span><span class="fu">coef</span>(bsim)[i,]</span>
<span id="cb56-8"><a href="lm.html#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="lm.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare matrix for simulated new data</span></span>
<span id="cb56-10"><a href="lm.html#cb56-10" aria-hidden="true" tabindex="-1"></a>newy <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat)) </span>
<span id="cb56-11"><a href="lm.html#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="lm.html#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for each simulated ﬁtted value, simulate one new y-value</span></span>
<span id="cb56-13"><a href="lm.html#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {</span>
<span id="cb56-14"><a href="lm.html#cb56-14" aria-hidden="true" tabindex="-1"></a>  newy[,i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">nrow</span>(newdat), <span class="at">mean =</span> fitmat[,i], <span class="at">sd =</span> bsim<span class="sc">@</span>sigma[i])</span>
<span id="cb56-15"><a href="lm.html#cb56-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-16"><a href="lm.html#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="lm.html#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate 2.5% and 97.5% quantiles</span></span>
<span id="cb56-18"><a href="lm.html#cb56-18" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>pred_lo <span class="ot">&lt;-</span> <span class="fu">apply</span>(newy, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.025</span>)</span>
<span id="cb56-19"><a href="lm.html#cb56-19" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>pred_up <span class="ot">&lt;-</span> <span class="fu">apply</span>(newy, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.975</span>)</span>
<span id="cb56-20"><a href="lm.html#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="lm.html#cb56-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the posterior predictive distribution to plot</span></span>
<span id="cb56-22"><a href="lm.html#cb56-22" aria-hidden="true" tabindex="-1"></a>regplot <span class="sc">+</span></span>
<span id="cb56-23"><a href="lm.html#cb56-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred_lo), <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb56-24"><a href="lm.html#cb56-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred_up), <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figlmer3"></span>
<img src="2.03-lm_files/figure-html/figlmer3-1.png" alt="Regression line with 95% credible interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines." width="672" />
<p class="caption">
Figure 11.5: Regression line with 95% credible interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines.
</p>
</div>
<p>Future observations are expected to be within the interval deﬁned by the broken lines in Fig. <a href="lm.html#fig:figlmer3">11.5</a> with a probability of 0.95. Increasing sample size will not necessarily give a narrower predictive distribution because the predictive distribution also depends on the residual variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The way we produced Fig. <a href="lm.html#fig:figlmer3">11.5</a> is somewhat tedious compared to how easy we could have obtained the same ﬁgure using frequentist methods: <code>predict(mod, newdata = newdat, interval = "prediction")</code> would have produced the y-values for the lower and upper lines in Fig. <a href="lm.html#fig:figlmer3">11.5</a> in one R-code line. However, once we have a simulated sample of the posterior predictive distribution, we have much more information than is contained in the frequentist prediction interval. For example, we could give an estimate for the proportion of observations greater than 20, given <span class="math inline">\(x = 25\)</span>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="lm.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(newy[newdat<span class="sc">$</span>x <span class="sc">==</span> <span class="dv">25</span>, ] <span class="sc">&gt;</span> <span class="dv">20</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.44504</code></pre>
<p>Thus, we expect 44% of future observations with <span class="math inline">\(x = 25\)</span> to be higher than 20. We can extract similar information for any relevant threshold value.</p>
<p>Another reason to learn the more complicated R code we presented here, compared to the frequentist methods, is that, for more complicated models such as mixed models, the frequentist methods to obtain conﬁdence intervals of ﬁtted values are much more complicated than the Bayesian method just presented. The latter can be used with only slight adaptations for mixed models and also for generalized linear mixed models.</p>
</div>
<div id="frequentist-results" class="section level3" number="11.1.4">
<h3><span class="header-section-number">11.1.4</span> Frequentist Results</h3>
<p>The solution for <span class="math inline">\(\boldsymbol{\beta}\)</span> is the Equation <a href="lm.html#eq:lmmatrix">(11.3)</a>. Most statistical software, including R, return an estimated frequentist standard error for each <span class="math inline">\(\beta_k\)</span>. We extract these standard errors together with the estimates for the model parameters using the <code>summary</code> function.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="lm.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.5777  -3.6280  -0.0532   3.9873  12.1374 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   2.0050     2.5349   0.791       0.433    
## x             0.6880     0.1186   5.800 0.000000507 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.049 on 48 degrees of freedom
## Multiple R-squared:  0.412,  Adjusted R-squared:  0.3998 
## F-statistic: 33.63 on 1 and 48 DF,  p-value: 0.0000005067</code></pre>
<p>The summary output ﬁrst gives a rough summary of the residual distribution. However, we will do more rigorous residual analyses in Chapter <a href="residualanalysis.html#residualanalysis">12</a>. The estimates of the model coefﬁcients follow. The column “Estimate” contains the estimates for the intercept <span class="math inline">\(\beta_0\)</span> and the slope <span class="math inline">\(\beta_1\)</span> . The column “Std. Error” contains the estimated (frequentist) standard errors of the estimates. The last two columns contain the t-value and the p-value of the classical t-test for the null hypothesis that the coefﬁcient equals zero. The last part of the summary output gives the parameter <span class="math inline">\(\sigma\)</span> of the model, named “residual standard error” and the residual degrees of freedom.</p>
<p>We try to avoid the name “residual standard error” and use “sigma” instead, because <span class="math inline">\(\sigma\)</span> is not a measurement of uncertainty of a parameter estimate like the standard errors of the model coefﬁcients are. <span class="math inline">\(\sigma\)</span> is a model parameter that describes how the observations scatter around the ﬁtted values, that is, it is a standard deviation. It is independent of sample size, whereas the standard errors of the estimates for the model parameters will decrease with increasing sample size. Such a standard error of the estimate of <span class="math inline">\(\sigma\)</span>, however, is not given in the summary output. Note that, by using Bayesian methods, we could easily obtain the standard error of the estimated <span class="math inline">\(\sigma\)</span> by calculating the standard deviation of the posterior distribution of <span class="math inline">\(\sigma\)</span>. The <span class="math inline">\(R^2\)</span> and the adjusted <span class="math inline">\(R^2\)</span> are explained in Section == “Posterior Predictive Model Checking and Proportion of Explained Variance”==.</p>
</div>
</div>
<div id="regression-variants-anova-ancova-and-multiple-regression" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Regression Variants: ANOVA, ANCOVA, and Multiple Regression</h2>
<div id="one-way-anova" class="section level3" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> One-Way ANOVA</h3>
<p>The aim of analysis of variance (ANOVA) is to compare means of an outcome variable y between different groups (categorical variables). To do so in the frequentist’s framework, variances between and within the groups are compared (hence the name “analysis of variance”). If the variance between the group means is larger than expected by chance , we reject the null hypothesis of no differences between the groups. When doing an ANOVA in a Bayesian way, inference is based on the posterior distributions of the group means and the differences between the group means.</p>
<p>One-way ANOVA means that we only have one explanatory variable (a factor). We illustrate the one-way ANOVA based on an example of simulated data (Fig. <a href="lm.html#fig:figanova">11.6</a>). We have measured weights of 30 virtual individuals for each of 3 groups. Possible research questions could be: How big are the differences between the group means? Are individuals from group 2 heavier than the ones from group 1? Which group mean is higher than 7.5 g?</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="lm.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># settings for the simulation</span></span>
<span id="cb61-2"><a href="lm.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">626436</span>)</span>
<span id="cb61-3"><a href="lm.html#cb61-3" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">12</span>        <span class="co"># mean of group 1 (reference group)</span></span>
<span id="cb61-4"><a href="lm.html#cb61-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">2</span>      <span class="co"># residual standard deviation</span></span>
<span id="cb61-5"><a href="lm.html#cb61-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="dv">3</span>         <span class="co"># difference between group 1 and group 2</span></span>
<span id="cb61-6"><a href="lm.html#cb61-6" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">5</span>        <span class="co"># difference between group 1 and group 3</span></span>
<span id="cb61-7"><a href="lm.html#cb61-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">90</span>         <span class="co"># sample size</span></span>
<span id="cb61-8"><a href="lm.html#cb61-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-9"><a href="lm.html#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co"># generate data</span></span>
<span id="cb61-10"><a href="lm.html#cb61-10" aria-hidden="true" tabindex="-1"></a>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;group 1&quot;</span>,<span class="st">&quot;group 2&quot;</span>, <span class="st">&quot;group 3&quot;</span>), <span class="at">each=</span><span class="dv">30</span>))</span>
<span id="cb61-11"><a href="lm.html#cb61-11" aria-hidden="true" tabindex="-1"></a>simresid <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span>sigma)  <span class="co"># simulate residuals</span></span>
<span id="cb61-12"><a href="lm.html#cb61-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> b0 <span class="sc">+</span> </span>
<span id="cb61-13"><a href="lm.html#cb61-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(group<span class="sc">==</span><span class="st">&quot;group 2&quot;</span>) <span class="sc">*</span> b1 <span class="sc">+</span> </span>
<span id="cb61-14"><a href="lm.html#cb61-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(group<span class="sc">==</span><span class="st">&quot;group 3&quot;</span>) <span class="sc">*</span> b2 <span class="sc">+</span> </span>
<span id="cb61-15"><a href="lm.html#cb61-15" aria-hidden="true" tabindex="-1"></a>  simresid</span>
<span id="cb61-16"><a href="lm.html#cb61-16" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(y, group)</span>
<span id="cb61-17"><a href="lm.html#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="lm.html#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co"># make figure</span></span>
<span id="cb61-19"><a href="lm.html#cb61-19" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span> </span>
<span id="cb61-20"><a href="lm.html#cb61-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> group, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb61-21"><a href="lm.html#cb61-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-22"><a href="lm.html#cb61-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Weight (g)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-23"><a href="lm.html#cb61-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figanova"></span>
<img src="2.03-lm_files/figure-html/figanova-1.png" alt="Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box." width="672" />
<p class="caption">
Figure 11.6: Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box.
</p>
</div>
<p>An ANOVA is a linear regression with a categorical predictor variable instead of a continuous one. The categorical predictor variable with <span class="math inline">\(k\)</span> levels is (as a default in R) transformed to <span class="math inline">\(k-1\)</span> indicator variables. An indicator variable is a binary variable containing 0 and 1 where 1 indicates a speciﬁc level (a category of a nominal variable). Often, one indicator variable is constructed for every level except for the reference level. In our example, the categorical variable is group (<span class="math inline">\(g\)</span>) with the three levels “group 1,” “group 2,” and “group 3” (<span class="math inline">\(k = 3\)</span>). Group 1 is taken as the reference level, and for each of the other two groups an indicator variable is constructed, <span class="math inline">\(I(g_i = 2)\)</span> and <span class="math inline">\(I(g_i = 3)\)</span>. We can write the model as a formula:</p>
<p><span class="math display" id="eq:anova">\[\begin{align} 
  \mu_i &amp;=\beta_0 + \beta_1 I(g_i=2) + \beta_1 I(g_i=3) \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.5}
\end{align}\]</span></p>
<p>where <span class="math inline">\(yi\)</span> is the <span class="math inline">\(i\)</span>-th observation (weight measurement for individual i in our example), and <span class="math inline">\(\beta_{0,1,2}\)</span> are the model coefﬁcients. The residual variance is <span class="math inline">\(\sigma^2\)</span>. The model coefﬁcients <span class="math inline">\(\beta_{0,1,2}\)</span> constitute the deterministic part of the model. From the model formula it follows that the group means, <span class="math inline">\(m_g\)</span>, are:</p>
<p><span class="math display" id="eq:anovamw">\[\begin{align} 
  m_1 &amp;=\beta_0 \\
  m_2 &amp;=\beta_0 + \beta_1 \\
  m_3 &amp;=\beta_0 + \beta_2 \\
  \tag{11.6}
\end{align}\]</span></p>
<p>There are other possibilities to describe three group means with three parameters, for example:</p>
<p><span class="math display" id="eq:anovamwalt">\[\begin{align} 
  m_1 &amp;=\beta_1 \\
  m_2 &amp;=\beta_2 \\
  m_3 &amp;=\beta_3 \\
  \tag{11.7}
\end{align}\]</span></p>
<p>In this case, the model formula would be:</p>
<p><span class="math display" id="eq:anovaalt">\[\begin{align} 
  \mu_i &amp;= \beta_1 I(g_i=1) + \beta_2 I(g_i=2) + \beta_3 I(g_i=3) \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.8}
\end{align}\]</span></p>
<p>The way the group means are described is called the parameterization of the model. Different statistical softwares use different parameterizations. The parameterization used by R by default is the one shown in Equation <a href="lm.html#eq:anova">(11.5)</a>. R automatically takes the ﬁrst level as the reference (the ﬁrst level is the ﬁrst one alphabetically unless the user deﬁnes a different order for the levels). The mean of the ﬁrst group (i.e., of the ﬁrst factor level) is the intercept, <span class="math inline">\(b_0\)</span> , of the model. The mean of another factor level is obtained by adding, to the intercept, the estimate of the corresponding parameter (which is the difference from the reference group mean). R calls this parameterization “treatment contrasts.”</p>
<p>The parameterization of the model is deﬁned by the model matrix. In the case of a one-way ANOVA, there are as many columns in the model matrix as there are factor levels (i.e., groups); thus there are k factor levels and k model coefﬁcients. Recall from Equation <a href="lm.html#eq:lmmatrix">(11.3)</a> that for each observation, the entry in the <span class="math inline">\(j\)</span>-th column of the model matrix is multiplied by the <span class="math inline">\(j\)</span>-th element of the model coefﬁcients and the <span class="math inline">\(k\)</span> products are summed to obtain the ﬁtted values. For a data set with <span class="math inline">\(n = 5\)</span> observations of which the ﬁrst two are from group 1, the third from group 2, and the last two from group 3, the model matrix used for the parameterization described in Equation <a href="lm.html#eq:anovamw">(11.6)</a> is</p>
<p><span class="math display">\[\begin{align} 
\boldsymbol{X}=
\begin{pmatrix} 
      1 &amp; 0 &amp; 0 \\ 
      1 &amp; 0 &amp; 0 \\ 
      1 &amp; 1 &amp; 0 \\ 
      1 &amp; 0 &amp; 1 \\ 
      1 &amp; 0 &amp; 1 \\ 
  \end{pmatrix}
\end{align}\]</span></p>
<p>If parameterization of Equation <a href="lm.html#eq:anovamwalt">(11.7)</a> were used,</p>
<p><span class="math display">\[\begin{align} 
\boldsymbol{X}=
\begin{pmatrix} 
      1 &amp; 0 &amp; 0 \\ 
      1 &amp; 0 &amp; 0 \\ 
      0 &amp; 1 &amp; 0 \\ 
      0 &amp; 0 &amp; 1 \\ 
      0 &amp; 0 &amp; 1 \\ 
  \end{pmatrix}
\end{align}\]</span></p>
<p>Other possibilities of model parameterizations, particularly for ordered factors, are introduced in Section <a href="lm.html#orderedfactors">11.2.8</a>.</p>
<p>To obtain the parameter estimates for model parameterized according to Equation <a href="lm.html#eq:anovamw">(11.6)</a> we ﬁt the model in R:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="lm.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb62-2"><a href="lm.html#cb62-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>group)  </span>
<span id="cb62-3"><a href="lm.html#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="lm.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># parameter estimates </span></span>
<span id="cb62-5"><a href="lm.html#cb62-5" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ group)
## 
## Coefficients:
##  (Intercept)  groupgroup 2  groupgroup 3  
##       12.367         2.215        -5.430</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="lm.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 1.684949</code></pre>
<p>The “Intercept” is <span class="math inline">\(\beta_0\)</span>. The other coefﬁcients are named with the factor name (“group”) and the factor level (either “group 2” or “group 3”). These are <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> , respectively. Before drawing conclusions from an R output we need to examine whether the model assumptions are met, that is, we need to do a residual analysis as described in Chapter <a href="residualanalysis.html#residualanalysis">12</a>.</p>
<p>Different questions can be answered using the above ANOVA: What are the group means? What is the difference in the means between group 1 and group 2? What is the difference between the means of the heaviest and lightest group? In a Bayesian framework we can directly assess how strongly the data support the hypothesis that the mean of the group 2 is larger than the mean of group 1. We ﬁrst simulate from the posterior distribution of the model parameters.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="lm.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb66-2"><a href="lm.html#cb66-2" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb66-3"><a href="lm.html#cb66-3" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span></code></pre></div>
<p>Then we obtain the posterior distributions for the group means according to the parameterization of the model formula (Equation <a href="lm.html#eq:anovamw">(11.6)</a>).</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="lm.html#cb67-1" aria-hidden="true" tabindex="-1"></a>m.g1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>]  </span>
<span id="cb67-2"><a href="lm.html#cb67-2" aria-hidden="true" tabindex="-1"></a>m.g2 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] </span>
<span id="cb67-3"><a href="lm.html#cb67-3" aria-hidden="true" tabindex="-1"></a>m.g3 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(bsim)[,<span class="dv">3</span>] </span></code></pre></div>
<p>The histograms of the simulated values from the posterior distributions of the three means are given in Fig. <a href="lm.html#fig:figanovares">11.7</a>. The three means are well separated and, based on our data, we are conﬁdent that the group means differ. From these simulated posterior distributions we obtain the means and use the 2.5% and 97.5% quantiles as limits of the 95% credible intervals (Fig. <a href="lm.html#fig:figanovares">11.7</a>, right).</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="lm.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save simulated values from posterior distribution in  tibble</span></span>
<span id="cb68-2"><a href="lm.html#cb68-2" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> </span>
<span id="cb68-3"><a href="lm.html#cb68-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="st">`</span><span class="at">group 1</span><span class="st">`</span> <span class="ot">=</span> m.g1, <span class="st">`</span><span class="at">group 2</span><span class="st">`</span> <span class="ot">=</span> m.g2, <span class="st">`</span><span class="at">group 3</span><span class="st">`</span> <span class="ot">=</span> m.g3) <span class="sc">%&gt;%</span> </span>
<span id="cb68-4"><a href="lm.html#cb68-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="st">&quot;groups&quot;</span>, <span class="st">&quot;Group means&quot;</span>) </span>
<span id="cb68-5"><a href="lm.html#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="lm.html#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="co"># histograms per group</span></span>
<span id="cb68-7"><a href="lm.html#cb68-7" aria-hidden="true" tabindex="-1"></a>leftplot <span class="ot">&lt;-</span> </span>
<span id="cb68-8"><a href="lm.html#cb68-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(post, <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">fill =</span> groups)) <span class="sc">+</span></span>
<span id="cb68-9"><a href="lm.html#cb68-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..), <span class="at">binwidth =</span> <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb68-10"><a href="lm.html#cb68-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb68-11"><a href="lm.html#cb68-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;top&quot;</span>, <span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb68-12"><a href="lm.html#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="lm.html#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="co"># plot mean and 95%-CrI</span></span>
<span id="cb68-14"><a href="lm.html#cb68-14" aria-hidden="true" tabindex="-1"></a>rightplot <span class="ot">&lt;-</span> </span>
<span id="cb68-15"><a href="lm.html#cb68-15" aria-hidden="true" tabindex="-1"></a>  post <span class="sc">%&gt;%</span> </span>
<span id="cb68-16"><a href="lm.html#cb68-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(groups) <span class="sc">%&gt;%</span> </span>
<span id="cb68-17"><a href="lm.html#cb68-17" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(</span>
<span id="cb68-18"><a href="lm.html#cb68-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">mean</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>),</span>
<span id="cb68-19"><a href="lm.html#cb68-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">CrI_lo =</span> <span class="fu">quantile</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">probs =</span> <span class="fl">0.025</span>),</span>
<span id="cb68-20"><a href="lm.html#cb68-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">CrI_up =</span> <span class="fu">quantile</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">probs =</span> <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb68-21"><a href="lm.html#cb68-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> groups, <span class="at">y =</span> mean)) <span class="sc">+</span></span>
<span id="cb68-22"><a href="lm.html#cb68-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb68-23"><a href="lm.html#cb68-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> CrI_lo, <span class="at">ymax =</span> CrI_up), <span class="at">width =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb68-24"><a href="lm.html#cb68-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>) <span class="sc">+</span></span>
<span id="cb68-25"><a href="lm.html#cb68-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Weight (g)&quot;</span>, <span class="at">x =</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb68-26"><a href="lm.html#cb68-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-27"><a href="lm.html#cb68-27" aria-hidden="true" tabindex="-1"></a><span class="fu">multiplot</span>(leftplot, rightplot, <span class="at">cols =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figanovares"></span>
<img src="2.03-lm_files/figure-html/figanovares-1.png" alt="Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% credible intervals obtained from the simulated distributions (right)." width="672" />
<p class="caption">
Figure 11.7: Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% credible intervals obtained from the simulated distributions (right).
</p>
</div>
<p>To obtain the posterior distribution of the difference between the means of group 1 and group 2, we simply calculate this difference for each draw from the joint posterior distribution of the group means.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="lm.html#cb69-1" aria-hidden="true" tabindex="-1"></a>d.g1<span class="fl">.2</span> <span class="ot">&lt;-</span> m.g1 <span class="sc">-</span> m.g2</span>
<span id="cb69-2"><a href="lm.html#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(d.g1<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## [1] -2.209551</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="lm.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(d.g1<span class="fl">.2</span>, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## -3.128721 -1.342693</code></pre>
<p>The estimated difference is -2.2095511. We are 95% sure that the difference between the means of group 1 and 2 is between -3.1287208 and -1.3426929.</p>
<p>How strongly do the data support the hypothesis that the mean of group 2 is larger than the mean of group 1? To answer this question we calculate the proportion of the draws from the joint posterior distribution for which the mean of group 2 is larger than the mean of group 1.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="lm.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(m.g2 <span class="sc">&gt;</span> m.g1) <span class="sc">/</span> nsim </span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>This means that in all of the 1000 simulations from the joint posterior distribution, the mean of group 2 was larger than the mean of group 1. Therefore, there is a very high probability (i.e., it is close to 1; because probabilities are never exactly 1, we write &gt;0.999) that the mean of group 2 is larger than the mean of group 1.</p>
</div>
<div id="frequentist-results-from-a-one-way-anova" class="section level3" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Frequentist Results from a One-Way ANOVA</h3>
</div>
<div id="two-way-anova" class="section level3" number="11.2.3">
<h3><span class="header-section-number">11.2.3</span> Two-Way ANOVA</h3>
</div>
<div id="frequentist-results-from-a-two-way-anova" class="section level3" number="11.2.4">
<h3><span class="header-section-number">11.2.4</span> Frequentist Results from a Two-Way ANOVA</h3>
</div>
<div id="multiple-comparisons-and-post-hoc-tests" class="section level3" number="11.2.5">
<h3><span class="header-section-number">11.2.5</span> Multiple Comparisons and Post Hoc Tests</h3>
</div>
<div id="analysis-of-covariance" class="section level3" number="11.2.6">
<h3><span class="header-section-number">11.2.6</span> Analysis of Covariance</h3>
</div>
<div id="multiple-regression-and-collinearity" class="section level3" number="11.2.7">
<h3><span class="header-section-number">11.2.7</span> Multiple Regression and Collinearity</h3>
</div>
<div id="orderedfactors" class="section level3" number="11.2.8">
<h3><span class="header-section-number">11.2.8</span> Ordered Factors and Contrasts</h3>
</div>
<div id="quadratic-and-higher-polynomial-terms" class="section level3" number="11.2.9">
<h3><span class="header-section-number">11.2.9</span> Quadratic and Higher Polynomial Terms</h3>
</div>
</div>
<div id="pendenzen" class="section level2" number="11.3">
<h2><span class="header-section-number">11.3</span> Pendenzen</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Manchmal schreiben wir <span class="math inline">\(\sigma\)</span> und manchmal <span class="math inline">\(\sigma^2\)</span>. Bin mir nicht sicher, ob die Unterscheidung jedes Mal richtig ist. Das sollten wir noch kontrollieren und irgendwo im Text auch auf den Unterschied hinweisen.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="priors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="residualanalysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/TobiasRoth/BDAEcology/edit/master/2.03-lm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
