
# Linear Mixed Effect Models{#lmer}


<a href="" target="_blank"><img src="images/himmelsherold.jpg" width="640" style="display: block; margin: auto;" /></a>

------


## Background




### Why Mixed Effects Models?

Mixed effects models (or hierarchical models; see @Gelman.2007 for a discussion on the terminology) are used to analyze nonindependent, grouped, or hierarchical data. For example, when we measure growth rates of nestlings in different nests by taking mass measurements of each nestling several times during the nestling phase, the measurements are grouped within nestlings (because there are repeated measurements of each) and the nestlings are grouped within nests. Measurements from the same individual are likely to be more similar than measurements from different individuals, and individuals from the same nest are likely to be more similar than nestlings from different nests. Measurements of the same group (here, the “groups” are individuals or nests) are not independent. If the grouping structure of the data is ignored in the model, the residuals do not fulfill the independence assumption.  

Further, predictor variables can be measured on different hierarchical levels. For example, in each nest some nestlings were treated with a hormone implant whereas others received a placebo. Thus, the treatment is measured at the level of the individual, while clutch size is measured at the level of the nest. Clutch size was measured only once per nest but entered in the data file more than once (namely for each individual from the same nest). Repeated measure results in pseudoreplication if we do not account for the hierarchical data structure in the model. Mixed models allow modeling of the hierarchical structure of the data and, therefore, account for pseudoreplication.  

Mixed models are further used to analyze variance components. For example, when the nestlings were cross-fostered so that they were not raised by their genetic parents, we would like to estimate the proportions of the variance (in a measurement, e.g., wing length) that can be assigned to genetic versus to environmental differences.  

The three problems - grouped data, repeated measure and interest in variances - are solved by adding further variance parameters to the model. As a result, the linear predictor of such models contains predictors with fixed parameters and predictors with non-fixed  parameters. The former are called "fixed effects", the latter "random effects". "Fixed effects" can be a slope (for a continuous predictor), or a defined set of levels for a factor (often called a "fixed factor"). The non-mixed models presented in chapter \@ref(lm) (and later in chapter \@ref(glm)) only have such "fixed effects". A mixed model, on the other hand, contains fixed and random effects. Typically, a grouping variable as described above is treated as a "random factor". "Random factor" is a somewhat misleading name as it is not the factor that is random. Rather, the levels of the factor are seen as a random sample from a bigger population of levels (e.g. nests), and  we assume that the level-specific parameter value comes from a distribution, usually the normal distribution. Thus, a random effect in a model can be seen as a model (for a parameter) that is nested within the model for the data.

Predictors that are defined as fixed effects are either numeric or, if they are categorical, they have a finite (“fixed”) number of levels, defined by the research question. For example, the factor “treatment” in the Barn owl study below has exactly two levels: "placebo" and "corticosterone", and nothing more. In contrast, random effects have a theoretically infinite number of levels of which we have measured a random sample. For example, we have measured 10 nests, but there are many more nests in the world that we have not measured. Normally, fixed effects have a low number of levels whereas random effects have a large number of levels (at least 3!). For fixed effects we are interested in the specific differences between levels (e.g., between males and females, placebo and corticosterone, etc), whereas for random effects we are mainly interested in the between-level (between-group, e.g., between-nest) variance rather than in differences between specific levels (e.g., nest A versus nest B).
Typical fixed effects are: treatment, sex, age classes, or season. Typical random effects are: nest, individual, field, school, or study plot. It depends sometimes on the aim of the study whether a factor should be treated as fixed or random. When we would like to compare the average size of a corn cob between specific regions, then we include region as a fixed factor. However, when we would like to know how the size of a corn cob is related to the irrigation system and we have several measurements within each of a sample of regions, then we treat region as a random factor.


### Random Factors and Partial Pooling

In a model with fixed factors, the differences of the group means to the mean of the reference group are separately estimated as model parameters. This produces $k-1$ (independent) model parameters, where $k$ is the number of groups (or number of factor levels). In contrast, for a random factor, the between-group variance is estimated and the $k$ group-specific means are assumed to be normally distributed around the population mean. These $k$ means are thus not independent. We usually call the differences between the specific mean of group $g$ and the mean of all groups $b_g$ (one value per group). They are assumed to be realizations of the same (in most cases normal) distribution with a mean of zero. They are like residuals. The variance of the $b_g$ values is the among-group variance. 

Treating a factor as a random factor is equivalent to "partial pooling" of the data. There are three different ways to obtain means for grouped data. First, the grouping structure of the data can be ignored - we simply estimate a mean across all data. This is called complete pooling (left panel in Figure \@ref(fig:pooling)).  

Second, group means may be estimated separately for each group. In this case, the data from all other groups are ignored when estimating a group mean. No pooling occurs in this case (right panel in Figure \@ref(fig:pooling)). A fixed factor return such means.

Third, the data of the different groups can be partially pooled (i.e., treated as a random effect). Thereby, the group means are weighted averages of the population mean and the unpooled group means. The weights are proportional to sample size and the inverse of the variance (see @Gelman.2007, p. 252). Further, the overall mean (the "population mean") is close to the mean of the group-specific means, thus, every group is weighed similarly for calculating this overall mean. In contrast, in the complete pooling case, the groups get weights proportional to their sample sizes, i.e. each observation has the same weight. In the no-pooling case, there is no overall mean.



Complete pooling    | Partial pooling       | No pooling        | 
:-------------------|:----------------------|:------------------|
 $\hat{y_i} = \beta_0$ \ $y_i \sim normal(\hat{y_i}, \sigma^2)$ | $\hat{y_i} = \beta_0 + b_{g[i]}$ \ $b_g \sim normal(0, \sigma_b^2)$ \ $y_i \sim normal(\hat{y_i}, \sigma^2)$  | $\hat{y_i} = \beta_{0[g[i]]}$ \ $y_i \sim normal(\hat{y_i}, \sigma_g^2)$ |  


<div class="figure">
<img src="2.05-lmer_files/figure-html/pooling-1.png" alt="Three possibilities to obtain group and/or population means for grouped data: complete pooling, partial pooling, and no pooling. Open symbols = data, orange dots with vertical bars = group means with 95% uncertainty intervals, horizontal black line with shaded interval = population mean with 95% uncertainty interval." width="672" />
<p class="caption">(\#fig:pooling)Three possibilities to obtain group and/or population means for grouped data: complete pooling, partial pooling, and no pooling. Open symbols = data, orange dots with vertical bars = group means with 95% uncertainty intervals, horizontal black line with shaded interval = population mean with 95% uncertainty interval.</p>
</div>


What is the advantage of analyses using partial pooling (i.e., mixed, hierarchical, or multilevel modelling) compared to the complete or no pooling analyses? Complete pooling ignores the grouping structure of the data. As a result, the uncertainty interval of the population mean may be too narrow. We are too confident in the result because we assume that all observations are independent when they are not. This is a typical case of pseudoreplication. On the other hand, the no pooling method (which is equivalent to treating the factor as fixed) has the danger of overestimation of the among-group variance because the group means are estimated independently of each other. The danger of overestimating the among-group variance is particularly large when sample sizes per group are low and within-group variance large. In contrast, the partial pooling method assumes that the group means are a random sample from a common distribution. Therefore, information is exchanged between groups. Estimated means for groups with low sample sizes, large variances, and means far away from the population mean are shrunk towards the population mean. Thus, group means that are estimated with a lot of imprecision (because of low sample size and high variance) are shrunk towards the population mean. How strongly they are shrunk depends on the precision of the estimates for the group specific means and the population mean. 

An example will help make this clear. Imagine that we measured 60 nestling birds from 10 nests (6 nestlings per nest) and found that the average nestling mass at day 10 was around 20&nbsp;g with an among-nest standard deviation of 2&nbsp;g. Then, we measure only one nestling from one additional nest (from the same population) whose mass was 12&nbsp;g. What do we know about the average mass of this new nest? The mean of the measurements for this nest is 12&nbsp;g, but with n = 1 uncertainty is high. Because we know that the average mass of the other nests was 20&nbsp;g, and because the new nest belonged to the same population, a value higher than 12&nbsp;g is a better estimate for an average nestling mass of the new nest than the 12&nbsp;g measurement of one single nestling, which could, by chance, have been an exceptionally light individual. This is the shrinkage that partial pooling allows in a mixed model. Because of this shrinkage, the estimates for group means from a mixed model are sometimes called shrinkage estimators. A consequence of the shrinkage is that the residuals are positively correlated with the fitted values. 
To summarize, mixed models are used to appropriately estimate among-group variance, and to account for non-independency among data points.

## Fitting a normal linear mixed model in R

To introduce the linear mixed model, we use repeated hormone measures at nestling barn owls *Tyto alba*. The cortbowl data set contains stress hormone data (corticosterone, variable ‘totCort’) of nestling barn owls which were either treated with a corticosterone implant, or with a placebo-implant as the control group. The aim of the study was to quantify the corticosterone increase due to the corticosterone implant [@Almasi.2009]. In each brood, one or two nestlings were implanted with a corticosterone implant and one or two nestlings with a placebo implant (variable ‘Implant’). Blood samples were taken just before implantation, and at days 2 and 20 after implantation. 


``` r
data(cortbowl)
dat <- cortbowl
dat$days <- factor(dat$days, levels=c("before", "2", "20")) 
str(dat)  # the data was sampled in 2004,2005, and 2005 by the Swiss Ornithologicla Institute
```

```
## 'data.frame':	287 obs. of  6 variables:
##  $ Brood  : Factor w/ 54 levels "231","232","233",..: 7 7 7 7 8 8 9 9 10 10 ...
##  $ Ring   : Factor w/ 151 levels "898054","898055",..: 44 45 45 46 31 32 9 9 18 19 ...
##  $ Implant: Factor w/ 2 levels "C","P": 2 2 2 1 2 1 1 1 2 1 ...
##  $ Age    : int  49 29 47 25 57 28 35 53 35 31 ...
##  $ days   : Factor w/ 3 levels "before","2","20": 3 2 3 2 3 1 2 3 2 2 ...
##  $ totCort: num  5.76 8.42 8.05 25.74 8.04 ...
```

In total, there are 287 measurements of 151 individuals (variable ‘Ring’) of 54 broods. Because the measurements from the same individual are non-independent, we use a mixed model to analyze these data. Two additional arguments for a mixed model are: a) The mixed model allows prediction of corticosterone levels for an ‘average’ individual, whereas the fixed effect model allows prediction of corticosterone levels only for the 151 individuals that were sampled. And b) fewer parameters are needed. If we include individual as a fixed factor, we would use 150 parameters, while the random factor needs a much lower number of parameters (only one classical parameter: the among-individual variance; the random factor increases model complexity by more than 1 parameter - by how many depends on the data, but it is usually much less than what is needed for a fixed factor with the same number of levels).
We first create a raw-data plot to show the development for each individual, separately for owls receiving corticosterone versus owls receiving a placebo (Figure \@ref(fig:corttest)).


<div class="figure">
<img src="2.05-lmer_files/figure-html/corttest-1.png" alt="Total corticosterone before and at day 2 and 20 after implantation of a corticosterone or a placebo implant. Lines connect measurements of the same individual." width="672" />
<p class="caption">(\#fig:corttest)Total corticosterone before and at day 2 and 20 after implantation of a corticosterone or a placebo implant. Lines connect measurements of the same individual.</p>
</div>

We fit a normal linear model with ‘Ring’ as a random factor, and ‘Implant’, ‘days’ and the interaction of ‘Implant’ $\times$ ‘days’ as fixed effects. Note that both ‘Implant’ and ‘days’ are defined as factors, thus R creates indicator variables for all levels except the reference level. Later, we will also include ‘Brood’ as a grouping level; for now, we ignore this level and start with a simpler (less perfect) model for illustrative purposes.

\[
\begin{aligned}
\hat{y_i} &= \beta_0 + b_{\text{Ring}[i]} + \beta_1 I(\text{days}=2) + \beta_2 I(\text{days}=20) + \beta_3 I(\text{Implant}=P) \\
&\quad + \beta_4 I(\text{days}=2) I(\text{Implant}=P) + \beta_5 I(\text{days}=20) I(\text{Implant}=P)
\end{aligned}
\]
$b_{Ring} \sim normal(0, \sigma_b)$  
$y_i \sim normal(\hat{y_i}, \sigma)$  

Several different functions to fit a mixed model have been written in R: `lme`, `gls`, `gee` have been the first ones. Then `lmer` followed, and now, `stan_lmer` and `brm` allow to fit a large variety of hierarchical models. We here start with using `lmer` from the package lme4, because it is a kind of basic function also for `stan_lmer`and `brm`. Further, `sim` can only work with lmer-objects but none of the others. To work with lmer, we usually load the package arm which contains the function `sim` and also automatically loads lme4.

The function `lmer` is used similarly to the function `lm`. The only difference is that the random factors are added in the model formula within parentheses, e.g. ‘+ (1|Ring)’. The ’1’ stands for the intercept and the ‘|’ means ‘grouped by’. ‘(1|Ring)’, therefore, adds the random deviations for each individual to the average intercept. These deviations are the $b_{Ring}$ in the model formula above.

To fit the model to our example data, we log-transformed the corticosterone values (i.e. the outcome variable) to achieve normally distributed residuals. After having fitted the model, in real life, we always first inspect the residuals before we look at the model output. Here, we skip this point to explain how the model is constructed right after having shown the model code. But we will come back to  the residual analyses.



``` r
mod <- lmer(log(totCort) ~ Implant + days + Implant:days + (1|Ring), 
            data=dat, REML=TRUE)
mod
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: log(totCort) ~ Implant + days + Implant:days + (1 | Ring)
##    Data: dat
## REML criterion at convergence: 611.9053
## Random effects:
##  Groups   Name        Std.Dev.
##  Ring     (Intercept) 0.3384  
##  Residual             0.6134  
## Number of obs: 287, groups:  Ring, 151
## Fixed Effects:
##     (Intercept)         ImplantP            days2           days20  
##         1.91446         -0.08523          1.65307          0.26278  
##  ImplantP:days2  ImplantP:days20  
##        -1.71999         -0.09514
```





