<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan</title>
  <meta name="description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.jpg" />
  <meta property="og:description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="github-repo" content="TobiasRoth/BDAEcology" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan" />
  
  <meta name="twitter:description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="twitter:image" content="images/cover.jpg" />

<meta name="author" content="Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Pius Korner-Nievergelt" />


<meta name="date" content="2022-12-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="priors.html"/>
<link rel="next" href="residualanalysis.html"/>
<script src="libs/header-attrs-2.14/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="settings/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="index.html#why-this-book" id="toc-why-this-book">Why this book?</a></li>
<li><a href="index.html#about-this-book" id="toc-about-this-book">About this book</a></li>
<li><a href="index.html#how-to-contribute" id="toc-how-to-contribute">How to contribute?</a></li>
<li><a href="index.html#acknowledgments" id="toc-acknowledgments">Acknowledgments</a></li>
</ul></li>
<li><a href="#part-basic-statistics-for-ecologists" id="toc-part-basic-statistics-for-ecologists">(PART) BASIC STATISTICS FOR ECOLOGISTS</a></li>
<li><a href="PART-I.html#PART-I" id="toc-PART-I"><span class="toc-section-number">1</span> Introduction to PART I</a>
<ul>
<li><a href="PART-I.html#further-reading" id="toc-further-reading"><span class="toc-section-number">1.1</span> Further reading</a></li>
</ul></li>
<li><a href="basics.html#basics" id="toc-basics"><span class="toc-section-number">2</span> Prerequisits: Basic statistical terms</a>
<ul>
<li><a href="basics.html#variables-and-observations" id="toc-variables-and-observations"><span class="toc-section-number">2.1</span> Variables and observations</a></li>
<li><a href="basics.html#displaying-and-summarizing-variables" id="toc-displaying-and-summarizing-variables"><span class="toc-section-number">2.2</span> Displaying and summarizing variables</a>
<ul>
<li><a href="basics.html#correlations" id="toc-correlations"><span class="toc-section-number">2.2.1</span> Correlations</a></li>
<li><a href="basics.html#principal-components-analyses-pca" id="toc-principal-components-analyses-pca"><span class="toc-section-number">2.2.2</span> Principal components analyses PCA</a></li>
</ul></li>
<li><a href="basics.html#inferential-statistics" id="toc-inferential-statistics"><span class="toc-section-number">2.3</span> Inferential statistics</a>
<ul>
<li><a href="basics.html#uncertainty" id="toc-uncertainty"><span class="toc-section-number">2.3.1</span> Uncertainty</a></li>
<li><a href="basics.html#standard-error" id="toc-standard-error"><span class="toc-section-number">2.3.2</span> Standard error</a></li>
</ul></li>
<li><a href="basics.html#bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods" id="toc-bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods"><span class="toc-section-number">2.4</span> Bayes theorem and the common aim of frequentist and Bayesian methods</a>
<ul>
<li><a href="basics.html#bayes-theorem-for-discrete-events" id="toc-bayes-theorem-for-discrete-events"><span class="toc-section-number">2.4.1</span> Bayes theorem for discrete events</a></li>
<li><a href="basics.html#bayes-theorem-for-continuous-parameters" id="toc-bayes-theorem-for-continuous-parameters"><span class="toc-section-number">2.4.2</span> Bayes theorem for continuous parameters</a></li>
<li><a href="basics.html#estimating-a-mean-assuming-that-the-variance-is-known" id="toc-estimating-a-mean-assuming-that-the-variance-is-known"><span class="toc-section-number">2.4.3</span> Estimating a mean assuming that the variance is known</a></li>
<li><a href="basics.html#estimating-the-mean-and-the-variance" id="toc-estimating-the-mean-and-the-variance"><span class="toc-section-number">2.4.4</span> Estimating the mean and the variance</a></li>
</ul></li>
<li><a href="basics.html#classical-frequentist-tests-and-alternatives" id="toc-classical-frequentist-tests-and-alternatives"><span class="toc-section-number">2.5</span> Classical frequentist tests and alternatives</a>
<ul>
<li><a href="basics.html#nullhypothesis-testing" id="toc-nullhypothesis-testing"><span class="toc-section-number">2.5.1</span> Nullhypothesis testing</a></li>
<li><a href="basics.html#comparison-of-a-sample-with-a-fixed-values-one-sample-t-test" id="toc-comparison-of-a-sample-with-a-fixed-values-one-sample-t-test"><span class="toc-section-number">2.5.2</span> Comparison of a sample with a fixed values (one-sample t-test)</a></li>
<li><a href="basics.html#comparison-of-the-locations-between-two-groups-two-sample-t-test" id="toc-comparison-of-the-locations-between-two-groups-two-sample-t-test"><span class="toc-section-number">2.5.3</span> Comparison of the locations between two groups (two-sample t-test)</a></li>
</ul></li>
<li><a href="basics.html#summary" id="toc-summary"><span class="toc-section-number">2.6</span> Summary</a></li>
</ul></li>
<li><a href="analyses-steps.html#analyses_steps" id="toc-analyses_steps"><span class="toc-section-number">3</span> Data analysis step by step</a>
<ul>
<li><a href="analyses-steps.html#step1" id="toc-step1"><span class="toc-section-number">3.1</span> Plausibility of Data</a></li>
<li><a href="analyses-steps.html#step2" id="toc-step2"><span class="toc-section-number">3.2</span> Relationships</a></li>
<li><a href="analyses-steps.html#step3" id="toc-step3"><span class="toc-section-number">3.3</span> Data Distribution</a></li>
<li><a href="analyses-steps.html#step4" id="toc-step4"><span class="toc-section-number">3.4</span> Preparation of Explanatory Variables</a></li>
<li><a href="analyses-steps.html#step5" id="toc-step5"><span class="toc-section-number">3.5</span> Data Structure</a></li>
<li><a href="analyses-steps.html#step6" id="toc-step6"><span class="toc-section-number">3.6</span> Define Prior Distributions</a></li>
<li><a href="analyses-steps.html#step7" id="toc-step7"><span class="toc-section-number">3.7</span> Fit the Model</a></li>
<li><a href="analyses-steps.html#step8" id="toc-step8"><span class="toc-section-number">3.8</span> Check Model</a></li>
<li><a href="analyses-steps.html#step9" id="toc-step9"><span class="toc-section-number">3.9</span> Model Uncertainty</a></li>
<li><a href="analyses-steps.html#step10" id="toc-step10"><span class="toc-section-number">3.10</span> Draw Conclusions</a></li>
<li><a href="analyses-steps.html#further-reading-1" id="toc-further-reading-1">Further reading</a></li>
</ul></li>
<li><a href="distributions.html#distributions" id="toc-distributions"><span class="toc-section-number">4</span> Probability distributions</a>
<ul>
<li><a href="distributions.html#introduction" id="toc-introduction"><span class="toc-section-number">4.1</span> Introduction</a></li>
<li><a href="distributions.html#normal-distribution" id="toc-normal-distribution"><span class="toc-section-number">4.2</span> Normal distribution</a></li>
<li><a href="distributions.html#poisson-distribution" id="toc-poisson-distribution"><span class="toc-section-number">4.3</span> Poisson distribution</a></li>
<li><a href="distributions.html#gamma-distribution" id="toc-gamma-distribution"><span class="toc-section-number">4.4</span> Gamma distribution</a>
<ul>
<li><a href="distributions.html#cauchydistri" id="toc-cauchydistri"><span class="toc-section-number">4.4.1</span> Cauchy distribution</a></li>
<li><a href="distributions.html#t-distribution" id="toc-t-distribution"><span class="toc-section-number">4.4.2</span> t-distribution</a></li>
</ul></li>
<li><a href="distributions.html#f-distribution" id="toc-f-distribution"><span class="toc-section-number">4.5</span> F-distribution</a></li>
</ul></li>
<li><a href="rfunctions.html#rfunctions" id="toc-rfunctions"><span class="toc-section-number">5</span> Important R-functions</a>
<ul>
<li><a href="rfunctions.html#data-preparation" id="toc-data-preparation"><span class="toc-section-number">5.1</span> Data preparation</a></li>
<li><a href="rfunctions.html#figures" id="toc-figures"><span class="toc-section-number">5.2</span> Figures</a></li>
<li><a href="rfunctions.html#summary-1" id="toc-summary-1"><span class="toc-section-number">5.3</span> Summary</a></li>
</ul></li>
<li><a href="reproducibleresearch.html#reproducibleresearch" id="toc-reproducibleresearch"><span class="toc-section-number">6</span> Reproducible research</a>
<ul>
<li><a href="reproducibleresearch.html#summary-2" id="toc-summary-2"><span class="toc-section-number">6.1</span> Summary</a></li>
<li><a href="reproducibleresearch.html#further-reading-2" id="toc-further-reading-2"><span class="toc-section-number">6.2</span> Further reading</a></li>
</ul></li>
<li><a href="furthertopics.html#furthertopics" id="toc-furthertopics"><span class="toc-section-number">7</span> Further topics</a>
<ul>
<li><a href="furthertopics.html#bioacoustic-analyse" id="toc-bioacoustic-analyse"><span class="toc-section-number">7.1</span> Bioacoustic analyse</a></li>
<li><a href="furthertopics.html#python" id="toc-python"><span class="toc-section-number">7.2</span> Python</a></li>
</ul></li>
<li><a href="#part-bayesian-data-analysis" id="toc-part-bayesian-data-analysis">(PART) BAYESIAN DATA ANALYSIS</a></li>
<li><a href="PART-II.html#PART-II" id="toc-PART-II"><span class="toc-section-number">8</span> Introduction to PART II</a>
<ul>
<li><a href="PART-II.html#further-reading-3" id="toc-further-reading-3">Further reading</a></li>
</ul></li>
<li><a href="bayesian-paradigm.html#bayesian_paradigm" id="toc-bayesian_paradigm"><span class="toc-section-number">9</span> The Bayesian paradigm</a>
<ul>
<li><a href="bayesian-paradigm.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">9.1</span> Introduction</a></li>
<li><a href="bayesian-paradigm.html#summary-3" id="toc-summary-3"><span class="toc-section-number">9.2</span> Summary</a></li>
</ul></li>
<li><a href="priors.html#priors" id="toc-priors"><span class="toc-section-number">10</span> Prior distributions</a>
<ul>
<li><a href="priors.html#introduction-2" id="toc-introduction-2"><span class="toc-section-number">10.1</span> Introduction</a></li>
<li><a href="priors.html#choosepriors" id="toc-choosepriors"><span class="toc-section-number">10.2</span> How to choose a prior</a></li>
<li><a href="priors.html#prior-sensitivity" id="toc-prior-sensitivity"><span class="toc-section-number">10.3</span> Prior sensitivity</a></li>
</ul></li>
<li><a href="lm.html#lm" id="toc-lm"><span class="toc-section-number">11</span> Normal Linear Models</a>
<ul>
<li><a href="lm.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">11.1</span> Linear regression</a>
<ul>
<li><a href="lm.html#background" id="toc-background"><span class="toc-section-number">11.1.1</span> Background</a></li>
<li><a href="lm.html#fitting-a-linear-regression-in-r" id="toc-fitting-a-linear-regression-in-r"><span class="toc-section-number">11.1.2</span> Fitting a Linear Regression in R</a></li>
<li><a href="lm.html#drawing-conclusions" id="toc-drawing-conclusions"><span class="toc-section-number">11.1.3</span> Drawing Conclusions</a></li>
<li><a href="lm.html#frequentist-results" id="toc-frequentist-results"><span class="toc-section-number">11.1.4</span> Frequentist Results</a></li>
</ul></li>
<li><a href="lm.html#regression-variants-anova-ancova-and-multiple-regression" id="toc-regression-variants-anova-ancova-and-multiple-regression"><span class="toc-section-number">11.2</span> Regression Variants: ANOVA, ANCOVA, and Multiple Regression</a>
<ul>
<li><a href="lm.html#one-way-anova" id="toc-one-way-anova"><span class="toc-section-number">11.2.1</span> One-Way ANOVA</a></li>
<li><a href="lm.html#other-variants-of-normal-linear-models-two-way-anova-analysis-of-covariance-and-multiple-regression" id="toc-other-variants-of-normal-linear-models-two-way-anova-analysis-of-covariance-and-multiple-regression"><span class="toc-section-number">11.2.2</span> Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression</a></li>
<li><a href="lm.html#partial-coefficients-and-some-comments-on-collinearity" id="toc-partial-coefficients-and-some-comments-on-collinearity"><span class="toc-section-number">11.2.3</span> Partial coefficients and some comments on collinearity</a></li>
<li><a href="lm.html#orderedfactors" id="toc-orderedfactors"><span class="toc-section-number">11.2.4</span> Ordered Factors and Contrasts</a></li>
<li><a href="lm.html#quadratic-and-higher-polynomial-terms" id="toc-quadratic-and-higher-polynomial-terms"><span class="toc-section-number">11.2.5</span> Quadratic and Higher Polynomial Terms</a></li>
</ul></li>
</ul></li>
<li><a href="residualanalysis.html#residualanalysis" id="toc-residualanalysis"><span class="toc-section-number">12</span> Assessing Model Assumptions</a>
<ul>
<li><a href="residualanalysis.html#model-assumptions" id="toc-model-assumptions"><span class="toc-section-number">12.1</span> Model Assumptions</a></li>
<li><a href="residualanalysis.html#independent-and-identically-distributed" id="toc-independent-and-identically-distributed"><span class="toc-section-number">12.2</span> Independent and Identically Distributed</a></li>
<li><a href="residualanalysis.html#qqplot" id="toc-qqplot"><span class="toc-section-number">12.3</span> The QQ-Plot</a></li>
<li><a href="residualanalysis.html#tempautocorrelation" id="toc-tempautocorrelation"><span class="toc-section-number">12.4</span> Temporal Autocorrelation</a></li>
<li><a href="residualanalysis.html#spatialautocorrelation" id="toc-spatialautocorrelation"><span class="toc-section-number">12.5</span> Spatial Autocorrelation</a></li>
<li><a href="residualanalysis.html#Heteroscedasticity" id="toc-Heteroscedasticity"><span class="toc-section-number">12.6</span> Heteroscedasticity</a></li>
</ul></li>
<li><a href="lmer.html#lmer" id="toc-lmer"><span class="toc-section-number">13</span> Linear Mixed Effect Models</a>
<ul>
<li><a href="lmer.html#background-1" id="toc-background-1"><span class="toc-section-number">13.1</span> Background</a>
<ul>
<li><a href="lmer.html#why-mixed-effects-models" id="toc-why-mixed-effects-models"><span class="toc-section-number">13.1.1</span> Why Mixed Effects Models?</a></li>
<li><a href="lmer.html#random-factors-and-partial-pooling" id="toc-random-factors-and-partial-pooling"><span class="toc-section-number">13.1.2</span> Random Factors and Partial Pooling</a></li>
</ul></li>
</ul></li>
<li><a href="glm.html#glm" id="toc-glm"><span class="toc-section-number">14</span> Generalized linear models</a>
<ul>
<li><a href="glm.html#introduction-3" id="toc-introduction-3"><span class="toc-section-number">14.1</span> Introduction</a></li>
<li><a href="glm.html#summary-4" id="toc-summary-4"><span class="toc-section-number">14.2</span> Summary</a></li>
</ul></li>
<li><a href="glmm.html#glmm" id="toc-glmm"><span class="toc-section-number">15</span> Generalized linear mixed models</a>
<ul>
<li><a href="glmm.html#introduction-4" id="toc-introduction-4"><span class="toc-section-number">15.1</span> Introduction</a>
<ul>
<li><a href="glmm.html#binomial-mixed-model" id="toc-binomial-mixed-model"><span class="toc-section-number">15.1.1</span> Binomial Mixed Model</a></li>
</ul></li>
<li><a href="glmm.html#summary-5" id="toc-summary-5"><span class="toc-section-number">15.2</span> Summary</a></li>
</ul></li>
<li><a href="modelchecking.html#modelchecking" id="toc-modelchecking"><span class="toc-section-number">16</span> Posterior predictive model checking</a>
<ul>
<li><a href="modelchecking.html#introduction-5" id="toc-introduction-5"><span class="toc-section-number">16.1</span> Introduction</a></li>
<li><a href="modelchecking.html#summary-6" id="toc-summary-6"><span class="toc-section-number">16.2</span> Summary</a></li>
</ul></li>
<li><a href="model-comparison.html#model_comparison" id="toc-model_comparison"><span class="toc-section-number">17</span> Model comparison and multimodel inference</a>
<ul>
<li><a href="model-comparison.html#introduction-6" id="toc-introduction-6"><span class="toc-section-number">17.1</span> Introduction</a></li>
<li><a href="model-comparison.html#summary-7" id="toc-summary-7"><span class="toc-section-number">17.2</span> Summary</a></li>
</ul></li>
<li><a href="stan.html#stan" id="toc-stan"><span class="toc-section-number">18</span> MCMC using Stan</a>
<ul>
<li><a href="stan.html#background-3" id="toc-background-3"><span class="toc-section-number">18.1</span> Background</a></li>
<li><a href="stan.html#install-rstan" id="toc-install-rstan"><span class="toc-section-number">18.2</span> Install <code>rstan</code></a></li>
<li><a href="stan.html#firststanmod" id="toc-firststanmod"><span class="toc-section-number">18.3</span> Writing a Stan model</a></li>
<li><a href="stan.html#run-stan-from-r" id="toc-run-stan-from-r"><span class="toc-section-number">18.4</span> Run Stan from R</a></li>
<li><a href="stan.html#further-reading-4" id="toc-further-reading-4">Further reading</a></li>
</ul></li>
<li><a href="ridge-regression.html#ridge_regression" id="toc-ridge_regression"><span class="toc-section-number">19</span> Ridge Regression</a>
<ul>
<li><a href="ridge-regression.html#introduction-7" id="toc-introduction-7"><span class="toc-section-number">19.1</span> Introduction</a></li>
</ul></li>
<li><a href="SEM.html#SEM" id="toc-SEM"><span class="toc-section-number">20</span> Structural equation models</a>
<ul>
<li><a href="SEM.html#introduction-8" id="toc-introduction-8"><span class="toc-section-number">20.1</span> Introduction</a></li>
</ul></li>
<li><a href="spatial-glmm.html#spatial_glmm" id="toc-spatial_glmm"><span class="toc-section-number">21</span> Modeling spatial data using GLMM</a>
<ul>
<li><a href="spatial-glmm.html#introduction-9" id="toc-introduction-9"><span class="toc-section-number">21.1</span> Introduction</a></li>
<li><a href="spatial-glmm.html#summary-8" id="toc-summary-8"><span class="toc-section-number">21.2</span> Summary</a></li>
</ul></li>
<li><a href="#part-ecological-models" id="toc-part-ecological-models">(PART) ECOLOGICAL MODELS</a></li>
<li><a href="PART-III.html#PART-III" id="toc-PART-III"><span class="toc-section-number">22</span> Introduction to PART III</a>
<ul>
<li><a href="PART-III.html#model-notations" id="toc-model-notations"><span class="toc-section-number">22.1</span> Model notations</a></li>
</ul></li>
<li><a href="zeroinflated-poisson-lmm.html#zeroinflated-poisson-lmm" id="toc-zeroinflated-poisson-lmm"><span class="toc-section-number">23</span> Zero-inflated Poisson Mixed Model</a>
<ul>
<li><a href="zeroinflated-poisson-lmm.html#introduction-10" id="toc-introduction-10"><span class="toc-section-number">23.1</span> Introduction</a></li>
<li><a href="zeroinflated-poisson-lmm.html#example-data" id="toc-example-data"><span class="toc-section-number">23.2</span> Example data</a></li>
<li><a href="zeroinflated-poisson-lmm.html#model" id="toc-model"><span class="toc-section-number">23.3</span> Model</a></li>
</ul></li>
<li><a href="dailynestsurv.html#dailynestsurv" id="toc-dailynestsurv"><span class="toc-section-number">24</span> Daily nest survival</a>
<ul>
<li><a href="dailynestsurv.html#background-4" id="toc-background-4"><span class="toc-section-number">24.1</span> Background</a></li>
<li><a href="dailynestsurv.html#models-for-estimating-daily-nest-survival" id="toc-models-for-estimating-daily-nest-survival"><span class="toc-section-number">24.2</span> Models for estimating daily nest survival</a></li>
<li><a href="dailynestsurv.html#known-fate-model" id="toc-known-fate-model"><span class="toc-section-number">24.3</span> Known fate model</a></li>
<li><a href="dailynestsurv.html#dailynestsurvstan" id="toc-dailynestsurvstan"><span class="toc-section-number">24.4</span> The Stan model</a></li>
<li><a href="dailynestsurv.html#prepare-data-and-run-stan" id="toc-prepare-data-and-run-stan"><span class="toc-section-number">24.5</span> Prepare data and run Stan</a></li>
<li><a href="dailynestsurv.html#check-convergence" id="toc-check-convergence"><span class="toc-section-number">24.6</span> Check convergence</a></li>
<li><a href="dailynestsurv.html#look-at-results" id="toc-look-at-results"><span class="toc-section-number">24.7</span> Look at results</a></li>
<li><a href="dailynestsurv.html#known-fate-model-for-irregular-nest-controls" id="toc-known-fate-model-for-irregular-nest-controls"><span class="toc-section-number">24.8</span> Known fate model for irregular nest controls</a></li>
<li><a href="dailynestsurv.html#further-reading-5" id="toc-further-reading-5">Further reading</a></li>
</ul></li>
<li><a href="cjs-with-mix.html#cjs_with_mix" id="toc-cjs_with_mix"><span class="toc-section-number">25</span> Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals</a>
<ul>
<li><a href="cjs-with-mix.html#introduction-11" id="toc-introduction-11"><span class="toc-section-number">25.1</span> Introduction</a></li>
<li><a href="cjs-with-mix.html#data-description" id="toc-data-description"><span class="toc-section-number">25.2</span> Data description</a></li>
<li><a href="cjs-with-mix.html#model-description" id="toc-model-description"><span class="toc-section-number">25.3</span> Model description</a></li>
<li><a href="cjs-with-mix.html#the-stan-code" id="toc-the-stan-code"><span class="toc-section-number">25.4</span> The Stan code</a></li>
<li><a href="cjs-with-mix.html#call-stan-from-r-check-convergence-and-look-at-results" id="toc-call-stan-from-r-check-convergence-and-look-at-results"><span class="toc-section-number">25.5</span> Call Stan from R, check convergence and look at results</a></li>
</ul></li>
<li><a href="#part-appendices" id="toc-part-appendices">(PART) APPENDICES</a></li>
<li><a href="referenzen.html#referenzen" id="toc-referenzen">Referenzen</a></li>
<li class="divider"></li>
</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis in Ecology with R and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lm" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Normal Linear Models</h1>
<div id="linear-regression" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> Linear regression</h2>
<p><a href="" target="_blank"><img src="images/snowfinch3.jpg" width="1658" style="display: block; margin: auto;" /></a></p>
<hr />
<div id="background" class="section level3" number="11.1.1">
<h3><span class="header-section-number">11.1.1</span> Background</h3>
<p>Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression.</p>
<p>Typical questions that can be answered using linear regression are: How does <span class="math inline">\(y\)</span> change with changes in <span class="math inline">\(x\)</span>? How is y predicted from <span class="math inline">\(x\)</span>? An ordinary linear regression (i.e., one numeric <span class="math inline">\(x\)</span> and one numeric <span class="math inline">\(y\)</span> variable) can be represented by a scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. We search for the line that ﬁts best and describe how the observations scatter around this regression line (see Fig. <a href="lm.html#fig:figlm">11.1</a> for an example). The model formula of a simple linear regression with one continuous predictor variable <span class="math inline">\(x_i\)</span> (the subscript <span class="math inline">\(i\)</span> denotes the <span class="math inline">\(i=1,\dots,n\)</span> data points) is:</p>
<p><span class="math display" id="eq:lm">\[\begin{align}
  \mu_i &amp;=\beta_0 + \beta_1 x_i \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.1}
\end{align}\]</span></p>
<p>While the first part of Equation <a href="lm.html#eq:lm">(11.1)</a> describes the regression line, the second part describes the differences between predicted values <span class="math inline">\(\mu_i\)</span> and observations <span class="math inline">\(y_i\)</span>. In other words: the observation <span class="math inline">\(y_i\)</span> stems from a normal distribution with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span> . The mean of the normal distribution, <span class="math inline">\(\mu_i\)</span> , equals the sum of the intercept (<span class="math inline">\(b_0\)</span> ) and the product of the slope (<span class="math inline">\(b_1\)</span>) and the continuous predictor value, <span class="math inline">\(x_i\)</span>.</p>
<p>The differences between observation <span class="math inline">\(y_i\)</span> and the predicted values <span class="math inline">\(\mu_i\)</span> are the residuals (i.e., <span class="math inline">\(\epsilon_i=y_i-\mu_i\)</span>). Equivalently to Equation <a href="lm.html#eq:lm">(11.1)</a>, the regression could thus be written as:</p>
<p><span class="math display" id="eq:lmalternativ">\[\begin{align}
  y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i\\
  \epsilon_i &amp;\sim Norm(0, \sigma^2)
  \tag{11.2}
\end{align}\]</span></p>
<p>We prefer the notation in Equation <a href="lm.html#eq:lm">(11.1)</a> because, in this formula, the stochastic part (second row) is nicely separated from the deterministic part (first row) of the model, whereas, in the second notation <a href="lm.html#eq:lmalternativ">(11.2)</a> the ﬁrst row contains both stochastic and deterministic parts.</p>
<p>Using matrix notation equation <a href="lm.html#eq:lm">(11.1)</a> can also be written in one row:</p>
<p><span class="math display">\[\boldsymbol{y} \sim
  Norm(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2\boldsymbol{I})\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{ I}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix (it transforms the variance parameter to a <span class="math inline">\(n \times n\)</span> matrix with its diagonal elements equal <span class="math inline">\(\sigma^2\)</span> ; <span class="math inline">\(n\)</span> is the sample size). The multiplication by <span class="math inline">\(\boldsymbol{ I}\)</span> is necessary because we use vector notation, <span class="math inline">\(\boldsymbol{y}\)</span> instead of <span class="math inline">\(y_{i}\)</span> . Here, <span class="math inline">\(\boldsymbol{y}\)</span> is the vector of all observations, whereas <span class="math inline">\(y_{i}\)</span> is a single observation, <span class="math inline">\(i\)</span>. When using vector notation, we can write the linear predictor of the model, <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> , as a multiplication of the vector of the model coefﬁcients</p>
<p><span class="math display">\[\boldsymbol{\beta} =
  \begin{pmatrix}
    \beta_0 \\
    \beta_1
  \end{pmatrix}\]</span></p>
<p>times the model matrix</p>
<p><span class="math display">\[\boldsymbol{X} =
  \begin{pmatrix}
      1     &amp; x_1   \\
      \dots &amp; \dots \\
      1     &amp; x_n
  \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(x_1 , \dots, x_n\)</span> are the observed values for the predictor variable, <span class="math inline">\(x\)</span>. The ﬁrst column of <span class="math inline">\(\boldsymbol{X}\)</span> contains only ones because the values in this column are multiplied with the intercept, <span class="math inline">\(\beta_0\)</span> . To the intercept, the product of the second element of <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\beta_1\)</span> , with each element in the second column of <span class="math inline">\(\boldsymbol{X}\)</span> is added to obtain the predicted value for each observation, <span class="math inline">\(\boldsymbol{\mu}\)</span>:</p>
<p><span class="math display" id="eq:lmmatrix">\[\begin{align}
\boldsymbol{\beta X}=
\begin{pmatrix}
    \beta_0 \\
    \beta_1
\end{pmatrix}
\times
\begin{pmatrix}
      1     &amp; x_1   \\
      \dots &amp; \dots \\
      1     &amp; x_n
  \end{pmatrix} =
  \begin{pmatrix}
    \beta_0 + \beta_1x_1 \\
    \dots \\
    \beta_0 + \beta_1x_n
\end{pmatrix}=
\begin{pmatrix}
    \hat{y}_1 \\
  \dots \\
    \hat{y}_n
\end{pmatrix} =
\boldsymbol{\mu}
\tag{11.3}
\end{align}\]</span></p>
</div>
<div id="fitting-a-linear-regression-in-r" class="section level3" number="11.1.2">
<h3><span class="header-section-number">11.1.2</span> Fitting a Linear Regression in R</h3>
<p>In Equation <a href="lm.html#eq:lm">(11.1)</a>, the predicted (synonym: fitted) values <span class="math inline">\(\mu_i\)</span> are directly deﬁned by the model coefﬁcients, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> . Therefore, when we can estimate <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span> , and <span class="math inline">\(\sigma^2\)</span>, the model is fully deﬁned . The last parameter <span class="math inline">\(\sigma^2\)</span> describes how the observations scatter around the regression line and relies on the assumption that the residuals are normally distributed. The estimates for the model parameters of a linear regression are obtained by searching for the best ﬁtting regression line. To do so, we search for the regression line that minimizes the sum of the squared residuals. This model ﬁtting method is called the least-squares method, abbreviated as LS. It has a very simple solution using matrix algebra <span class="citation">(see e.g., <a href="referenzen.html#ref-Aitkin.2009" role="doc-biblioref">Aitkin et al. 2009</a>)</span>.</p>
<p>Note that we can apply LS techniques independent of whether we use a Bayesian or frequentist framework to draw inference. In Bayesian statistics, Equation <a href="lm.html#eq:lm">(11.1)</a> is called the data model, because it describes mathematically the process that has (or, better, that we think has) produced the data. This nomenclature also helps to distinguish data models from models for parameters such as prior distributions.</p>
<p>The least-squares estimates for the model parameters of a linear regression are obtained in R using the function <code>lm</code>. For illustration, we ﬁrst simulate a data set and then ﬁt a linear regression to these simulated data. The advantage of simulating data is that the following analyses can be reproduced without having to read data into R.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="lm.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">34</span>)            <span class="co"># set a seed for the random number generator</span></span>
<span id="cb40-2"><a href="lm.html#cb40-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span>                 <span class="co"># sample size</span></span>
<span id="cb40-3"><a href="lm.html#cb40-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">5</span>              <span class="co"># standard deviation of the residuals</span></span>
<span id="cb40-4"><a href="lm.html#cb40-4" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">2</span>                 <span class="co"># intercept</span></span>
<span id="cb40-5"><a href="lm.html#cb40-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fl">0.7</span>               <span class="co"># slope</span></span>
<span id="cb40-6"><a href="lm.html#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="lm.html#cb40-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">10</span>, <span class="dv">30</span>)   <span class="co"># sample values of the covariate</span></span>
<span id="cb40-8"><a href="lm.html#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="lm.html#cb40-9" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x</span>
<span id="cb40-10"><a href="lm.html#cb40-10" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, mu, <span class="at">sd =</span> sigma)</span>
<span id="cb40-11"><a href="lm.html#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="lm.html#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Save data in table</span></span>
<span id="cb40-13"><a href="lm.html#cb40-13" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span></code></pre></div>
<p>Then, we ﬁt a linear regression to the data to obtain the results of the three parameters of the linear regression, that is intercept, slope and residual standard deviation.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="lm.html#cb41-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span>  <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dat)</span>
<span id="cb41-2"><a href="lm.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span></code></pre></div>
<pre><code>## (Intercept)           x 
##   2.0049517   0.6880415</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="lm.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 5.04918</code></pre>
<p>The object “mod” produced by <code>lm</code> contains the estimates for the intercept, <span class="math inline">\(\beta_0\)</span> , and the slope, <span class="math inline">\(\beta_1\)</span>. The residual standard deviation <span class="math inline">\(\sigma^2\)</span> is extracted using the function <code>summary</code>. We can show the result of the linear regression as a line in a scatter plot with the covariate (<code>x</code>) on the x-axis and the observations (<code>y</code>) on the y-axis (Fig. <a href="lm.html#fig:figlm">11.1</a>).</p>
<div class="figure"><span id="fig:figlm"></span>
<img src="2.03-lm_files/figure-html/figlm-1.png" alt="Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The &lt;U+FB01&gt;tted values lie where the orange dotted lines touch the blue regression line." width="672" />
<p class="caption">
Figure 11.1: Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The &lt;U+FB01&gt;tted values lie where the orange dotted lines touch the blue regression line.
</p>
</div>
<p>Conclusions drawn from a model depend on the model assumptions. When model assumptions are violated, estimates usually are biased and inappropriate conclusions can be drawn. We devote Chapter <a href="residualanalysis.html#residualanalysis">12</a> to the assessment of model assumptions, given its importance.</p>
</div>
<div id="drawing-conclusions" class="section level3" number="11.1.3">
<h3><span class="header-section-number">11.1.3</span> Drawing Conclusions</h3>
<p>To answer the question about how strongly <span class="math inline">\(y\)</span> is related to <span class="math inline">\(x\)</span>, or to predict <span class="math inline">\(y\)</span> from <span class="math inline">\(x\)</span>, and because we usually draw inference in a Bayesian framework, we are interested in the joint posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> (vector that contains <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> ) and <span class="math inline">\(\sigma^2\)</span> , the residual variance. The function <code>sim</code> does this for us. To somewhat demystify the <code>sim</code> function we brieﬂy explain what <code>sim</code> does. The principle is to ﬁrst draw a random value from the marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span>, and then to draw random values from the conditional posterior distribution for <span class="math inline">\(\boldsymbol{\beta}\)</span> <span class="citation">(<a href="referenzen.html#ref-Gelman.2014" role="doc-biblioref">A. Gelman et al. 2014a</a>)</span>.</p>
<p>The conditional posterior distribution of the parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})\)</span> is the posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> given a speciﬁc value for <span class="math inline">\(\sigma^2\)</span> . This conditional distribution can be analytically derived. With ﬂat prior distributions, it is a uni- or multivariate normal distribution <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})=Norm(\boldsymbol{\hat{\beta}},V_\beta,\sigma^2)\)</span> with:</p>
<p><span class="math display" id="eq:sim">\[\begin{align}
  \boldsymbol{\hat{\beta}=(\boldsymbol{X^TX})^{-1}X^Ty}
  \tag{11.4}
\end{align}\]</span></p>
<p>and <span class="math inline">\(V_\beta = (\boldsymbol{X^T X})^{-1}\)</span> . For models with the normal error distribution, the LS estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span> (given by Eq. <a href="lm.html#eq:sim">(11.4)</a>) equal the maximum likelihood (ML) estimates ==(see Chapter 5)==.</p>
<p>The marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span> is independent of speciﬁc values of <span class="math inline">\(\boldsymbol{\beta}\)</span>. It is, for ﬂat prior distributions, an inverse chi-square distribution <span class="math inline">\(p(\sigma^2|\boldsymbol{y,X})=Inv-\chi^2(n-k,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2 = \frac{1}{n-k}(\boldsymbol{y}-\boldsymbol{X,\hat{\beta}})^T(\boldsymbol{y}-\boldsymbol{X,\hat{\beta}})\)</span>, and <span class="math inline">\(k\)</span> is the number of parameters. The marginal posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> can be obtained by integrating the conditional posterior distribution <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})=Norm(\boldsymbol{\hat{\beta}},V_\beta\sigma^2)\)</span> over the distribution of <span class="math inline">\(\sigma^2\)</span> . This results in a uni- or multivariate <span class="math inline">\(t\)</span>-distribution.</p>
<p>However, it is not necessary to do this analytically. Using the function <code>sim</code> from the package, we can draw samples from <span class="math inline">\(p(\sigma^2|\boldsymbol{y,X})\)</span> and describe the marginal posterior distributions of <span class="math inline">\(\boldsymbol{\beta}\)</span> using the simulated values.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="lm.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb45-2"><a href="lm.html#cb45-2" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb45-3"><a href="lm.html#cb45-3" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim =</span> nsim)</span></code></pre></div>
<p>The function <code>sim</code> simulates (in our example) 1000 values from the joint posterior distribution of the three model parameters <span class="math inline">\(\beta_0\)</span> , <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span>. These simulated values are shown in Figure <a href="lm.html#fig:simfirstexample">11.2</a>.</p>
<div class="figure"><span id="fig:simfirstexample"></span>
<img src="2.03-lm_files/figure-html/simfirstexample-1.png" alt="Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters." width="768" />
<p class="caption">
Figure 11.2: Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters.
</p>
</div>
<p>The posterior distributions describe the range of plausible parameter values given the data and the model. They express our uncertainty about the model parameters; they show what we know about the model parameters after having looked at the data and given the model is realistic.</p>
<p>The 2.5% and 97.5% quantiles of the marginal posterior distributions can be used as 95% credible intervals of the model parameters. The function <code>coef</code> extracts the simulated values for the beta coefﬁcients, returning a matrix with <em>nsim</em> rows and the number of columns corresponding to the number of parameters. In our example, the ﬁrst column contains the simulated values from the posterior distribution of the intercept and the second column contains values from the posterior distribution of the slope. The “2” in the second argument of the apply-function (see Chapter <a href="#rmisc"><strong>??</strong></a>) indicates that the <code>quantile</code> function is applied columnwise.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="lm.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(<span class="at">X =</span> <span class="fu">coef</span>(bsim), <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> quantile, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb46-2"><a href="lm.html#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       (Intercept)    x
## 2.5%        -2.95 0.44
## 97.5%        7.17 0.92</code></pre>
<p>We also can calculate a credible interval of the estimated residual standard deviation, <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="lm.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>sigma, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb48-2"><a href="lm.html#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   4.2   6.3</code></pre>
<p>Using Bayesian methods allows us to get a posterior probability for speciﬁc hypotheses, such as “The slope parameter is larger than 1” or “The slope parameter is larger than 0.5”. These probabilities are the proportion of simulated values from the posterior distribution that are larger than 1 and 0.5, respectively.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="lm.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="dv">1</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.008</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="lm.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fl">0.5</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.936</code></pre>
<p>From this, there is very little evidence in the data that the slope is larger than 1, but we are quite conﬁdent that the slope is larger than 0.5 (assuming that our model is realistic).</p>
<p>We often want to show the effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> graphically, with information about the uncertainty of the parameter estimates included in the graph. To draw such effect plots, we use the simulated values from the posterior distribution of the model parameters. From the deterministic part of the model, we know the regression line <span class="math inline">\(\mu = \beta_0 + \beta_1 x_i\)</span>. The simulation from the joint posterior distribution of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We can draw these regression lines in an x-y plot (scatter plot) to show the uncertainty in the regression line estimation (Fig. <a href="lm.html#fig:figlmer1">11.3</a>, left). Note, that in this case it is not advisable to use <code>ggplot</code> because we draw many lines in one plot, which makes <code>ggplot</code> rather slow.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="lm.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb54-2"><a href="lm.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">las =</span> <span class="dv">1</span>, </span>
<span id="cb54-3"><a href="lm.html#cb54-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Covariate (x)&quot;</span>, </span>
<span id="cb54-4"><a href="lm.html#cb54-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Dependend variable (y)&quot;</span>)</span>
<span id="cb54-5"><a href="lm.html#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {</span>
<span id="cb54-6"><a href="lm.html#cb54-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="fu">coef</span>(bsim)[i,<span class="dv">1</span>], <span class="fu">coef</span>(bsim)[i,<span class="dv">2</span>], <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.05</span>))</span>
<span id="cb54-7"><a href="lm.html#cb54-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:figlmer1"></span>
<img src="2.03-lm_files/figure-html/figlmer1-1.png" alt="Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line." width="672" />
<p class="caption">
Figure 11.3: Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line.
</p>
</div>
<p>A more convenient way to show uncertainty is to draw the 95% credible interval, CrI, of the regression line. To this end, we ﬁrst deﬁne new x-values for which we would like to have the ﬁtted values (about 100 points across the range of x will produce smooth-looking lines when connected by line segments). We save these new x-values within the new tibble <code>newdat</code>. Then, we create a new model matrix that contains these new x-values (<code>newmodmat</code>) using the function <code>model.matrix</code>. We then calculate the 1000 ﬁtted values for each element of the new x (one value for each of the 1000 simulated regressions, Fig. <a href="lm.html#fig:figlmer1">11.3</a>), using matrix multiplication (%*%). We save these values in the matrix “ﬁtmat”. Finally, we extract the 2.5% and 97.5% quantiles for each x-value from ﬁtmat, and draw the lines for the lower and upper limits of the credible interval (Fig. <a href="lm.html#fig:figlmer2">11.4</a>).</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="lm.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate 95% credible interval</span></span>
<span id="cb55-2"><a href="lm.html#cb55-2" aria-hidden="true" tabindex="-1"></a>newdat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="dv">30</span>, <span class="at">by =</span> <span class="fl">0.1</span>))</span>
<span id="cb55-3"><a href="lm.html#cb55-3" aria-hidden="true" tabindex="-1"></a>newmodmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>( <span class="sc">~</span> x, <span class="at">data =</span> newdat)</span>
<span id="cb55-4"><a href="lm.html#cb55-4" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> nsim, <span class="at">nrow =</span> <span class="fu">nrow</span>(newdat))</span>
<span id="cb55-5"><a href="lm.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {fitmat[,i] <span class="ot">&lt;-</span> newmodmat <span class="sc">%*%</span> <span class="fu">coef</span>(bsim)[i,]}</span>
<span id="cb55-6"><a href="lm.html#cb55-6" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>CrI_lo <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.025</span>)</span>
<span id="cb55-7"><a href="lm.html#cb55-7" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>CrI_up <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.975</span>)</span>
<span id="cb55-8"><a href="lm.html#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="lm.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make plot</span></span>
<span id="cb55-10"><a href="lm.html#cb55-10" aria-hidden="true" tabindex="-1"></a>regplot <span class="ot">&lt;-</span> </span>
<span id="cb55-11"><a href="lm.html#cb55-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb55-12"><a href="lm.html#cb55-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb55-13"><a href="lm.html#cb55-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb55-14"><a href="lm.html#cb55-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> CrI_lo), <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb55-15"><a href="lm.html#cb55-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> CrI_up), <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb55-16"><a href="lm.html#cb55-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Covariate (x)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Dependend variable (y)&quot;</span>)</span>
<span id="cb55-17"><a href="lm.html#cb55-17" aria-hidden="true" tabindex="-1"></a>regplot</span></code></pre></div>
<div class="figure"><span id="fig:figlmer2"></span>
<img src="2.03-lm_files/figure-html/figlmer2-1.png" alt="Regression with 95% credible interval of the posterior distribution of the &lt;U+FB01&gt;tted values." width="672" />
<p class="caption">
Figure 11.4: Regression with 95% credible interval of the posterior distribution of the &lt;U+FB01&gt;tted values.
</p>
</div>
<p>The interpretation of the 95% credible interval is straightforward: We are 95% sure that the true regression line is within the credible interval. As always, this interpretation is only true if the the model is correct. The larger the sample size, the narrower the interval, because each additional data point increases information about the true regression line.</p>
<p>The credible interval measures uncertainty of the regression line, but it does not describe how new observations would scatter around the regression line. If we want to describe where future observations will be, we have to report the posterior predictive distribution. We can get a sample of random draws from the posterior predictive distribution <span class="math inline">\(\hat{y}|\boldsymbol{\beta},\sigma^2,\boldsymbol{X}\sim Norm( \boldsymbol{X \beta, \sigma^2})\)</span> using the simulated joint posterior distributions of the model parameters, thus taking the uncertainty of the parameter estimates into account. We draw a new <span class="math inline">\(\hat{y}\)</span>-value from <span class="math inline">\(Norm( \boldsymbol{X \beta, \sigma^2})\)</span> for each simulated set of model parameters. Then, we can visualize the 2.5% and 97.5% quantiles of this distribution for each new x-value.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="lm.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># increase number of simulation to procude smooth lines of the posterior</span></span>
<span id="cb56-2"><a href="lm.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictive distribution</span></span>
<span id="cb56-3"><a href="lm.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">34</span>)</span>
<span id="cb56-4"><a href="lm.html#cb56-4" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb56-5"><a href="lm.html#cb56-5" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span>
<span id="cb56-6"><a href="lm.html#cb56-6" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat))</span>
<span id="cb56-7"><a href="lm.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) fitmat[,i] <span class="ot">&lt;-</span> newmodmat<span class="sc">%*%</span><span class="fu">coef</span>(bsim)[i,]</span>
<span id="cb56-8"><a href="lm.html#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="lm.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare matrix for simulated new data</span></span>
<span id="cb56-10"><a href="lm.html#cb56-10" aria-hidden="true" tabindex="-1"></a>newy <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat)) </span>
<span id="cb56-11"><a href="lm.html#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="lm.html#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for each simulated ﬁtted value, simulate one new y-value</span></span>
<span id="cb56-13"><a href="lm.html#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {</span>
<span id="cb56-14"><a href="lm.html#cb56-14" aria-hidden="true" tabindex="-1"></a>  newy[,i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">nrow</span>(newdat), <span class="at">mean =</span> fitmat[,i], <span class="at">sd =</span> bsim<span class="sc">@</span>sigma[i])</span>
<span id="cb56-15"><a href="lm.html#cb56-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-16"><a href="lm.html#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="lm.html#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate 2.5% and 97.5% quantiles</span></span>
<span id="cb56-18"><a href="lm.html#cb56-18" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>pred_lo <span class="ot">&lt;-</span> <span class="fu">apply</span>(newy, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.025</span>)</span>
<span id="cb56-19"><a href="lm.html#cb56-19" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>pred_up <span class="ot">&lt;-</span> <span class="fu">apply</span>(newy, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.975</span>)</span>
<span id="cb56-20"><a href="lm.html#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="lm.html#cb56-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the posterior predictive distribution to plot</span></span>
<span id="cb56-22"><a href="lm.html#cb56-22" aria-hidden="true" tabindex="-1"></a>regplot <span class="sc">+</span></span>
<span id="cb56-23"><a href="lm.html#cb56-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred_lo), <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb56-24"><a href="lm.html#cb56-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred_up), <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figlmer3"></span>
<img src="2.03-lm_files/figure-html/figlmer3-1.png" alt="Regression line with 95% credible interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines." width="672" />
<p class="caption">
Figure 11.5: Regression line with 95% credible interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines.
</p>
</div>
<p>Future observations are expected to be within the interval deﬁned by the broken lines in Fig. <a href="lm.html#fig:figlmer3">11.5</a> with a probability of 0.95. Increasing sample size will not necessarily give a narrower predictive distribution because the predictive distribution also depends on the residual variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The way we produced Fig. <a href="lm.html#fig:figlmer3">11.5</a> is somewhat tedious compared to how easy we could have obtained the same ﬁgure using frequentist methods: <code>predict(mod, newdata = newdat, interval = "prediction")</code> would have produced the y-values for the lower and upper lines in Fig. <a href="lm.html#fig:figlmer3">11.5</a> in one R-code line. However, once we have a simulated sample of the posterior predictive distribution, we have much more information than is contained in the frequentist prediction interval. For example, we could give an estimate for the proportion of observations greater than 20, given <span class="math inline">\(x = 25\)</span>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="lm.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(newy[newdat<span class="sc">$</span>x <span class="sc">==</span> <span class="dv">25</span>, ] <span class="sc">&gt;</span> <span class="dv">20</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.44504</code></pre>
<p>Thus, we expect 44% of future observations with <span class="math inline">\(x = 25\)</span> to be higher than 20. We can extract similar information for any relevant threshold value.</p>
<p>Another reason to learn the more complicated R code we presented here, compared to the frequentist methods, is that, for more complicated models such as mixed models, the frequentist methods to obtain conﬁdence intervals of ﬁtted values are much more complicated than the Bayesian method just presented. The latter can be used with only slight adaptations for mixed models and also for generalized linear mixed models.</p>
</div>
<div id="frequentist-results" class="section level3" number="11.1.4">
<h3><span class="header-section-number">11.1.4</span> Frequentist Results</h3>
<p>The solution for <span class="math inline">\(\boldsymbol{\beta}\)</span> is the Equation <a href="lm.html#eq:lmmatrix">(11.3)</a>. Most statistical software, including R, return an estimated frequentist standard error for each <span class="math inline">\(\beta_k\)</span>. We extract these standard errors together with the estimates for the model parameters using the <code>summary</code> function.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="lm.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.5777  -3.6280  -0.0532   3.9873  12.1374 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   2.0050     2.5349   0.791       0.433    
## x             0.6880     0.1186   5.800 0.000000507 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.049 on 48 degrees of freedom
## Multiple R-squared:  0.412,  Adjusted R-squared:  0.3998 
## F-statistic: 33.63 on 1 and 48 DF,  p-value: 0.0000005067</code></pre>
<p>The summary output ﬁrst gives a rough summary of the residual distribution. However, we will do more rigorous residual analyses in Chapter <a href="residualanalysis.html#residualanalysis">12</a>. The estimates of the model coefﬁcients follow. The column “Estimate” contains the estimates for the intercept <span class="math inline">\(\beta_0\)</span> and the slope <span class="math inline">\(\beta_1\)</span> . The column “Std. Error” contains the estimated (frequentist) standard errors of the estimates. The last two columns contain the t-value and the p-value of the classical t-test for the null hypothesis that the coefﬁcient equals zero. The last part of the summary output gives the parameter <span class="math inline">\(\sigma\)</span> of the model, named “residual standard error” and the residual degrees of freedom.</p>
<p>We try to avoid the name “residual standard error” and use “sigma” instead, because <span class="math inline">\(\sigma\)</span> is not a measurement of uncertainty of a parameter estimate like the standard errors of the model coefﬁcients are. <span class="math inline">\(\sigma\)</span> is a model parameter that describes how the observations scatter around the ﬁtted values, that is, it is a standard deviation. It is independent of sample size, whereas the standard errors of the estimates for the model parameters will decrease with increasing sample size. Such a standard error of the estimate of <span class="math inline">\(\sigma\)</span>, however, is not given in the summary output. Note that, by using Bayesian methods, we could easily obtain the standard error of the estimated <span class="math inline">\(\sigma\)</span> by calculating the standard deviation of the posterior distribution of <span class="math inline">\(\sigma\)</span>. The <span class="math inline">\(R^2\)</span> and the adjusted <span class="math inline">\(R^2\)</span> are explained in Section == “Posterior Predictive Model Checking and Proportion of Explained Variance”==.</p>
</div>
</div>
<div id="regression-variants-anova-ancova-and-multiple-regression" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Regression Variants: ANOVA, ANCOVA, and Multiple Regression</h2>
<div id="one-way-anova" class="section level3" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> One-Way ANOVA</h3>
<p>The aim of analysis of variance (ANOVA) is to compare means of an outcome variable y between different groups (categorical variables). To do so in the frequentist’s framework, variances between and within the groups are compared (hence the name “analysis of variance”). If the variance between the group means is larger than expected by chance , we reject the null hypothesis of no differences between the groups. When doing an ANOVA in a Bayesian way, inference is based on the posterior distributions of the group means and the differences between the group means.</p>
<p>One-way ANOVA means that we only have one explanatory variable (a factor). We illustrate the one-way ANOVA based on an example of simulated data (Fig. <a href="lm.html#fig:figanova">11.6</a>). We have measured weights of 30 virtual individuals for each of 3 groups. Possible research questions could be: How big are the differences between the group means? Are individuals from group 2 heavier than the ones from group 1? Which group mean is higher than 7.5 g?</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="lm.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># settings for the simulation</span></span>
<span id="cb61-2"><a href="lm.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">626436</span>)</span>
<span id="cb61-3"><a href="lm.html#cb61-3" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">12</span>        <span class="co"># mean of group 1 (reference group)</span></span>
<span id="cb61-4"><a href="lm.html#cb61-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">2</span>      <span class="co"># residual standard deviation</span></span>
<span id="cb61-5"><a href="lm.html#cb61-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="dv">3</span>         <span class="co"># difference between group 1 and group 2</span></span>
<span id="cb61-6"><a href="lm.html#cb61-6" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">5</span>        <span class="co"># difference between group 1 and group 3</span></span>
<span id="cb61-7"><a href="lm.html#cb61-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">90</span>         <span class="co"># sample size</span></span>
<span id="cb61-8"><a href="lm.html#cb61-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-9"><a href="lm.html#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co"># generate data</span></span>
<span id="cb61-10"><a href="lm.html#cb61-10" aria-hidden="true" tabindex="-1"></a>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;group 1&quot;</span>,<span class="st">&quot;group 2&quot;</span>, <span class="st">&quot;group 3&quot;</span>), <span class="at">each=</span><span class="dv">30</span>))</span>
<span id="cb61-11"><a href="lm.html#cb61-11" aria-hidden="true" tabindex="-1"></a>simresid <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span>sigma)  <span class="co"># simulate residuals</span></span>
<span id="cb61-12"><a href="lm.html#cb61-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> b0 <span class="sc">+</span> </span>
<span id="cb61-13"><a href="lm.html#cb61-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(group<span class="sc">==</span><span class="st">&quot;group 2&quot;</span>) <span class="sc">*</span> b1 <span class="sc">+</span> </span>
<span id="cb61-14"><a href="lm.html#cb61-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(group<span class="sc">==</span><span class="st">&quot;group 3&quot;</span>) <span class="sc">*</span> b2 <span class="sc">+</span> </span>
<span id="cb61-15"><a href="lm.html#cb61-15" aria-hidden="true" tabindex="-1"></a>  simresid</span>
<span id="cb61-16"><a href="lm.html#cb61-16" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(y, group)</span>
<span id="cb61-17"><a href="lm.html#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="lm.html#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co"># make figure</span></span>
<span id="cb61-19"><a href="lm.html#cb61-19" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span> </span>
<span id="cb61-20"><a href="lm.html#cb61-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> group, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb61-21"><a href="lm.html#cb61-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-22"><a href="lm.html#cb61-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Weight (g)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-23"><a href="lm.html#cb61-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figanova"></span>
<img src="2.03-lm_files/figure-html/figanova-1.png" alt="Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box." width="672" />
<p class="caption">
Figure 11.6: Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box.
</p>
</div>
<p>An ANOVA is a linear regression with a categorical predictor variable instead of a continuous one. The categorical predictor variable with <span class="math inline">\(k\)</span> levels is (as a default in R) transformed to <span class="math inline">\(k-1\)</span> indicator variables. An indicator variable is a binary variable containing 0 and 1 where 1 indicates a speciﬁc level (a category of a nominal variable). Often, one indicator variable is constructed for every level except for the reference level. In our example, the categorical variable is group (<span class="math inline">\(g\)</span>) with the three levels “group 1”, “group 2”, and “group 3” (<span class="math inline">\(k = 3\)</span>). Group 1 is taken as the reference level, and for each of the other two groups an indicator variable is constructed, <span class="math inline">\(I(g_i = 2)\)</span> and <span class="math inline">\(I(g_i = 3)\)</span>. We can write the model as a formula:</p>
<p><span class="math display" id="eq:anova">\[\begin{align}
  \mu_i &amp;=\beta_0 + \beta_1 I(g_i=2) + \beta_1 I(g_i=3) \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.5}
\end{align}\]</span></p>
<p>where <span class="math inline">\(yi\)</span> is the <span class="math inline">\(i\)</span>-th observation (weight measurement for individual i in our example), and <span class="math inline">\(\beta_{0,1,2}\)</span> are the model coefﬁcients. The residual variance is <span class="math inline">\(\sigma^2\)</span>. The model coefﬁcients <span class="math inline">\(\beta_{0,1,2}\)</span> constitute the deterministic part of the model. From the model formula it follows that the group means, <span class="math inline">\(m_g\)</span>, are:</p>
<p><span class="math display" id="eq:anovamw">\[\begin{align}
  m_1 &amp;=\beta_0 \\
  m_2 &amp;=\beta_0 + \beta_1 \\
  m_3 &amp;=\beta_0 + \beta_2 \\
  \tag{11.6}
\end{align}\]</span></p>
<p>There are other possibilities to describe three group means with three parameters, for example:</p>
<p><span class="math display" id="eq:anovamwalt">\[\begin{align}
  m_1 &amp;=\beta_1 \\
  m_2 &amp;=\beta_2 \\
  m_3 &amp;=\beta_3 \\
  \tag{11.7}
\end{align}\]</span></p>
<p>In this case, the model formula would be:</p>
<p><span class="math display" id="eq:anovaalt">\[\begin{align}
  \mu_i &amp;= \beta_1 I(g_i=1) + \beta_2 I(g_i=2) + \beta_3 I(g_i=3) \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.8}
\end{align}\]</span></p>
<p>The way the group means are described is called the parameterization of the model. Different statistical softwares use different parameterizations. The parameterization used by R by default is the one shown in Equation <a href="lm.html#eq:anova">(11.5)</a>. R automatically takes the ﬁrst level as the reference (the ﬁrst level is the ﬁrst one alphabetically unless the user deﬁnes a different order for the levels). The mean of the ﬁrst group (i.e., of the ﬁrst factor level) is the intercept, <span class="math inline">\(b_0\)</span> , of the model. The mean of another factor level is obtained by adding, to the intercept, the estimate of the corresponding parameter (which is the difference from the reference group mean). R calls this parameterization “treatment contrasts”.</p>
<p>The parameterization of the model is deﬁned by the model matrix. In the case of a one-way ANOVA, there are as many columns in the model matrix as there are factor levels (i.e., groups); thus there are k factor levels and k model coefﬁcients. Recall from Equation <a href="lm.html#eq:lmmatrix">(11.3)</a> that for each observation, the entry in the <span class="math inline">\(j\)</span>-th column of the model matrix is multiplied by the <span class="math inline">\(j\)</span>-th element of the model coefﬁcients and the <span class="math inline">\(k\)</span> products are summed to obtain the ﬁtted values. For a data set with <span class="math inline">\(n = 5\)</span> observations of which the ﬁrst two are from group 1, the third from group 2, and the last two from group 3, the model matrix used for the parameterization described in Equation <a href="lm.html#eq:anovamw">(11.6)</a> is</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{X}=
\begin{pmatrix}
      1 &amp; 0 &amp; 0 \\
      1 &amp; 0 &amp; 0 \\
      1 &amp; 1 &amp; 0 \\
      1 &amp; 0 &amp; 1 \\
      1 &amp; 0 &amp; 1 \\
  \end{pmatrix}
\end{align}\]</span></p>
<p>If parameterization of Equation <a href="lm.html#eq:anovamwalt">(11.7)</a> were used,</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{X}=
\begin{pmatrix}
      1 &amp; 0 &amp; 0 \\
      1 &amp; 0 &amp; 0 \\
      0 &amp; 1 &amp; 0 \\
      0 &amp; 0 &amp; 1 \\
      0 &amp; 0 &amp; 1 \\
  \end{pmatrix}
\end{align}\]</span></p>
<p>Other possibilities of model parameterizations, particularly for ordered factors, are introduced in Section <a href="lm.html#orderedfactors">11.2.4</a>.</p>
<p>To obtain the parameter estimates for model parameterized according to Equation <a href="lm.html#eq:anovamw">(11.6)</a> we ﬁt the model in R:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="lm.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb62-2"><a href="lm.html#cb62-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>group)  </span>
<span id="cb62-3"><a href="lm.html#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="lm.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># parameter estimates </span></span>
<span id="cb62-5"><a href="lm.html#cb62-5" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ group)
## 
## Coefficients:
##  (Intercept)  groupgroup 2  groupgroup 3  
##       12.367         2.215        -5.430</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="lm.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 1.684949</code></pre>
<p>The “Intercept” is <span class="math inline">\(\beta_0\)</span>. The other coefﬁcients are named with the factor name (“group”) and the factor level (either “group 2” or “group 3”). These are <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> , respectively. Before drawing conclusions from an R output we need to examine whether the model assumptions are met, that is, we need to do a residual analysis as described in Chapter <a href="residualanalysis.html#residualanalysis">12</a>.</p>
<p>Different questions can be answered using the above ANOVA: What are the group means? What is the difference in the means between group 1 and group 2? What is the difference between the means of the heaviest and lightest group? In a Bayesian framework we can directly assess how strongly the data support the hypothesis that the mean of the group 2 is larger than the mean of group 1. We ﬁrst simulate from the posterior distribution of the model parameters.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="lm.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb66-2"><a href="lm.html#cb66-2" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb66-3"><a href="lm.html#cb66-3" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span></code></pre></div>
<p>Then we obtain the posterior distributions for the group means according to the parameterization of the model formula (Equation <a href="lm.html#eq:anovamw">(11.6)</a>).</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="lm.html#cb67-1" aria-hidden="true" tabindex="-1"></a>m.g1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>]  </span>
<span id="cb67-2"><a href="lm.html#cb67-2" aria-hidden="true" tabindex="-1"></a>m.g2 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] </span>
<span id="cb67-3"><a href="lm.html#cb67-3" aria-hidden="true" tabindex="-1"></a>m.g3 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(bsim)[,<span class="dv">3</span>] </span></code></pre></div>
<p>The histograms of the simulated values from the posterior distributions of the three means are given in Fig. <a href="lm.html#fig:figanovares">11.7</a>. The three means are well separated and, based on our data, we are conﬁdent that the group means differ. From these simulated posterior distributions we obtain the means and use the 2.5% and 97.5% quantiles as limits of the 95% credible intervals (Fig. <a href="lm.html#fig:figanovares">11.7</a>, right).</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="lm.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save simulated values from posterior distribution in  tibble</span></span>
<span id="cb68-2"><a href="lm.html#cb68-2" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> </span>
<span id="cb68-3"><a href="lm.html#cb68-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="st">`</span><span class="at">group 1</span><span class="st">`</span> <span class="ot">=</span> m.g1, <span class="st">`</span><span class="at">group 2</span><span class="st">`</span> <span class="ot">=</span> m.g2, <span class="st">`</span><span class="at">group 3</span><span class="st">`</span> <span class="ot">=</span> m.g3) <span class="sc">%&gt;%</span> </span>
<span id="cb68-4"><a href="lm.html#cb68-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="st">&quot;groups&quot;</span>, <span class="st">&quot;Group means&quot;</span>) </span>
<span id="cb68-5"><a href="lm.html#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="lm.html#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="co"># histograms per group</span></span>
<span id="cb68-7"><a href="lm.html#cb68-7" aria-hidden="true" tabindex="-1"></a>leftplot <span class="ot">&lt;-</span> </span>
<span id="cb68-8"><a href="lm.html#cb68-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(post, <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">fill =</span> groups)) <span class="sc">+</span></span>
<span id="cb68-9"><a href="lm.html#cb68-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..), <span class="at">binwidth =</span> <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb68-10"><a href="lm.html#cb68-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb68-11"><a href="lm.html#cb68-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;top&quot;</span>, <span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb68-12"><a href="lm.html#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="lm.html#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="co"># plot mean and 95%-CrI</span></span>
<span id="cb68-14"><a href="lm.html#cb68-14" aria-hidden="true" tabindex="-1"></a>rightplot <span class="ot">&lt;-</span> </span>
<span id="cb68-15"><a href="lm.html#cb68-15" aria-hidden="true" tabindex="-1"></a>  post <span class="sc">%&gt;%</span> </span>
<span id="cb68-16"><a href="lm.html#cb68-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(groups) <span class="sc">%&gt;%</span> </span>
<span id="cb68-17"><a href="lm.html#cb68-17" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(</span>
<span id="cb68-18"><a href="lm.html#cb68-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">mean</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>),</span>
<span id="cb68-19"><a href="lm.html#cb68-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">CrI_lo =</span> <span class="fu">quantile</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">probs =</span> <span class="fl">0.025</span>),</span>
<span id="cb68-20"><a href="lm.html#cb68-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">CrI_up =</span> <span class="fu">quantile</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">probs =</span> <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb68-21"><a href="lm.html#cb68-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> groups, <span class="at">y =</span> mean)) <span class="sc">+</span></span>
<span id="cb68-22"><a href="lm.html#cb68-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb68-23"><a href="lm.html#cb68-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> CrI_lo, <span class="at">ymax =</span> CrI_up), <span class="at">width =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb68-24"><a href="lm.html#cb68-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>) <span class="sc">+</span></span>
<span id="cb68-25"><a href="lm.html#cb68-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Weight (g)&quot;</span>, <span class="at">x =</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb68-26"><a href="lm.html#cb68-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-27"><a href="lm.html#cb68-27" aria-hidden="true" tabindex="-1"></a><span class="fu">multiplot</span>(leftplot, rightplot, <span class="at">cols =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figanovares"></span>
<img src="2.03-lm_files/figure-html/figanovares-1.png" alt="Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% credible intervals obtained from the simulated distributions (right)." width="672" />
<p class="caption">
Figure 11.7: Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% credible intervals obtained from the simulated distributions (right).
</p>
</div>
<p>To obtain the posterior distribution of the difference between the means of group 1 and group 2, we simply calculate this difference for each draw from the joint posterior distribution of the group means.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="lm.html#cb69-1" aria-hidden="true" tabindex="-1"></a>d.g1<span class="fl">.2</span> <span class="ot">&lt;-</span> m.g1 <span class="sc">-</span> m.g2</span>
<span id="cb69-2"><a href="lm.html#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(d.g1<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## [1] -2.209551</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="lm.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(d.g1<span class="fl">.2</span>, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## -3.128721 -1.342693</code></pre>
<p>The estimated difference is -2.2095511. We are 95% sure that the difference between the means of group 1 and 2 is between -3.1287208 and -1.3426929.</p>
<p>How strongly do the data support the hypothesis that the mean of group 2 is larger than the mean of group 1? To answer this question we calculate the proportion of the draws from the joint posterior distribution for which the mean of group 2 is larger than the mean of group 1.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="lm.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(m.g2 <span class="sc">&gt;</span> m.g1) <span class="sc">/</span> nsim </span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>This means that in all of the 1000 simulations from the joint posterior distribution, the mean of group 2 was larger than the mean of group 1. Therefore, there is a very high probability (i.e., it is close to 1; because probabilities are never exactly 1, we write &gt;0.999) that the mean of group 2 is larger than the mean of group 1.</p>
</div>
<div id="other-variants-of-normal-linear-models-two-way-anova-analysis-of-covariance-and-multiple-regression" class="section level3" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression</h3>
</div>
<div id="partial-coefficients-and-some-comments-on-collinearity" class="section level3" number="11.2.3">
<h3><span class="header-section-number">11.2.3</span> Partial coefficients and some comments on collinearity</h3>
<p>Many biologists think that it is forbidden to include correlated predictor variables in a model. They use variance inflating factors (VIF) to omit some of the variables. However, omitting important variables from the model just because a correlation coefficient exceeds a threshold value can have undesirable effects. Here, we explain why and we present the usefulness and limits of partial coefficients (also called partial correlation or partial effects). We start with an example illustrating the usefulness of partial coefficients and then, give some guidelines on how to assess and deal with collinearity.</p>
<p>As an example, we look at hatching dates of Snowfinches and how these dates relate to the date when snow melt started defined as the first date in the season when a minimum of 5% ground is snow free. A thorough analyses of the data is presented by <span class="citation">Schano et al. (<a href="referenzen.html#ref-Schano.2021" role="doc-biblioref">2021</a>)</span>. An important question is how well can Snowfinches adjust their hatching dates to the snow conditions. Snowfinch nestlings grow faster when they are reared during the snow melt compared to after snow has completely melted, because their parents find nutrient rich insect larvae in the edges of melting snow patches.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="lm.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;RData/snowfinch_hatching_date.rda&quot;</span>)</span>
<span id="cb75-2"><a href="lm.html#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="lm.html#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pearson&#39;s correlation coefficient</span></span>
<span id="cb75-4"><a href="lm.html#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(datsf<span class="sc">$</span>elevation, datsf<span class="sc">$</span>meltstart, <span class="at">use =</span> <span class="st">&quot;pairwise.complete&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.3274635</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="lm.html#cb77-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(meltstart<span class="sc">~</span>elevation, <span class="at">data=</span>datsf)</span>
<span id="cb77-2"><a href="lm.html#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="dv">100</span><span class="sc">*</span><span class="fu">coef</span>(mod)[<span class="dv">2</span>] <span class="co"># change in meltstart with 100m change in elevation</span></span></code></pre></div>
<pre><code>## elevation 
##   2.97768</code></pre>
<p>Hatching dates of Snowfinch broods were inferred from citizen science data from the Alps, where snow melt starts later at higher elevations compared to lower elevations. Thus, the start of snow melt is correlated with elevation (Pearson’s correlation coefficient 0.33). In average, snow starts melting 3 days later with every 100m increase in elevation.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="lm.html#cb79-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(hatchday.mean<span class="sc">~</span>meltstart, <span class="at">data=</span>datsf)</span>
<span id="cb79-2"><a href="lm.html#cb79-2" aria-hidden="true" tabindex="-1"></a>mod1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hatchday.mean ~ meltstart, data = datsf)
## 
## Coefficients:
## (Intercept)    meltstart  
##   167.99457      0.06325</code></pre>
<p>From a a normal linear regression of hatching date on the snow melt date, we obtain an estimate of 0.06 days delay in hatching date with one day later snow melt. This effect sizes describes what we see in the data, i.e. for data collected along an elevational gradient. Along the elevational gradient there are other factors than the start of snow melt that change such as average temperature, air pressure or sun radiation, and that may have an influence on the birds decision to start breeding. However, we are interested in the correlation between hatching date and date of snow melt independent of other factors changing with elevation. In other words, we would like to measure how much in average hatching date delays when snow melt starts one day later while all other factors are kept constant. This is called the partial effect of snow melt date. Therefore, we include elevation as a covariate in the model.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="lm.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb81-2"><a href="lm.html#cb81-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(hatchday.mean<span class="sc">~</span>elevation <span class="sc">+</span> meltstart, <span class="at">data=</span>datsf)</span>
<span id="cb81-3"><a href="lm.html#cb81-3" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hatchday.mean ~ elevation + meltstart, data = datsf)
## 
## Coefficients:
## (Intercept)    elevation    meltstart  
##  154.383936     0.007079     0.037757</code></pre>
<p>From this model, we obtain an estimate of 0.04 days delay in hatching date with one day later snow melt at a given elevation. We further get an estimate of 0.71 days later hatching date for each 100m shift in elevation. The difference in hatching date between early and late years (around one month difference in snow melt date) at a given elevation is 1.13 days (Figure <a href="lm.html#fig:sfexfig">11.8</a>).</p>
<p>The correlation between hatching date and start of snow melt date we estimate in the first model (black regression line in Figure <a href="lm.html#fig:sfexfig">11.8</a>), that does not include elevation, is called “pseudocorrelation” because its is not purely “caused” by the date of snow melt. When we estimate the correlation within a constant elevation (coloured regression lines), it is much lower and closer to a causal relationship. However, in such observational studies, we never can be sure whether the partial coefficients can be interpreted as a causal relationship unless we include all factors that influence hatching date. Nevertheless, partial effects give much more insight into a system compared to univariate analyses because we can separated effects of simultaneously acting variables.</p>
<div class="figure"><span id="fig:sfexfig"></span>
<img src="2.03-lm_files/figure-html/sfexfig-1.png" alt="Illustration of the partial coefficient of snow melt date in a  model of hatching date. Panel A shows the entire raw data together with the regression lines drawn for three different elevations. The regression lines span the range of snow melt dates occurring at the respective elevation (shown in panel C). Panel B is the same as panel A, but zoomed in to the better see the regression lines and with an additional regression line (in black) from the model that does not take elevation into account is added." width="672" />
<p class="caption">
Figure 11.8: Illustration of the partial coefficient of snow melt date in a model of hatching date. Panel A shows the entire raw data together with the regression lines drawn for three different elevations. The regression lines span the range of snow melt dates occurring at the respective elevation (shown in panel C). Panel B is the same as panel A, but zoomed in to the better see the regression lines and with an additional regression line (in black) from the model that does not take elevation into account is added.
</p>
</div>
<p>We have seen that it can be very useful to include more than one predictor variable in a model even if they are correlated with each other. In fact, there is nothing wrong with that. However, correlated predictors (collinearity) make things more complicated.</p>
<p>For example, partial regression lines should not be drawn across the whole range of values of a variable, to avoid extrapolating out of data. At 2800 m asl snow melt never starts in the beginning of March. Therefore, the blue regression line would not make sense for snow melt dates in March.</p>
<p>Further, sometimes correlations among predictors indicate that these predictors measure the same underlying aspect and we are actually interested in the effect of this underlying aspect on our response. For example, we could include also the date of the end of snow melt. Both variables, the start and the end of the snow melt measure the timing of snow melt. Including both as predictor in the model would result in partial coefficients that measure how much hatching date changes when the snow melt starts one day later, while the end date is constant. That interpretation is a mixture of the effect of timing and duration rather than of snow melt timing alone. Similarly, the coefficient of the end of snow melt measures a mixture of duration and timing. Thus, if we include two variables that are correlated because they measure the same aspect (just a little bit differently), we get coefficients that are hard to interpret and may not measure what we actually are interested in. In such a cases, we get easier to interpret model coefficients, if we include just one variable of each aspect that we are interested in, e.g. we could include one timing variable (e.g. start of snow melt) and the duration of snow melt that may or may not be correlated with the start of snow melt.<br />
To summarize, the decision of what to do with correlated predictors primarily relies on the question we are interested in, i.e., what exactly should the partial coefficients be an estimate for.</p>
<p>A further drawback of collinearity is that model fitting can become difficult. When strong correlations are present, model fitting algorithms may fail. If they do not fail, the statistical uncertainty of the estimates often becomes large. This is because the partial coefficient of one variable needs to be estimated for constant values of the other predictors in the model which means that a reduced range of values is available as illustrated in Figure <a href="lm.html#fig:sfexfig">11.8</a> C. However, if uncertainty intervals (confidence, credible or compatibility intervals) are reported alongside the estimates, then using correlated predictors in the same model is absolutely fine, if the fitting algorithm was successful.</p>
<p>The correlations per se can be interesting. Further readings on how to visualize and analyse data with complex correlation structures:</p>
<ul>
<li>principal component analysis <span class="citation">(<a href="referenzen.html#ref-Manly.1994" role="doc-biblioref">Manly 1994</a>)</span><br />
</li>
<li>path analyses (e.g. <span class="citation">Shipley (<a href="referenzen.html#ref-Shipley.2009" role="doc-biblioref">2009</a>)</span>)<br />
</li>
<li>structural equation models <span class="citation">(<a href="referenzen.html#ref-Hoyle2012" role="doc-biblioref">Hoyle 2012</a>)</span></li>
</ul>
<p><a href="" target="_blank"><img src="images/ruchen.jpg" width="1452" style="display: block; margin: auto;" /></a></p>
</div>
<div id="orderedfactors" class="section level3" number="11.2.4">
<h3><span class="header-section-number">11.2.4</span> Ordered Factors and Contrasts</h3>
</div>
<div id="quadratic-and-higher-polynomial-terms" class="section level3" number="11.2.5">
<h3><span class="header-section-number">11.2.5</span> Quadratic and Higher Polynomial Terms</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="priors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="residualanalysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/TobiasRoth/BDAEcology/edit/master/2.03-lm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
