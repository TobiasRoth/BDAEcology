# Data analysis step by step {#analyses_steps}

In this chapter we provide a checklist with some guidance for data analysis. However, do not expect the list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps \@ref(step2) to \@ref(step8) until we find a model that fits the data well and that is realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful model. 

There is a chance and danger at the same time: we may find interesting results that answer different questions than those we originally asked. They may be very exciting and important, however they may be biased. We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we must be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process.

## Plausibility of data {#step1}
Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries.

## Relationships {#step2}
Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful. We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible.

## Data distribution  {#step3}
What is the nature of the variable of interest (i.e. the outcome variable)? At this stage, there is no need to formally compare the distribution of the outcome variable to a statistical distribution, because the raw data is not required to follow a specific distribution. The models assume that, conditional on the explanatory variables and the model structure, the outcome variable follows a specific distribution. Therefore, checking how well the chosen distribution fits the data is done after the model fit (Chapter \@ref(step8)). This first choice is solely based on the nature of the data. Normally, our first choice is one of the classical distributions for which robust software for model fitting is available. 

Here is a rough guideline for this first choice:


type of outcome variable | | distribution
:---- | :- | :-----
continuous measurements | &#10233; | normal = Gauss |
time-to-event data | &#10233; | (see survival analysis) |
count | &#10233; | Poisson or negative-binomial|
count with upper bound (proportion) | &#10233; | binomial |
binary | &#10233; | Bernoulli |
rate (count by a reference) | &#10233; | Poisson or negative-binomial with offset |
nominal | &#10233; | multinomial distribution |


Chapter \@ref(distributions) gives an overview of the distributions that are most relevant for ecologists.

## Preparation of explanatory variables  {#step4}
1. Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus, there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step:  

a) Is the variance (of the explanatory variable) big enough that an effect of the variable can be measured?  

b) Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable (e.g., log, square-root; see Chapter \@ref(transformations)).  

c) Does it show a bimodal distribution? Consider making the variable binary.  

d) Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a transformation (e.g., log) could linearize the relationship between x and y (see Chapter \@ref(transformations)).  

2. Centering: Centering ($x.c = x-mean(x)$) often makes sense for continuous predictors, making coefficients more interpretable (see Chapter \@ref(transformations))

3. Scaling: Scaling ($x.s = x/c$, where $c$ is a constant) is a transformation that changes the unit of the variable. Also scaling is optional. We have three reasons to scale an predictor variable. 1) To make the effect sizes better understandable. For example, a population change from one year to the next may be very small and hard to interpret. When we give the change for a 10-year period, its ecological meaning is better understandable. 2) To make the estimate of the effect sizes comparable between variables, we may use $x.s = x/sd(x)$. The resulting variable has a unit of one standard deviation. A standard deviation may be comparable between variables that originally are measured in different units (meters, seconds etc). @Gelman.2007 (p. 55 f) suggest to scale the variables by two times the standard deviation ($x.s = x/(2*sd(x))$) to make effect sizes comparable between numeric and binary variables. 3) Scaling can be important for model convergence, especially when polynomials are included. Also, consider the use of orthogonal polynomials, see Chapter 4.2.9 in @KornerNievergelt.2015.

4. Collinearity: Look at the correlation among the explanatory variables (using a pairs plot or a correlation matrix). If the explanatory variables are correlated, go back to step 2 and add this relationship. Further, read our thoughts about [collinearity](#collinearity).

5. Are [polynomial terms](#polynomials) needed in the model? If not already
done in step 2, think about the relationship between each explanatory variable and the dependent variable. Is it linear, or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM).  

6. Interactions: Could the effect of one explanatory variable depend on the value of
another explanatory variable (interaction)?


## Data structure  {#step5}
After having considered the (fixed effect) terms from step 4: are the observations independent or grouped/structured? What random factors are needed in the model? Are the data obviously temporally or spatially correlated? Or, are other correlation structures present, such as phylogenetic relationships? 
Our strategy is to start with a rather simple model that may not account for all correlation structures that in fact are present in the data. We first, only include those that are known to be important a priory. Only when residual analyses reveals important additional correlation structures, we include them in the model.


## Define prior distributions {#step6}
Decide whether we would like to use informative prior distributions or whether we would like use priors that only have a negligible effect on the results. When the results are later used for informing authorities or for making a decision (as usual in applied sciences), then we would like to base the results on all information available. Information from the literature is then used to construct informative prior distributions. In contrast to applied sciences, in basic research we often would like to show only the information in the data that should not be influenced by earlier results. Therefore, in basic research we look for priors that do not influence the results. 


## Fit the model  {#step7}
Fit the model.

## Check model {#step8}
We assess model fit by graphical [analyses of the residuals](#residualanalysis) and by [predictive model checking](#modelchecking). 

For non-Gaussian models it is often easier to assess model fit using [posterior predictive checks](#modelchecking) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, we list in Chapter 16 of @KornerNievergelt.2015 some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases.

## Model uncertainty  {#step9}
If, while working through steps 1 to 8, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we [model comparison](#model_comparison) is needed or inference may be drawn from more than one model. If we have only one model, we proceed to \@ref(step10).

## Present model results  {#step10}
Draw values from the joint posterior distribution of the model parameters, usnig `sim` or Stan via rstanarm-package or `brm`. Use these draws to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities, and show effect plots.

## Further reading {-}

- [R for Data Science by Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz): Introduces the tidyverse framework. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it.  




