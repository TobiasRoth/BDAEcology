<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Basics of statistics | Bayesian Data Analysis in Ecology with R and Stan</title>
  <meta name="description" content="This GitHub-book is a collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Basics of statistics | Bayesian Data Analysis in Ecology with R and Stan" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/cover.jpg" />
  <meta property="og:description" content="This GitHub-book is a collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="github-repo" content="TobiasRoth/BDAEcology" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Basics of statistics | Bayesian Data Analysis in Ecology with R and Stan" />
  
  <meta name="twitter:description" content="This GitHub-book is a collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="twitter:image" content="/images/cover.jpg" />

<meta name="author" content="Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Louis Hunninck, Pius Korner-Nievergelt" />


<meta name="date" content="2025-11-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="PART-I.html"/>
<link rel="next" href="analyses_steps.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="settings/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book"><i class="fa fa-check"></i>Why this book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-contribute"><i class="fa fa-check"></i>How to contribute</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="part"><span><b>I BASIC STATISTICS FOR ECOLOGISTS</b></span></li>
<li class="chapter" data-level="1" data-path="PART-I.html"><a href="PART-I.html"><i class="fa fa-check"></i><b>1</b> Introduction to PART I</a>
<ul>
<li class="chapter" data-level="1.1" data-path="PART-I.html"><a href="PART-I.html#further-reading"><i class="fa fa-check"></i><b>1.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>2</b> Basics of statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basics.html"><a href="basics.html#variables-and-observations"><i class="fa fa-check"></i><b>2.1</b> Variables and observations</a></li>
<li class="chapter" data-level="2.2" data-path="basics.html"><a href="basics.html#displaying-and-summarizing-data"><i class="fa fa-check"></i><b>2.2</b> Displaying and summarizing data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="basics.html"><a href="basics.html#histogram"><i class="fa fa-check"></i><b>2.2.1</b> Histogram</a></li>
<li class="chapter" data-level="2.2.2" data-path="basics.html"><a href="basics.html#location-and-scatter"><i class="fa fa-check"></i><b>2.2.2</b> Location and scatter</a></li>
<li class="chapter" data-level="2.2.3" data-path="basics.html"><a href="basics.html#correlations"><i class="fa fa-check"></i><b>2.2.3</b> Correlations</a></li>
<li class="chapter" data-level="2.2.4" data-path="basics.html"><a href="basics.html#principal-components-analyses-pca"><i class="fa fa-check"></i><b>2.2.4</b> Principal components analyses PCA</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="basics.html"><a href="basics.html#inferential-statistics"><i class="fa fa-check"></i><b>2.3</b> Inferential statistics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="basics.html"><a href="basics.html#uncertainty"><i class="fa fa-check"></i><b>2.3.1</b> Uncertainty</a></li>
<li class="chapter" data-level="2.3.2" data-path="basics.html"><a href="basics.html#standard-error"><i class="fa fa-check"></i><b>2.3.2</b> Standard error</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="basics.html"><a href="basics.html#bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods"><i class="fa fa-check"></i><b>2.4</b> Bayes theorem and the common aim of frequentist and Bayesian methods</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="basics.html"><a href="basics.html#bayes-theorem-for-discrete-events"><i class="fa fa-check"></i><b>2.4.1</b> Bayes theorem for discrete events</a></li>
<li class="chapter" data-level="2.4.2" data-path="basics.html"><a href="basics.html#bayes-theorem-for-continuous-parameters"><i class="fa fa-check"></i><b>2.4.2</b> Bayes theorem for continuous parameters</a></li>
<li class="chapter" data-level="2.4.3" data-path="basics.html"><a href="basics.html#estimating-a-mean-assuming-that-the-variance-is-known"><i class="fa fa-check"></i><b>2.4.3</b> Estimating a mean assuming that the variance is known</a></li>
<li class="chapter" data-level="2.4.4" data-path="basics.html"><a href="basics.html#estimating-the-mean-and-the-variance"><i class="fa fa-check"></i><b>2.4.4</b> Estimating the mean and the variance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basics.html"><a href="basics.html#classical-frequentist-tests-and-alternatives"><i class="fa fa-check"></i><b>2.5</b> Classical frequentist tests and alternatives</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="basics.html"><a href="basics.html#nullhypothesis-testing"><i class="fa fa-check"></i><b>2.5.1</b> Nullhypothesis testing</a></li>
<li class="chapter" data-level="2.5.2" data-path="basics.html"><a href="basics.html#comparison-of-a-sample-with-a-fixed-value-one-sample-t-test"><i class="fa fa-check"></i><b>2.5.2</b> Comparison of a sample with a fixed value (one-sample t-test)</a></li>
<li class="chapter" data-level="2.5.3" data-path="basics.html"><a href="basics.html#comparison-of-the-locations-between-two-groups-two-sample-t-test"><i class="fa fa-check"></i><b>2.5.3</b> Comparison of the locations between two groups (two-sample t-test)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="basics.html"><a href="basics.html#whyBayes"><i class="fa fa-check"></i><b>2.6</b> Comparing frequentist and Bayesian approach - an why we use Bayes</a></li>
<li class="chapter" data-level="2.7" data-path="basics.html"><a href="basics.html#summary"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyses_steps.html"><a href="analyses_steps.html"><i class="fa fa-check"></i><b>3</b> Data analysis step by step</a>
<ul>
<li class="chapter" data-level="3.1" data-path="analyses_steps.html"><a href="analyses_steps.html#step1"><i class="fa fa-check"></i><b>3.1</b> Plausibility of data</a></li>
<li class="chapter" data-level="3.2" data-path="analyses_steps.html"><a href="analyses_steps.html#step2"><i class="fa fa-check"></i><b>3.2</b> Relationships</a></li>
<li class="chapter" data-level="3.3" data-path="analyses_steps.html"><a href="analyses_steps.html#step3"><i class="fa fa-check"></i><b>3.3</b> Data distribution</a></li>
<li class="chapter" data-level="3.4" data-path="analyses_steps.html"><a href="analyses_steps.html#step4"><i class="fa fa-check"></i><b>3.4</b> Preparation of explanatory variables</a></li>
<li class="chapter" data-level="3.5" data-path="analyses_steps.html"><a href="analyses_steps.html#step5"><i class="fa fa-check"></i><b>3.5</b> Data structure</a></li>
<li class="chapter" data-level="3.6" data-path="analyses_steps.html"><a href="analyses_steps.html#step6"><i class="fa fa-check"></i><b>3.6</b> Define prior distributions</a></li>
<li class="chapter" data-level="3.7" data-path="analyses_steps.html"><a href="analyses_steps.html#step7"><i class="fa fa-check"></i><b>3.7</b> Fit the model</a></li>
<li class="chapter" data-level="3.8" data-path="analyses_steps.html"><a href="analyses_steps.html#step8"><i class="fa fa-check"></i><b>3.8</b> Check model</a></li>
<li class="chapter" data-level="3.9" data-path="analyses_steps.html"><a href="analyses_steps.html#step9"><i class="fa fa-check"></i><b>3.9</b> Model uncertainty</a></li>
<li class="chapter" data-level="3.10" data-path="analyses_steps.html"><a href="analyses_steps.html#step10"><i class="fa fa-check"></i><b>3.10</b> Present model results</a></li>
<li class="chapter" data-level="" data-path="analyses_steps.html"><a href="analyses_steps.html#further-reading-1"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>4</b> Probability distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distributions.html"><a href="distributions.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="distributions.html"><a href="distributions.html#discrete-distributions"><i class="fa fa-check"></i><b>4.2</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distributions.html"><a href="distributions.html#bernoulli-dist"><i class="fa fa-check"></i><b>4.2.1</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="4.2.2" data-path="distributions.html"><a href="distributions.html#binomial-dist"><i class="fa fa-check"></i><b>4.2.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="4.2.3" data-path="distributions.html"><a href="distributions.html#poisson"><i class="fa fa-check"></i><b>4.2.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="4.2.4" data-path="distributions.html"><a href="distributions.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.2.4</b> Negative-binomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="distributions.html"><a href="distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>4.3</b> Continuous distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="distributions.html"><a href="distributions.html#beta-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="distributions.html"><a href="distributions.html#normdist"><i class="fa fa-check"></i><b>4.3.2</b> Normal distribution</a></li>
<li class="chapter" data-level="4.3.3" data-path="distributions.html"><a href="distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.3</b> Gamma distribution</a></li>
<li class="chapter" data-level="4.3.4" data-path="distributions.html"><a href="distributions.html#cauchydistri"><i class="fa fa-check"></i><b>4.3.4</b> Cauchy distribution</a></li>
<li class="chapter" data-level="4.3.5" data-path="distributions.html"><a href="distributions.html#t-distribution"><i class="fa fa-check"></i><b>4.3.5</b> t-distribution</a></li>
<li class="chapter" data-level="4.3.6" data-path="distributions.html"><a href="distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.3.6</b> F-distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>5</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="transformations.html"><a href="transformations.html#some-r-specific-aspects"><i class="fa fa-check"></i><b>5.1</b> Some R-specific aspects</a></li>
<li class="chapter" data-level="5.2" data-path="transformations.html"><a href="transformations.html#first-aid-transformations"><i class="fa fa-check"></i><b>5.2</b> First-aid transformations</a></li>
<li class="chapter" data-level="5.3" data-path="transformations.html"><a href="transformations.html#logtransformation"><i class="fa fa-check"></i><b>5.3</b> Log-transformation with Stahel</a></li>
<li class="chapter" data-level="5.4" data-path="transformations.html"><a href="transformations.html#ztransformation"><i class="fa fa-check"></i><b>5.4</b> Centering and scaling (z-transformation)</a></li>
<li class="chapter" data-level="5.5" data-path="transformations.html"><a href="transformations.html#transformationspolynomials"><i class="fa fa-check"></i><b>5.5</b> Raw and orthogonal polynomials</a></li>
<li class="chapter" data-level="5.6" data-path="transformations.html"><a href="transformations.html#square-root-transformation"><i class="fa fa-check"></i><b>5.6</b> Square-root transformation</a></li>
<li class="chapter" data-level="5.7" data-path="transformations.html"><a href="transformations.html#arcsinus-square-root-transformation"><i class="fa fa-check"></i><b>5.7</b> Arcsinus-square-root transformation</a></li>
<li class="chapter" data-level="5.8" data-path="transformations.html"><a href="transformations.html#logit-transformation"><i class="fa fa-check"></i><b>5.8</b> Logit transformation</a></li>
<li class="chapter" data-level="5.9" data-path="transformations.html"><a href="transformations.html#categorizing-and-decategorizing"><i class="fa fa-check"></i><b>5.9</b> Categorizing and decategorizing</a></li>
<li class="chapter" data-level="5.10" data-path="transformations.html"><a href="transformations.html#transformationscircular"><i class="fa fa-check"></i><b>5.10</b> Sinus and cosinus transformation for circular variables</a></li>
<li class="chapter" data-level="5.11" data-path="transformations.html"><a href="transformations.html#cloglog-probit-inverse-transformation"><i class="fa fa-check"></i><b>5.11</b> Cloglog, probit, inverse transformation</a></li>
<li class="chapter" data-level="5.12" data-path="transformations.html"><a href="transformations.html#identity-transformation"><i class="fa fa-check"></i><b>5.12</b> Identity transformation</a></li>
<li class="chapter" data-level="5.13" data-path="transformations.html"><a href="transformations.html#transformations-on-the-outcome-variable"><i class="fa fa-check"></i><b>5.13</b> Transformations on the outcome variable</a></li>
<li class="chapter" data-level="5.14" data-path="transformations.html"><a href="transformations.html#back-transformation"><i class="fa fa-check"></i><b>5.14</b> Back-transformation</a></li>
<li class="chapter" data-level="5.15" data-path="transformations.html"><a href="transformations.html#applythesametransformation"><i class="fa fa-check"></i><b>5.15</b> Applying the transformations to new data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html"><i class="fa fa-check"></i><b>6</b> Reproducible research</a>
<ul>
<li class="chapter" data-level="6.1" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html#summary-1"><i class="fa fa-check"></i><b>6.1</b> Summary</a></li>
<li class="chapter" data-level="6.2" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html#further-reading-2"><i class="fa fa-check"></i><b>6.2</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="furthertopics.html"><a href="furthertopics.html"><i class="fa fa-check"></i><b>7</b> Further topics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="furthertopics.html"><a href="furthertopics.html#bioacoustic-analyse"><i class="fa fa-check"></i><b>7.1</b> Bioacoustic analyse</a></li>
<li class="chapter" data-level="7.2" data-path="furthertopics.html"><a href="furthertopics.html#python"><i class="fa fa-check"></i><b>7.2</b> Python</a></li>
<li class="chapter" data-level="7.3" data-path="furthertopics.html"><a href="furthertopics.html#some-r"><i class="fa fa-check"></i><b>7.3</b> Some R</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="furthertopics.html"><a href="furthertopics.html#date-on-x-axis"><i class="fa fa-check"></i><b>7.3.1</b> Date on x-axis</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II BAYESIAN DATA ANALYSIS</b></span></li>
<li class="chapter" data-level="8" data-path="PART-II.html"><a href="PART-II.html"><i class="fa fa-check"></i><b>8</b> Introduction to PART II</a>
<ul>
<li class="chapter" data-level="" data-path="PART-II.html"><a href="PART-II.html#further-reading-3"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html"><i class="fa fa-check"></i><b>9</b> The Bayesian paradigm and likelihood in a frequentist and Bayesian framework</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#short-historical-overview"><i class="fa fa-check"></i><b>9.1</b> Short historical overview</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#the-bayesian-way"><i class="fa fa-check"></i><b>9.2</b> The Bayesian way</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#likelihood"><i class="fa fa-check"></i><b>9.3</b> Likelihood</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#theory"><i class="fa fa-check"></i><b>9.3.1</b> Theory</a></li>
<li class="chapter" data-level="9.3.2" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#the-maximum-likelihood-method"><i class="fa fa-check"></i><b>9.3.2</b> The maximum likelihood method</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#the-log-pointwise-predictive-density"><i class="fa fa-check"></i><b>9.4</b> The log pointwise predictive density</a></li>
<li class="chapter" data-level="9.5" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#further-reading-4"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="priors.html"><a href="priors.html"><i class="fa fa-check"></i><b>10</b> Prior distributions and prior sensitivity analyses</a>
<ul>
<li class="chapter" data-level="10.1" data-path="priors.html"><a href="priors.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="priors.html"><a href="priors.html#choosepriors"><i class="fa fa-check"></i><b>10.2</b> How to choose a prior</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="priors.html"><a href="priors.html#priors-for-variance-parameters"><i class="fa fa-check"></i><b>10.2.1</b> Priors for variance parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="priors.html"><a href="priors.html#prior-sensitivity"><i class="fa fa-check"></i><b>10.3</b> Prior sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>11</b> Normal Linear Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lm.html"><a href="lm.html#linear-regression"><i class="fa fa-check"></i><b>11.1</b> Linear regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="lm.html"><a href="lm.html#background"><i class="fa fa-check"></i><b>11.1.1</b> Background</a></li>
<li class="chapter" data-level="11.1.2" data-path="lm.html"><a href="lm.html#fitting-a-linear-regression-in-r"><i class="fa fa-check"></i><b>11.1.2</b> Fitting a linear regression in R</a></li>
<li class="chapter" data-level="11.1.3" data-path="lm.html"><a href="lm.html#presenting-the-results"><i class="fa fa-check"></i><b>11.1.3</b> Presenting the results</a></li>
<li class="chapter" data-level="11.1.4" data-path="lm.html"><a href="lm.html#interpretation-of-the-r-summary-output"><i class="fa fa-check"></i><b>11.1.4</b> Interpretation of the R summary output</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="lm.html"><a href="lm.html#linear-model-with-one-categorical-predictor-one-way-anova"><i class="fa fa-check"></i><b>11.2</b> Linear model with one categorical predictor (one-way ANOVA)</a></li>
<li class="chapter" data-level="11.3" data-path="lm.html"><a href="lm.html#other-variants-of-normal-linear-models"><i class="fa fa-check"></i><b>11.3</b> Other variants of normal linear models</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="lm.html"><a href="lm.html#twowayanova"><i class="fa fa-check"></i><b>11.3.1</b> Linear model with two categorical predictors (two-way ANOVA)</a></li>
<li class="chapter" data-level="11.3.2" data-path="lm.html"><a href="lm.html#a-linear-model-with-a-categorical-and-a-numeric-predictor-ancova"><i class="fa fa-check"></i><b>11.3.2</b> A linear model with a categorical and a numeric predictor (ANCOVA)</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="lm.html"><a href="lm.html#collinearity"><i class="fa fa-check"></i><b>11.4</b> Partial coefficients and some comments on collinearity</a></li>
<li class="chapter" data-level="11.5" data-path="lm.html"><a href="lm.html#orderedfactors"><i class="fa fa-check"></i><b>11.5</b> Ordered factors and contrasts</a></li>
<li class="chapter" data-level="11.6" data-path="lm.html"><a href="lm.html#lmpolynomials"><i class="fa fa-check"></i><b>11.6</b> Quadratic and higher polynomial terms</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="residualanalysis.html"><a href="residualanalysis.html"><i class="fa fa-check"></i><b>12</b> Assessing model assumptions</a>
<ul>
<li class="chapter" data-level="12.1" data-path="residualanalysis.html"><a href="residualanalysis.html#model-assumptions"><i class="fa fa-check"></i><b>12.1</b> Model assumptions</a></li>
<li class="chapter" data-level="12.2" data-path="residualanalysis.html"><a href="residualanalysis.html#independent-and-identically-distributed"><i class="fa fa-check"></i><b>12.2</b> Independent and identically distributed</a></li>
<li class="chapter" data-level="12.3" data-path="residualanalysis.html"><a href="residualanalysis.html#qqplot"><i class="fa fa-check"></i><b>12.3</b> The QQ-plot</a></li>
<li class="chapter" data-level="12.4" data-path="residualanalysis.html"><a href="residualanalysis.html#tempautocorrelation"><i class="fa fa-check"></i><b>12.4</b> Temporal autocorrelation</a></li>
<li class="chapter" data-level="12.5" data-path="residualanalysis.html"><a href="residualanalysis.html#spatialautocorrelation"><i class="fa fa-check"></i><b>12.5</b> Spatial autocorrelation</a></li>
<li class="chapter" data-level="12.6" data-path="residualanalysis.html"><a href="residualanalysis.html#Heteroscedasticity"><i class="fa fa-check"></i><b>12.6</b> Heteroscedasticity</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lmer.html"><a href="lmer.html"><i class="fa fa-check"></i><b>13</b> Linear mixed effect models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="lmer.html"><a href="lmer.html#background-2"><i class="fa fa-check"></i><b>13.1</b> Background</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="lmer.html"><a href="lmer.html#why-mixed-effects-models"><i class="fa fa-check"></i><b>13.1.1</b> Why mixed effects models?</a></li>
<li class="chapter" data-level="13.1.2" data-path="lmer.html"><a href="lmer.html#shrinkage"><i class="fa fa-check"></i><b>13.1.2</b> Random factors and partial pooling</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="lmer.html"><a href="lmer.html#fitting-a-normal-linear-mixed-model-in-r"><i class="fa fa-check"></i><b>13.2</b> Fitting a normal linear mixed model in R</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="lmer.html"><a href="lmer.html#background-3"><i class="fa fa-check"></i><b>13.2.1</b> Background</a></li>
<li class="chapter" data-level="13.2.2" data-path="lmer.html"><a href="lmer.html#fitting-a-normal-linear-mixed-model-using-lmer-then-use-sim"><i class="fa fa-check"></i><b>13.2.2</b> Fitting a normal linear mixed model using lmer, then use sim</a></li>
<li class="chapter" data-level="13.2.3" data-path="lmer.html"><a href="lmer.html#fitting-a-normal-linear-model-using-rstanarm"><i class="fa fa-check"></i><b>13.2.3</b> Fitting a normal linear model using rstanarm</a></li>
<li class="chapter" data-level="13.2.4" data-path="lmer.html"><a href="lmer.html#fitting-a-normal-linear-model-using-brm"><i class="fa fa-check"></i><b>13.2.4</b> Fitting a normal linear model using brm</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="lmer.html"><a href="lmer.html#presenting-the-results-1"><i class="fa fa-check"></i><b>13.3</b> Presenting the results</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="lmer.html"><a href="lmer.html#presenting-the-results-from-sim"><i class="fa fa-check"></i><b>13.3.1</b> Presenting the results: from sim</a></li>
<li class="chapter" data-level="13.3.2" data-path="lmer.html"><a href="lmer.html#presenting-the-results-from-rstanarm"><i class="fa fa-check"></i><b>13.3.2</b> Presenting the results: from rstanarm</a></li>
<li class="chapter" data-level="13.3.3" data-path="lmer.html"><a href="lmer.html#presenting-the-results-from-brms"><i class="fa fa-check"></i><b>13.3.3</b> Presenting the results: from brms</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="lmer.html"><a href="lmer.html#random-intercept-and-slope"><i class="fa fa-check"></i><b>13.4</b> Random intercept and slope</a></li>
<li class="chapter" data-level="13.5" data-path="lmer.html"><a href="lmer.html#nested-and-crossed-random-effects"><i class="fa fa-check"></i><b>13.5</b> Nested and crossed random effects</a></li>
<li class="chapter" data-level="13.6" data-path="lmer.html"><a href="lmer.html#model-selection-in-mixed-models"><i class="fa fa-check"></i><b>13.6</b> Model selection in mixed models</a></li>
<li class="chapter" data-level="13.7" data-path="lmer.html"><a href="lmer.html#further-reading-5"><i class="fa fa-check"></i><b>13.7</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>14</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="glm.html"><a href="glm.html#introduction-2"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="glm.html"><a href="glm.html#bernoulli-model"><i class="fa fa-check"></i><b>14.2</b> Bernoulli model</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="glm.html"><a href="glm.html#background-4"><i class="fa fa-check"></i><b>14.2.1</b> Background</a></li>
<li class="chapter" data-level="14.2.2" data-path="glm.html"><a href="glm.html#fitting-a-bernoulli-model-in-r"><i class="fa fa-check"></i><b>14.2.2</b> Fitting a Bernoulli model in R</a></li>
<li class="chapter" data-level="14.2.3" data-path="glm.html"><a href="glm.html#assessing-model-assumptions-in-a-bernoulli-model"><i class="fa fa-check"></i><b>14.2.3</b> Assessing model assumptions in a Bernoulli model</a></li>
<li class="chapter" data-level="14.2.4" data-path="glm.html"><a href="glm.html#visualising-the-results"><i class="fa fa-check"></i><b>14.2.4</b> Visualising the results</a></li>
<li class="chapter" data-level="14.2.5" data-path="glm.html"><a href="glm.html#some-remarks"><i class="fa fa-check"></i><b>14.2.5</b> Some remarks</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="glm.html"><a href="glm.html#binomial-model"><i class="fa fa-check"></i><b>14.3</b> Binomial model</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="glm.html"><a href="glm.html#background-5"><i class="fa fa-check"></i><b>14.3.1</b> Background</a></li>
<li class="chapter" data-level="14.3.2" data-path="glm.html"><a href="glm.html#fitting-a-binomial-model-in-r"><i class="fa fa-check"></i><b>14.3.2</b> Fitting a binomial model in R</a></li>
<li class="chapter" data-level="14.3.3" data-path="glm.html"><a href="glm.html#assessing-assumptions-in-a-binomial-model"><i class="fa fa-check"></i><b>14.3.3</b> Assessing assumptions in a binomial model</a></li>
<li class="chapter" data-level="14.3.4" data-path="glm.html"><a href="glm.html#visualising-the-results-1"><i class="fa fa-check"></i><b>14.3.4</b> Visualising the results</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="glm.html"><a href="glm.html#poisson-model"><i class="fa fa-check"></i><b>14.4</b> Poisson model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="glm.html"><a href="glm.html#background-6"><i class="fa fa-check"></i><b>14.4.1</b> Background</a></li>
<li class="chapter" data-level="14.4.2" data-path="glm.html"><a href="glm.html#fitting-a-poisson-model-in-r"><i class="fa fa-check"></i><b>14.4.2</b> Fitting a Poisson model in R</a></li>
<li class="chapter" data-level="14.4.3" data-path="glm.html"><a href="glm.html#assessing-model-assumptions"><i class="fa fa-check"></i><b>14.4.3</b> Assessing model assumptions</a></li>
<li class="chapter" data-level="14.4.4" data-path="glm.html"><a href="glm.html#visualising-results"><i class="fa fa-check"></i><b>14.4.4</b> Visualising results</a></li>
<li class="chapter" data-level="14.4.5" data-path="glm.html"><a href="glm.html#modeling-rates-and-densities-poisson-model-with-an-offset"><i class="fa fa-check"></i><b>14.4.5</b> Modeling rates and densities: Poisson model with an offset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="glmm.html"><a href="glmm.html"><i class="fa fa-check"></i><b>15</b> Generalized linear mixed models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="glmm.html"><a href="glmm.html#background-7"><i class="fa fa-check"></i><b>15.1</b> Background</a></li>
<li class="chapter" data-level="15.2" data-path="glmm.html"><a href="glmm.html#binomial-mixed-model"><i class="fa fa-check"></i><b>15.2</b> Binomial mixed model</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="glmm.html"><a href="glmm.html#background-8"><i class="fa fa-check"></i><b>15.2.1</b> Background</a></li>
<li class="chapter" data-level="15.2.2" data-path="glmm.html"><a href="glmm.html#fitting-a-binomial-mixed-model-in-r"><i class="fa fa-check"></i><b>15.2.2</b> Fitting a binomial mixed model in R</a></li>
<li class="chapter" data-level="15.2.3" data-path="glmm.html"><a href="glmm.html#presenting-the-results-2"><i class="fa fa-check"></i><b>15.2.3</b> Presenting the results</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="glmm.html"><a href="glmm.html#poisson-mixed-model"><i class="fa fa-check"></i><b>15.3</b> Poisson mixed model</a></li>
<li class="chapter" data-level="15.4" data-path="glmm.html"><a href="glmm.html#summary-2"><i class="fa fa-check"></i><b>15.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modelchecking.html"><a href="modelchecking.html"><i class="fa fa-check"></i><b>16</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="17" data-path="model_comparison.html"><a href="model_comparison.html"><i class="fa fa-check"></i><b>17</b> Model comparison and multimodel inference</a>
<ul>
<li class="chapter" data-level="17.1" data-path="model_comparison.html"><a href="model_comparison.html#when-and-why-we-compare-models-and-why-model-selection-is-difficult"><i class="fa fa-check"></i><b>17.1</b> When and why we compare models and why model selection is difficult</a></li>
<li class="chapter" data-level="17.2" data-path="model_comparison.html"><a href="model_comparison.html#methods-for-model-commparison"><i class="fa fa-check"></i><b>17.2</b> Methods for model commparison</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="model_comparison.html"><a href="model_comparison.html#cross-validation"><i class="fa fa-check"></i><b>17.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="17.2.2" data-path="model_comparison.html"><a href="model_comparison.html#information-criteria-akaike-information-criterion-and-widely-applicable-information-criterion"><i class="fa fa-check"></i><b>17.2.2</b> Information criteria: Akaike information criterion and widely applicable information criterion</a></li>
<li class="chapter" data-level="17.2.3" data-path="model_comparison.html"><a href="model_comparison.html#other-information-criteria"><i class="fa fa-check"></i><b>17.2.3</b> Other information criteria</a></li>
<li class="chapter" data-level="17.2.4" data-path="model_comparison.html"><a href="model_comparison.html#bayes-factors-and-posterior-model-probabilities"><i class="fa fa-check"></i><b>17.2.4</b> Bayes factors and posterior model probabilities</a></li>
<li class="chapter" data-level="17.2.5" data-path="model_comparison.html"><a href="model_comparison.html#model-based-methods-to-obtain-posterior-model-probabilities-and-inclusion-probabilities"><i class="fa fa-check"></i><b>17.2.5</b> Model-based methods to obtain posterior model probabilities and inclusion probabilities</a></li>
<li class="chapter" data-level="17.2.6" data-path="model_comparison.html"><a href="model_comparison.html#least-absolute-shrinkage-and-selection-operator-lasso-and-ridge-regression"><i class="fa fa-check"></i><b>17.2.6</b> Least absolute shrinkage and selection operator (LASSO) and ridge regression</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="model_comparison.html"><a href="model_comparison.html#multimodel-inference"><i class="fa fa-check"></i><b>17.3</b> Multimodel inference</a></li>
<li class="chapter" data-level="17.4" data-path="model_comparison.html"><a href="model_comparison.html#which-method-to-choose-and-which-strategy-to-follow"><i class="fa fa-check"></i><b>17.4</b> Which method to choose and which strategy to follow?</a></li>
<li class="chapter" data-level="17.5" data-path="model_comparison.html"><a href="model_comparison.html#further-reading-6"><i class="fa fa-check"></i><b>17.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="stan.html"><a href="stan.html"><i class="fa fa-check"></i><b>18</b> MCMC using Stan via rstanarm, brms or rstan</a>
<ul>
<li class="chapter" data-level="18.1" data-path="stan.html"><a href="stan.html#background-9"><i class="fa fa-check"></i><b>18.1</b> Background</a></li>
<li class="chapter" data-level="18.2" data-path="stan.html"><a href="stan.html#assessing-convergence-of-the-markov-chains-and-trouble-shooting-warnings-of-stan"><i class="fa fa-check"></i><b>18.2</b> Assessing convergence of the Markov chains and trouble shooting warnings of Stan</a></li>
<li class="chapter" data-level="18.3" data-path="stan.html"><a href="stan.html#using-stan-via-rstan"><i class="fa fa-check"></i><b>18.3</b> Using Stan via rstan</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="stan.html"><a href="stan.html#firststanmod"><i class="fa fa-check"></i><b>18.3.1</b> Writing a Stan model</a></li>
<li class="chapter" data-level="18.3.2" data-path="stan.html"><a href="stan.html#run-stan-from-r-using-rstan"><i class="fa fa-check"></i><b>18.3.2</b> Run Stan from R using rstan</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="stan.html"><a href="stan.html#further-reading-7"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ridge_regression.html"><a href="ridge_regression.html"><i class="fa fa-check"></i><b>19</b> Ridge Regression</a></li>
<li class="chapter" data-level="20" data-path="SEM.html"><a href="SEM.html"><i class="fa fa-check"></i><b>20</b> Structural equation models</a>
<ul>
<li class="chapter" data-level="20.1" data-path="SEM.html"><a href="SEM.html#introduction-3"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="spatial_glmm.html"><a href="spatial_glmm.html"><i class="fa fa-check"></i><b>21</b> Modeling spatial data using GLMM</a>
<ul>
<li class="chapter" data-level="21.1" data-path="spatial_glmm.html"><a href="spatial_glmm.html#introduction-4"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="spatial_glmm.html"><a href="spatial_glmm.html#summary-3"><i class="fa fa-check"></i><b>21.2</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>III ECOLOGICAL MODELS</b></span></li>
<li class="chapter" data-level="22" data-path="PART-III.html"><a href="PART-III.html"><i class="fa fa-check"></i><b>22</b> Introduction to PART III</a>
<ul>
<li class="chapter" data-level="22.1" data-path="PART-III.html"><a href="PART-III.html#model-notations"><i class="fa fa-check"></i><b>22.1</b> Model notations</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html"><i class="fa fa-check"></i><b>23</b> Zero-inflated Poisson mixed model</a>
<ul>
<li class="chapter" data-level="23.1" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#introduction-5"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#example-data"><i class="fa fa-check"></i><b>23.2</b> Example data</a></li>
<li class="chapter" data-level="23.3" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#model"><i class="fa fa-check"></i><b>23.3</b> Model</a></li>
<li class="chapter" data-level="23.4" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#further-packages-and-readings"><i class="fa fa-check"></i><b>23.4</b> Further packages and readings</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="dailynestsurv.html"><a href="dailynestsurv.html"><i class="fa fa-check"></i><b>24</b> Daily nest survival</a>
<ul>
<li class="chapter" data-level="24.1" data-path="dailynestsurv.html"><a href="dailynestsurv.html#background-10"><i class="fa fa-check"></i><b>24.1</b> Background</a></li>
<li class="chapter" data-level="24.2" data-path="dailynestsurv.html"><a href="dailynestsurv.html#models-for-estimating-daily-nest-survival"><i class="fa fa-check"></i><b>24.2</b> Models for estimating daily nest survival</a></li>
<li class="chapter" data-level="24.3" data-path="dailynestsurv.html"><a href="dailynestsurv.html#known-fate-model"><i class="fa fa-check"></i><b>24.3</b> Known fate model</a></li>
<li class="chapter" data-level="24.4" data-path="dailynestsurv.html"><a href="dailynestsurv.html#dailynestsurvstan"><i class="fa fa-check"></i><b>24.4</b> The Stan model</a></li>
<li class="chapter" data-level="24.5" data-path="dailynestsurv.html"><a href="dailynestsurv.html#prepare-data-and-run-stan"><i class="fa fa-check"></i><b>24.5</b> Prepare data and run Stan</a></li>
<li class="chapter" data-level="24.6" data-path="dailynestsurv.html"><a href="dailynestsurv.html#check-convergence"><i class="fa fa-check"></i><b>24.6</b> Check convergence</a></li>
<li class="chapter" data-level="24.7" data-path="dailynestsurv.html"><a href="dailynestsurv.html#look-at-results"><i class="fa fa-check"></i><b>24.7</b> Look at results</a></li>
<li class="chapter" data-level="24.8" data-path="dailynestsurv.html"><a href="dailynestsurv.html#known-fate-model-for-irregular-nest-controls"><i class="fa fa-check"></i><b>24.8</b> Known fate model for irregular nest controls</a></li>
<li class="chapter" data-level="" data-path="dailynestsurv.html"><a href="dailynestsurv.html#further-reading-8"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html"><i class="fa fa-check"></i><b>25</b> Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals</a>
<ul>
<li class="chapter" data-level="25.1" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#introduction-6"><i class="fa fa-check"></i><b>25.1</b> Introduction</a></li>
<li class="chapter" data-level="25.2" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#data-description"><i class="fa fa-check"></i><b>25.2</b> Data description</a></li>
<li class="chapter" data-level="25.3" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#model-description"><i class="fa fa-check"></i><b>25.3</b> Model description</a></li>
<li class="chapter" data-level="25.4" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#the-stan-code"><i class="fa fa-check"></i><b>25.4</b> The Stan code</a></li>
<li class="chapter" data-level="25.5" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#call-stan-from-r-check-convergence-and-look-at-results"><i class="fa fa-check"></i><b>25.5</b> Call Stan from R, check convergence and look at results</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="samplesize.html"><a href="samplesize.html"><i class="fa fa-check"></i><b>26</b> What sample size?</a>
<ul>
<li class="chapter" data-level="26.1" data-path="samplesize.html"><a href="samplesize.html#introduction-7"><i class="fa fa-check"></i><b>26.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>IV APPENDICES</b></span></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis in Ecology with R and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basics" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Basics of statistics<a href="basics.html#basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter introduces some important terms useful for doing data analyses. We introduce the Bayesian approach of data analyses. We also introduce the essentials of the classical frequentist tests (e.g. t-tests), which can be seen as an alternative to the Bayesian approach. Even though we will not use null hypotheses tests later <span class="citation">(<a href="referenzen.html#ref-Amrhein.2019">Amrhein, Greenland, and McShane 2019</a>)</span>, we introduce them here because we need to understand the scientific literature. For each classical test treated, we provide a suggestion how to present the statistical results without using null hypothesis tests. We further discuss some differences between the Bayesian and frequentist approach.</p>
<div id="variables-and-observations" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Variables and observations<a href="basics.html#variables-and-observations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Empirical research involves data collection. Data are collected by recording measurements of variables for observational units. An observational unit may be, for example, an individual, a plot, a time interval or a combination of those. The collection of all units ideally is a random sample of the entire population of units we are interested in. The measurements (or observations) of the random sample are stored in a data table. A data table is a collection of variables (columns). Data tables normally are handled as objects of class <code>data.frame</code> in R. All measurements on a row in a data table belong to the same observational unit. The variables can be of different scales (Table <a href="basics.html#tab:scalemeasurement">2.1</a>).</p>
<p><br></p>
<table>
<caption><span id="tab:scalemeasurement">Table 2.1: </span> Scales of measurements</caption>
<colgroup>
<col width="11%" />
<col width="28%" />
<col width="28%" />
<col width="31%" />
</colgroup>
<thead>
<tr>
<th align="left">Scale</th>
<th align="left">Examples</th>
<th align="left">Properties</th>
<th align="left">Coding in R</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Nominal</td>
<td align="left">Sex, genotype, habitat</td>
<td align="left">Identity (values have a unique meaning)</td>
<td align="left"><code>factor()</code></td>
</tr>
<tr>
<td align="left">Ordinal</td>
<td align="left">Elevational zones</td>
<td align="left">Identity and order (values have an ordered relationship)</td>
<td align="left"><code>ordered()</code></td>
</tr>
<tr>
<td align="left">Numeric</td>
<td align="left">Discrete: counts; continuous: body weight, wing length</td>
<td align="left">Identity, order, and interval</td>
<td align="left"><code>intgeger()</code> <code>numeric()</code></td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Nominal and ordinal variables may also be called “categorical” variables.</p>
<p>The aim of many studies is to describe how a variable of interest (<span class="math inline">\(y\)</span>; e.g. the time to build a nest) is related to one or more predictor variables (<span class="math inline">\(x\)</span>; e.g. the sex of the bird, its age class, and the number of individuals in the colony - representing a nominal, ordinal and numeric predictor). Depending on the author, the y-variable is called “outcome variable”, “response” or “dependent variable”. The x-variables are called “predictors”, “explanatory variables” or “independent variables”. We avoid the terms “dependent” and “independent” variables because often we do not know whether the variable <span class="math inline">\(y\)</span> is in fact depending on the <span class="math inline">\(x\)</span> variables, and often the x-variables are not independent of each other. In this book, we try to use “outcome” and “predictor” variables because these terms sound most neutral to us in that they refer to how the statistical model is constructed rather than to an assumed real relationship.</p>
<p>“Predictors” are often called a “covariable” if they are numeric (e.g. the colony size), and “factor” if they are nominal or ordinal (e.g. sex and age class). The characteristic of a factor is that it has defined values, called levels (in our example, the factor “sex” has the levels “female” and “male”, the factor “age class” has the levels “juvenile”, “immature” and “adult”).</p>
</div>
<div id="displaying-and-summarizing-data" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Displaying and summarizing data<a href="basics.html#displaying-and-summarizing-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="histogram" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Histogram<a href="basics.html#histogram" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While nominal and ordinal variables can be summarized by giving the absolute number or the proportion of observations for each level (e.g number of females and number of males), numeric variables normally are summarized by a location and a scatter statistic, such as the mean and the standard deviation, or the median and some quantiles (see below). Hence, the location tells us around what value our observations lay and it is sometimes called the “measure of central tendency”. The distribution of a numeric variable is graphically displayed in a histogram (Fig. <a href="basics.html#fig:histogram">2.1</a>).</p>
<div class="figure"><span style="display:block;" id="fig:histogram"></span>
<img src="1.1-prerequisites_files/figure-html/histogram-1.png" alt="Histogram of the length of the forearm of statistics course participants." width="384" />
<p class="caption">
Figure 2.1: Histogram of the length of the forearm of statistics course participants.
</p>
</div>
<font size="2">
<div style="border: 2px solid grey; padding: 10px">
<p><strong>Draw a histogram</strong></p>
<p>To draw a histogram, the variable is displayed on the x-axis and the observed values are assigned to classes. We use the function <span class="codeInBox">hist</span>. Remember to call the helpfile, if you forgot how a function works and what arguments it has; for that, type <span class="codeInBox">?hist</span> in the R console. There, we see that the edges of the classes can be set with the argument <span class="codeInBox">breaks=</span>. The values given in the <span class="codeInBox">breaks=</span> argument must at least span the values of the variable. If the argument <span class="codeInBox">breaks=</span> is not specified, R searches for break-values that make the histogram look smooth. The number of observations falling in each class is given on the y-axis. The y-axis can be re-scaled so that the area of the histogram equals 1 by setting the argument <span class="codeInBox">density=TRUE</span>. In that case, the values on the y-axis correspond to the density values of a probability distribution (Chapter <a href="distributions.html#distributions">4</a>). You can also save the result of the hist-function into an object, e.g. <span class="codeInBox">t.hist &lt;- hist(dat$ell)</span>, possibly with the argument <span class="codeInBox">plot=F (F for FALSE)</span>. Using <span class="codeInBox">t.hist</span>, you may then fully customize your histogram (e.g. overlay two histograms with slightly shifted columns).</p>
</div>
<p></font></p>
</div>
<div id="location-and-scatter" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Location and scatter<a href="basics.html#location-and-scatter" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Typical location statistics are mean, median or mode.</p>
<p>There are different types of means, e.g.:</p>
<ul>
<li>Arithmetic mean: <span class="math inline">\(\hat{\mu} = \bar{x} = \frac{1}{n} \sum_{1}^{n}x_i\)</span></li>
</ul>
<p>(R function <code>mean</code>), where <span class="math inline">\(n\)</span> is the sample size. The parameter <span class="math inline">\(\mu\)</span> is the (unknown) true mean of the entire population of which the <span class="math inline">\(n\)</span> measurements <span class="math inline">\(x_i\)</span> are a random sample of. <span class="math inline">\(\bar{x}\)</span> is called the sample mean and it is used as an estimate for <span class="math inline">\(\mu\)</span>. The <span class="math inline">\(^\)</span> (the “hat”) above any parameter indicates that the parameter value is obtained from a sample and, therefore, it may be different from the true value; it is an estimate of the true value.</p>
<ul>
<li>Geometric mean: <span class="math inline">\(\hat{\mu}_{geom} = \bar{x}_{geom} = \sqrt{\prod_{1}^{n}x_i}\)</span></li>
</ul>
<p>(no R function in the base package, but you may use: <code>exp(mean(log(x)))</code>)</p>
<p>The median is the 50% quantile: 50% of the measurements are below (and, hence 50% above) the median. If <span class="math inline">\(x_1,..., x_n\)</span> are the ordered measurements of a variable, then the median is:</p>
<ul>
<li><span class="math inline">\(\begin{aligned}
&amp; Median =
\begin{cases}
x_{(n+1)/2} &amp; \quad \text{if } n \text{ is odd}\\
\frac{1}{2}(x_{n/2} + x_{n/2+1}) &amp; \quad \text{if } n \text{ is even}
\end{cases}
\end{aligned}\)</span></li>
</ul>
<p>(R function <code>median</code>)</p>
<p>The mode is the value that is occurring with highest frequency or that has the highest density.</p>
<p>Scatter is also called spread, scale or variance. It describes how far away from the location parameter single observations can be found, or how the measurements are scattered around their mean. The variance is defined as the average squared difference between the observations and the mean:</p>
<ul>
<li>Variance <span class="math inline">\(\hat{\sigma^2} = s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2\)</span></li>
</ul>
<p>(R function <code>var</code>). The term <span class="math inline">\((n-1)\)</span> is called the degrees of freedom. It is used in the denominator of the variance formula instead of <span class="math inline">\(n\)</span> to prevent underestimating the (true) variance - remember that the <span class="math inline">\(x_i\)</span> are a sample of the population we are interested in, and <span class="math inline">\(\hat{\sigma^2}\)</span> is used as an estimate of the true variance <span class="math inline">\(\sigma^2\)</span> of this population. Because <span class="math inline">\(\bar{x}\)</span> is in average closer to <span class="math inline">\(x_i\)</span> than the unknown true mean <span class="math inline">\(\mu\)</span> would be, the variance would be underestimated if <span class="math inline">\(n\)</span> is used in the denominator.</p>
<!-- <font size="1"> The maximum likelihood estimate of the variance corresponds to the variance formula using $n$ instead of $n-1$ in the denominator, see, e.g., @Royle.2008b.</font> -->
<p>The variance is used to compare the degree of scatter among different groups. However, its values are difficult to interpret because of the squared unit. Therefore, the square root of the variance, the standard deviation is normally reported:</p>
<ul>
<li>Standard deviation <span class="math inline">\(\hat{\sigma} = s = \sqrt{s^2}\)</span></li>
</ul>
<p>(R Function <code>sd</code>). The standard deviation is approximately the average deviation of an observation from the sample mean. In the case of a <a href="distributions.html#normdist">normal distribution</a>, about two thirds (68%) of the data are within one standard deviation around the mean, and 95% within two standard deviations (more precisely within 1.96 SD).</p>
<p>Sometimes, the inverse of the variance is used, called precision:</p>
<ul>
<li>Precision <span class="math inline">\(\hat{p} = \frac{1}{\sigma^2}\)</span></li>
</ul>
<p>The variance and standard deviation each describe the scatter with a single value. Thus, we have to assume that the observations are scattered symmetrically around their mean in order to get a picture of the distribution of the measurements. When the measurements are spread asymmetrically (skewed distribution), then it may be more useful to describe the scatter with more than one value. Such statistics can be quantiles from the lower and upper tail of the data.</p>
<p>Quantiles inform us about both location and spread of a distribution. The <span class="math inline">\(p\)</span>th-quantile is the value with the property that the proportion <span class="math inline">\(p\)</span> of all values are less than or equal to the value of the quantile. The median is the 50% quantile. The 25% quantile and the 75% quantiles are also called the lower and upper quartiles, respectively. The range between the 25% and the 75% quartiles is called the interquartile range. This range includes 50% of the (central part of the) distribution and is also a measure of scatter. The R function <code>quantile</code> extracts sample quantiles. The median, the quartiles, and the interquartile range can be graphically displayed using box-and-whisker plots (boxplots in short, R function <code>boxplot</code>). The horizontal fat bar is the median (Fig. <a href="basics.html#fig:boxplot">2.2</a>). The box marks the interquartile range. The whiskers reach out to the last observation within 1.5 times the interquartile range from the quartile. Circles mark observations beyond the whiskers.</p>
<div class="figure"><span style="display:block;" id="fig:boxplot"></span>
<img src="1.1-prerequisites_files/figure-html/boxplot-1.png" alt="Boxplot of the length of forearm of statistics course participants who own car or not." width="384" />
<p class="caption">
Figure 2.2: Boxplot of the length of forearm of statistics course participants who own car or not.
</p>
</div>
<p>The boxplot is an appealing tool for comparing location, variance and distribution of measurements among groups.</p>
<p>A value that is far away from the distribution - such that it seems not to belong to the distribution - is called an outlier. It is not always clear whether an observation at the edge of a distribution should be considered an outlier or not, and how to deal with it. A first step is to check whether it could be due to a data entry error. Then, analyses may be done without and with the outlier(s) to see whether that has an effect on our conclusions. If you omit outliers, you must report and justify this. Note that median and interquartile range are less affected by outliers than mean and standard deviation.</p>
<p>To summarize: For symmetric distributions, mean and standard deviation (SD) area meaningful statistics to describe the distribution (Fig. <a href="basics.html#fig:locationScatter">2.3</a>, left). Mean and median are identical. The more skewed a distribution is, the more different are mean and median. Mean $$1 SD becomes flawed as it suggests symmetry that does not exist, and its end may even go beyond the distribution (e.g. below the minimum in Fig. <a href="basics.html#fig:locationScatter">2.3</a>, right). For skewed distributions, median and interquartile range are more informative.</p>
<div class="figure"><span style="display:block;" id="fig:locationScatter"></span>
<img src="1.1-prerequisites_files/figure-html/locationScatter-1.png" alt="A symmetric and a skewed distribution with the corresponding mean (dot, with $\pm1$ SD) and median (triangle, with interquartile range)." width="576" />
<p class="caption">
Figure 2.3: A symmetric and a skewed distribution with the corresponding mean (dot, with <span class="math inline">\(\pm1\)</span> SD) and median (triangle, with interquartile range).
</p>
</div>
</div>
<div id="correlations" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Correlations<a href="basics.html#correlations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A correlation measures the strength with which characteristics of two variables are associated with each other (co-occur). If both variables are numeric, we can visualize the correlation using a scatterplot.</p>
<div class="figure"><span style="display:block;" id="fig:scatterplot"></span>
<img src="1.1-prerequisites_files/figure-html/scatterplot-1.png" alt="Scatterplot of the length of forewarm and the comfort temperature of statistics course participants." width="384" />
<p class="caption">
Figure 2.4: Scatterplot of the length of forewarm and the comfort temperature of statistics course participants.
</p>
</div>
<p>The covariance between variable <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is defined as:</p>
<ul>
<li>Covariance <span class="math inline">\(q = \frac{1}{n-1}\sum_{i=1}^{n}((x_i-\bar{x})*(y_i-\bar{y}))\)</span></li>
</ul>
<p>(R function <code>cov</code>), with <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> being the means of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. As for the variance, the units of the covariance are sqared and, therefore, covariance values are difficult to interpret. A standardized covariance is the Pearson correlation coefficient:</p>
<ul>
<li>Pearson correlation coefficient: <span class="math inline">\(r=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2\sum_{i=1}^{n}(y_i-\bar{y})^2}}\)</span></li>
</ul>
<p>(R function <code>cor</code>)</p>
<p>Mean, variance, standard deviation, covariance and correlation are sensitive to outliers. Single observations with extreme values normally have a disproportionately high influence on these statistics. When outliers are present in the data, we may prefer a more robust correlation measure such as the Spearman correlation or Kendall’s tau. Both are based on the ranks of the measurements instead of the measurements themselves. The rank is simply the rank of each observation when these are sorted. The function <code>rank</code> ranks values, with different methods to deal with ties (see the helpfile using <code>?rank</code>).</p>
<ul>
<li>Spearman correlation coefficient: correlation between rank(x) and rank(y), i.e. <span class="math inline">\(r_s=\frac{\sum_{i=1}^{n}(R_{x_i}-\bar{R_x})(R_{y_i}-\bar{R_y})}{\sqrt{\sum_{i=1}^{n}(R_{x_i}-\bar{R_x})^2\sum_{i=1}^{n}(R_{y_i}-\bar{R_y})^2}}\)</span></li>
</ul>
<p>(R function <code>cor(x,y, method="spearman")</code>), with <span class="math inline">\(R_x\)</span> being the rank of observation <span class="math inline">\(x\)</span> among all <span class="math inline">\(x\)</span> observations, and <span class="math inline">\(\bar{R_x}\)</span> the mean of the ranks of <span class="math inline">\(x\)</span>.</p>
<ul>
<li>Kendall’s tau: <span class="math inline">\(\tau = 1-\frac{4I}{(n(n-1))}\)</span>, where <span class="math inline">\(I\)</span> = number of <span class="math inline">\((i,k)\)</span> for which <span class="math inline">\((x_i &lt; x_k)\)</span> &amp; <span class="math inline">\((y_i &gt; y_k)\)</span> or vice versa, and <span class="math inline">\((i,k)\)</span> has all pairs of observations with <span class="math inline">\(k&gt;i\)</span>.</li>
</ul>
<p>(R function <code>cor(x,y, method="kendall")</code>). <span class="math inline">\(I\)</span> counts the number of observation pairs that are “disconcordant”, i.e. where the ranking of the measurements of the two data points is not the same for the both variables. The more such discordant pairs, the less the correlation.</p>
<p>Figure <a href="basics.html#fig:correlations">2.5</a> illustrates that the correlation <span class="math inline">\(q\)</span> is larger with more data points (B vs. A) and steeper relationships (C vs. D). Pearson correlation <span class="math inline">\(r\)</span>, Spearman correlation <span class="math inline">\(r_s\)</span> and Kendall’s <span class="math inline">\(\tau\)</span> are always 1 when dots are in line (A-D), or -1 when the line descends (D), in which case <span class="math inline">\(q\)</span> is negative, too. The outlier in E has a strong effect on <span class="math inline">\(r\)</span>, but much less on <span class="math inline">\(r_s\)</span> and <span class="math inline">\(\tau\)</span>. F shows a more real world example. <span class="math inline">\(q\)</span> can have any value from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>, while <span class="math inline">\(r\)</span>, <span class="math inline">\(r_s\)</span> and <span class="math inline">\(\tau\)</span> are bound between -1 and 1. If the dots are completely horizontal (not shown), <span class="math inline">\(q\)</span> is 0 and the other three measures of correlation are <span class="math inline">\(NA\)</span>, i.e. not defined.</p>
<div class="figure"><span style="display:block;" id="fig:correlations"></span>
<img src="1.1-prerequisites_files/figure-html/correlations-1.png" alt="Covariance, Pearson correlation, Spearman correlation, and Kendall's tau for different patterns of x-y-data." width="672" />
<p class="caption">
Figure 2.5: Covariance, Pearson correlation, Spearman correlation, and Kendall’s tau for different patterns of x-y-data.
</p>
</div>
<p>For nominal variables, covariance and the above presented correlations cannot be calculated and would not be meaningful. But nominal variables may also be correlated or not - generally, we talk of “balanced” data if they are uncorrelated and “unbalanced” data otherwise. Measures of two nominal variables can be aggregated in a crosstabulation, e.g. the number of birds observed per combination of sex (male or female) and age (juvenile, immature or adult). In this table, there are 6 cells, and if there is the same number of males and females for each age class (and, thereby, also the same number per age class for both sexes), the data is balanced, i.e. sex and age are uncorrelated. The more the 6 numbers deviate from this, the more unbalanced the data is regarding sex and age. The <span class="math inline">\(\chi^2\)</span>-value is a measure of this balancedness, with a value of 0 for perfect balanced data, and increasingly larger values for less balanced data (also dependent on the size of the crosstabulation).</p>
</div>
<div id="principal-components-analyses-pca" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Principal components analyses PCA<a href="basics.html#principal-components-analyses-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The principal components analysis (PCA) is a multivariate correlation analysis. A multidimensional data set with <span class="math inline">\(k\)</span> variables can be seen as a cloud of points (observations) in a <span class="math inline">\(k\)</span>-dimensional space. Imagine, we could move around in the space and look at the cloud from different locations. From some locations, the data looks highly correlated, whereas from others, we cannot see the correlation. That is what PCA is doing. It is rotating the coordinate system (defined by the original variables) of the data cloud so that the correlations are no longer visible. The axes of the new coordinates system are linear combinations of the original variables. They are called principal components. There are as many principal components as there are original variables, i.e. <span class="math inline">\(k\)</span>, <span class="math inline">\(p_1, ..., p_k\)</span>. The principal components meet further requirements:</p>
<ul>
<li>the first component explains most variance</li>
<li>the second component explains most of the remaining variance and is perpendicular (= uncorrelated) to the first one</li>
<li>third component explains most of the remaining variance and is perpendicular to the first two</li>
<li>and so on</li>
</ul>
<p>For example, in a two-dimensional data set <span class="math inline">\((x_1, x_2)\)</span> the principal components become</p>
<p><span class="math inline">\(pc_{1i} = b_{11}x_{1i} + b_{12}x_{2i}\)</span><br />
<span class="math inline">\(pc_{2i} = b_{21}x_{1i} + b_{22}x_{2i}\)</span><br />
with <span class="math inline">\(b_{jk}\)</span> being loadings of original variable <span class="math inline">\(k\)</span> on principal component <span class="math inline">\(j\)</span>. Fig. <a href="basics.html#fig:principal">2.6</a> shows the two principal components PC1 and PC2 for a two-dimensional data set. They can be calculated using linear algebra: Principal components are eigenvectors of the covariance or correlation matrix. Note how the data spreads maximally along PC1.</p>
<div class="figure"><span style="display:block;" id="fig:principal"></span>
<img src="1.1-prerequisites_files/figure-html/principal-1.png" alt="Principal components of a two dimensional data set based on the covariance matrix (green) and the correlation matrix (brown)." width="384" />
<p class="caption">
Figure 2.6: Principal components of a two dimensional data set based on the covariance matrix (green) and the correlation matrix (brown).
</p>
</div>
<p>The choice between correlation or covariance matrix is important. The covariance matrix is an unstandardized correlation matrix and, therefore, the units, e.g. whether centimeters or meters are used, influence the result of the PCA. When the PCA is based on the covariance matrix, the result will change when we change the unit of a variable, e.g. from centimeters to meters. A PCA based on the covariance matrix only makes sense when the variances are comparable among the variables, i.e. when all variables are measured in the same unit, and when we want to weight each variable according to its variance. Usually, this is not the case and, therefore, the PCA must be based on the correlation matrix. Note that this is not the default setting in the most commonly used function <code>princomp</code> (use argument <code>cor=TRUE</code>) and <code>prcomp</code> (use argument <code>scale.=TRUE</code>)!</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="basics.html#cb1-1" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="fu">cbind</span>(x1,x2)) <span class="co"># PCA based on covariance matrix</span></span>
<span id="cb1-2"><a href="basics.html#cb1-2" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="fu">cbind</span>(x1,x2), <span class="at">cor=</span><span class="cn">TRUE</span>) <span class="co"># PCA based on correlation matrix</span></span>
<span id="cb1-3"><a href="basics.html#cb1-3" tabindex="-1"></a><span class="fu">loadings</span>(pca)</span></code></pre></div>
<pre><code>## 
## Loadings:
##    Comp.1 Comp.2
## x1  0.707  0.707
## x2  0.707 -0.707
## 
##                Comp.1 Comp.2
## SS loadings       1.0    1.0
## Proportion Var    0.5    0.5
## Cumulative Var    0.5    1.0</code></pre>
<p>The loadings measure the correlation of each variable with the principal components. They inform about what aspects of the data each component is measuring. The signs of the loadings are arbitrary, thus we can multiplied them by -1 without changing the PCA. Sometimes this can be handy for describing the meaning of the principal component in a paper. For example, <span class="citation">Zbinden et al. (<a href="referenzen.html#ref-Zbinden.2018">2018</a>)</span> combined the number of hunting licenses, the duration of the hunting period, and the number of black grouse cocks that were allowed to be hunted per hunter in a principal component in order to measure hunting pressure. All three variables had a negative loading on the first component, so that high values of the component meant low hunting pressure. Before the subsequent analyses, for which a measure of hunting pressure was of interest, the authors changed the signs of the loadings so that the first principal component could be used as a measure of hunting pressure.</p>
<p>This example also illustrates what PCA is often used for in ecology: You can reduce the number of predictors (for a model) by combining meaningful sets of predictors into fewer PCs. In the example, three predictors were combined into one predictor. This is often useful when you have many predictors, especially when these contain ecologically meaningful sets of predictors (in the example: hunting pressure variables; could also be weather variables, habitat variables etc) and, importantly, when they correlate. If there is not correlation, you don’t gain anything by using PCs instead of the original variables. But if they correlate, the first PC (or the first few PCs) can represent a relevant proportion of the total variance.</p>
<p>Hence, this proportion of variance explained by each PC is, beside the loadings, an important information. If the first few components explain the main part of the variance, it means that maybe not all <span class="math inline">\(k\)</span> variables are necessary to describe the data, or, in other words, the original <span class="math inline">\(k\)</span> variables contain a lot of redundant information. Then, you can use fewer than <span class="math inline">\(k\)</span> PCs without loosing (much) information.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="basics.html#cb3-1" tabindex="-1"></a><span class="co"># extract the variance captured by each component</span></span>
<span id="cb3-2"><a href="basics.html#cb3-2" tabindex="-1"></a><span class="fu">summary</span>(pca)</span></code></pre></div>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2
## Standard deviation     1.2679406 0.6263598
## Proportion of Variance 0.8038367 0.1961633
## Cumulative Proportion  0.8038367 1.0000000</code></pre>
<p>Ridge regression is similar to doing a PCA within a linear model while components with low variance are shrinked to a higher degree than components with a high variance [may be there will be a chapter on ridge regression some day].</p>
<p>As stated, PCA can be used to generate a lower-dimensional representation of multi-dimensional data, aiming to represent as much of the original multi-dimensional variance as possible e.g. in two dimensions (PC1 vs. PC2). There is a number of other techniques, especially from community ecology, with a similar aim, e.g. correspondence analyses (CA), redundancy analyses (RDA), or non-metric multidimensional scaling (NMDS). All these methods, including PCA, are “ordination techniques”. PCA simply turns the data cloud around without any distortion (only scaling), thereby showing as much of the euclidean distance as possible in the first PCs. Other techniques use other distance measures than euclidean. There may be additional variables that constrain the ordination (e.g. constrained = canonical CA). The R package vegan provides many of these ordination techniques and tutorials are available online. You may also start with wikipedia, e.g. on <a href="https://en.wikipedia.org/wiki/Multivariate_statistics">multivariate statistics</a>.</p>
</div>
</div>
<div id="inferential-statistics" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Inferential statistics<a href="basics.html#inferential-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="uncertainty" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Uncertainty<a href="basics.html#uncertainty" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>There is never a “yes-or-no” answer,<br />
there will always be uncertainty<br />
<a href="https://peerj.com/preprints/26857">Amrhein (2017)</a></p>
</blockquote>
<p>The decision whether an effect is important or not cannot be based on data alone. For making a decision, we should, beside the data, carefully consider the consequences of each decision, the aims we would like to achieve, and the risk, i.e. how bad it is to make the wrong decision. Structured decision making or decision analyses provide methods to combine consequences of decisions, objectives of different stakeholders, and risk attitudes of decision makers to make optimal decisions <span class="citation">Runge et al. (<a href="referenzen.html#ref-Runge.2020">2020</a>)</span>. In most data analyses, particularly in basic research and when working on case studies, we normally do not consider consequences of decisions. However, the results will be more useful when presented in a way that other scientists can use them for a meta-analysis (including structured decision making), or stakeholders and politicians can use them for making better decisions. Results useful for this must include information on the size of a parameter of interest, such as the effect of a drug or an average survival, together with an uncertainty measure.</p>
<p>Therefore, inferential statistics aims to describe the process that presumably has generated the data, and it quantifies the uncertainty of the described process that is due to the fact that the data is just a random sample from the larger population we would like to know the process of. In other words: Using regression modelling, we find patterns in our data, and if we make good models and careful ecological reasoning, we can make an educated guess about the underlying process.</p>
<p>Quantification of uncertainty is only possible if:<br />
1. the mechanisms that generated the data are known<br />
2. the observations are a random sample from the population of interest</p>
<p>Most studies aim at understanding the mechanisms that generated the data, thus they are generally not known beforehand. To overcome that problem, we construct models, e.g. statistical models, that are (strong) abstractions of the data generating process. And we report the model assumptions. All uncertainty measures are conditional on the model we used to analyze the data, i.e., they are only reliable if the model describes the data generating process realistically. Because statistical models essentially never describe the data generating process perfectly, the true uncertainty almost always is (much) higher than the one we report.</p>
<p>We can only make inference about the population under study if we have a random sample from this population. In order to obtain a random sample, a good study design is a prerequisite. To illustrate how inference about a big population is drawn from a small sample, we here use simulated data. The advantage of using simulated data is that the mechanism that generated the data is known, and we can play around with a big population.</p>
<p>Imagine there are 300 000 PhD students in the world and we would like to know how many statistics courses they have taken before they started their PhD on average (Fig. <a href="basics.html#fig:histtruesample">2.7</a>). We use random number generators (<code>rpois</code> and <code>rgamma</code>) to simulate a number of courses for each of the 300 000 virtual students. We use these 300 000 as the big population that in real life we almost never can sample in total. Normally, we only have values for a small sample of students. To simulate that situation we draw 12 numbers at random from the 300 000 (R function <code>sample</code>). Then, we estimate the average number of pre-PhD courses from the sample of 12 students, and we compare that <em>sample mean</em> with the <em>true mean</em> of the 300 000 students.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="basics.html#cb5-1" tabindex="-1"></a><span class="co"># simulate the virtual true population</span></span>
<span id="cb5-2"><a href="basics.html#cb5-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">235325</span>)   <span class="co"># set seed for random number generator</span></span>
<span id="cb5-3"><a href="basics.html#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="basics.html#cb5-4" tabindex="-1"></a><span class="co"># simulate fake data of the whole population using an overdispersed Poisson </span></span>
<span id="cb5-5"><a href="basics.html#cb5-5" tabindex="-1"></a><span class="co"># distribution, i.e. a Poisson distribution whose mean has a gamma distribution</span></span>
<span id="cb5-6"><a href="basics.html#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="basics.html#cb5-7" tabindex="-1"></a>statscourses <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">300000</span>, <span class="fu">rgamma</span>(<span class="dv">300000</span>, <span class="dv">2</span>, <span class="dv">3</span>))  </span>
<span id="cb5-8"><a href="basics.html#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="basics.html#cb5-9" tabindex="-1"></a><span class="co"># draw a random sample from the population</span></span>
<span id="cb5-10"><a href="basics.html#cb5-10" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">12</span>            <span class="co"># sample size</span></span>
<span id="cb5-11"><a href="basics.html#cb5-11" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sample</span>(statscourses, <span class="dv">12</span>, <span class="at">replace=</span><span class="cn">FALSE</span>)         </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:histtruesample"></span>
<img src="1.1-prerequisites_files/figure-html/histtruesample-1.png" alt="Histogram of the number of statistics courses that 300000 virtual PhD students have taken before their PhD started. The rug (small ticks) on the x-axis shows the random sample of 12 drawn from the 300000 students. The black vertical line indicates the mean of the 300000 students (true mean) and the blue line indicates the mean of the sample (sample mean)." width="432" />
<p class="caption">
Figure 2.7: Histogram of the number of statistics courses that 300000 virtual PhD students have taken before their PhD started. The rug (small ticks) on the x-axis shows the random sample of 12 drawn from the 300000 students. The black vertical line indicates the mean of the 300000 students (true mean) and the blue line indicates the mean of the sample (sample mean).
</p>
</div>
<p>We observe the sample mean - but from that, what do we know about the population mean? There are two different approaches to answer this question. 1) We could ask us, how much the sample mean would scatter, if we repeated the study many times? This approach is the basis of frequentist statistics. 2) We could ask us for any possible value, what is the probability that it is the true population mean? To do so, we use probability theory and that is the basis of Bayesian statistics.</p>
<p>Both approaches use (essentially similar) models. Only the mathematical techniques to calculate uncertainty measures differ between the two approaches. When no information other than the data is used to construct the model, the results are practically identical (at least for large enough sample sizes).</p>
<p>A frequentist 95% confidence interval CI (blue horizontal segment in Fig. <a href="basics.html#fig:CImean">2.8</a>) is constructed such that, if you were to (hypothetically) repeat the sampling many times, 95% of these many intervals constructed would contain the true value of the parameter (here the mean number of courses). From the Bayesian approach, we get a so-called posterior distribution (pink in Fig. <a href="basics.html#fig:CImean">2.8</a>), from which we can construct a 95% interval (e.g. using the 2.5% and 97.5% quantiles). This interval has traditionally been called 95% credible interval CrI. It can be interpreted that we are 95% sure that the true mean is inside that interval.</p>
<p>Both, confidence interval and posterior distribution, represent to the statistical uncertainty of the sample mean, i.e., they measure how far away the sample mean could be from the true mean. In this virtual example, we know the true mean is 0.66, thus in the lower part of the 95% CI or in the lower quantiles of the posterior distribution (hence, in the lower part of the 95% CrI). The grey histogram in Fig. <a href="basics.html#fig:CImean">2.8</a> shows how the means of many different virtual samples of 12 students scatter around the true mean. The 95% interval of these virtual means corresponds to the 95% CI, and the variance of these virtual means correspond to the variance of the posterior distribution. This virtual example shows that posterior distribution and 95% CI correctly measure the statistical uncertainty (variance, width of the interval), however we never know exactly how far the sample mean is from the true mean. In real life, we do not know the true mean.</p>
<div class="figure"><span style="display:block;" id="fig:CImean"></span>
<img src="1.1-prerequisites_files/figure-html/CImean-1.png" alt="Histogram (in gray) of the means of repeated samples from the true population. The scatter of these means visualizes the true uncertainty of the mean in this example. The blue vertical line indicates the mean of one sample. The blue segment shows its 95% confidence interval (obtained by fequensist methods). The violet line shows the posterior distribution of the mean (obtained by Bayesian methods)." width="576" />
<p class="caption">
Figure 2.8: Histogram (in gray) of the means of repeated samples from the true population. The scatter of these means visualizes the true uncertainty of the mean in this example. The blue vertical line indicates the mean of one sample. The blue segment shows its 95% confidence interval (obtained by fequensist methods). The violet line shows the posterior distribution of the mean (obtained by Bayesian methods).
</p>
</div>
<p>Uncertainty intervals are indispensable for inference from statistical analyses. An estimate without an uncertainty interval, called a point estimate, is quite useless. E.g. a point estimate of 10 has a very different meaning if its 95% uncertainty interval spans from -120 to 130 compared to when it spans from 9.6 to 10.4. Therefore, in most cases we provide the point estimate together with its uncertainty interval. But note that this does not yet guarantee correct inference: uncertainty intervals, as point estimates, are only reliable if the model is a realistic abstraction of the data generating process (i.e. if the model assumptions are realistic).</p>
<p>Both terms, confidence and credible interval, suggest that the interval indicates <em>confidence</em> or <em>credibility</em>, but the intervals actually show <em>uncertainty</em>. Therefore, it has been suggested to rename the interval into <em>uncertainty</em> interval, or <em>compatibility</em> interval as it indicates the range of values of the parameter (e.g. the mean) that is compatible with the data <span class="citation">(<a href="referenzen.html#ref-Gelman.2019">Andrew Gelman and Greenland 2019</a>)</span>.</p>
</div>
<div id="standard-error" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Standard error<a href="basics.html#standard-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The standard error SE is, beside the uncertainty interval, an alternative possibility to measure uncertainty. It measures an average deviation of the sample mean from the (unknown) true population mean. The frequentist method for obtaining the SE is based on the central limit theorem. According to the central limit theorem, the sum of independent, not necessarily normally distributed random numbers are normally distributed when sample size is large enough (Chapter <a href="distributions.html#distributions">4</a>). Because the mean is a sum (divided by a constant, the sample size) it can be assumed that the distribution of the means of many samples is normal. The standard deviation SD of the many means is called the standard error SE. It can be mathematically shown that the standard error SE equals the standard deviation SD of the sample divided by the square root of the sample size:</p>
<ul>
<li><p>Frequentist SE = SD/sqrt(n) = <span class="math inline">\(\frac{\hat{\sigma}}{\sqrt{n}}\)</span></p></li>
<li><p>Bayesian SE = the SD of the posterior distribution</p></li>
</ul>
<p>It is very important to keep the difference between SE and SD in mind! SD measures the scatter of the data, whereas SE measures the statistical uncertainty of the mean (or of another estimated parameter, Fig. <a href="basics.html#fig:sesd">2.9</a>). SD is a descriptive statistic, describing a characteristic of the data. The SE, on the other hand, is an inferential statistic; it is a measure of how far the sample mean may be away from the true mean (in Bayesian reasoning, the true mean is expected to be within <span class="math inline">\(\pm\)</span> 1 SE of the sample mean with a probability of about 67%, and within <span class="math inline">\(\pm\)</span> 2 SE with 95%). As sample size increases, the SE becomes smaller, whereas SD does not change (on average). The SE becomes smaller because the uncertainty about our sample mean is reduced with increasing sample size. The SD of the sample, however, is an estimate of the SD in the big population, so whether we have a small or large sample, its SD remains the same on average.</p>
<div class="figure"><span style="display:block;" id="fig:sesd"></span>
<img src="1.1-prerequisites_files/figure-html/sesd-1.png" alt="Illustration of the difference between SD and SE. The SD measures the scatter in the data (sample, tickmarks on the x-axis). The SD is an estimate for the scatter in the big population (grey histogram, normally not known). The SE measures the uncertainty of the sample mean (in blue). The SE measures approximately how far, in average the sample mean (blue) is from the true mean (brown)." width="480" />
<p class="caption">
Figure 2.9: Illustration of the difference between SD and SE. The SD measures the scatter in the data (sample, tickmarks on the x-axis). The SD is an estimate for the scatter in the big population (grey histogram, normally not known). The SE measures the uncertainty of the sample mean (in blue). The SE measures approximately how far, in average the sample mean (blue) is from the true mean (brown).
</p>
</div>
</div>
</div>
<div id="bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Bayes theorem and the common aim of frequentist and Bayesian methods<a href="basics.html#bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="bayes-theorem-for-discrete-events" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Bayes theorem for discrete events<a href="basics.html#bayes-theorem-for-discrete-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Bayes theorem describes the probability of event A conditional on event B (the probability of A after B has already occurred) from the probability of B conditional on A and the two probabilities of the events A and B:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<p>Imagine, event A is “The person likes wine as a birthday present.” and event B “The person has no car.”. The conditional probability of A given B is the probability that a person not owing a car likes wine. Answers from students whether they have a car and what they like as a birthday present are summarized in Table <a href="basics.html#tab:winecar">2.2</a>.</p>
<p><br></p>
<table>
<caption><span id="tab:winecar">Table 2.2: </span> Cross table of the student’s birthday preference and car ownership.</caption>
<thead>
<tr>
<th align="left">car/birthday</th>
<th align="left">flowers</th>
<th align="left">wine</th>
<th align="left"><strong>sum</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">no car</td>
<td align="left">6</td>
<td align="left">9</td>
<td align="left"><strong>15</strong></td>
</tr>
<tr>
<td align="left">car</td>
<td align="left">1</td>
<td align="left">6</td>
<td align="left"><strong>7</strong></td>
</tr>
<tr>
<td align="left"><strong>sum</strong></td>
<td align="left"><strong>7</strong></td>
<td align="left"><strong>15</strong></td>
<td align="left"><strong>22</strong></td>
</tr>
</tbody>
</table>
<p><br></p>
<p>We can apply the Bayes theorem to obtain the probability that the student likes wine given it has no car, <span class="math inline">\(P(A|B)\)</span>. Let’s assume that only the ones who prefer wine go together for having a glass of wine at the bar after the statistics course. While they drink wine, the tell each other about their cars and they obtain the probability that a student who likes wine has no car, <span class="math inline">\(P(B|A) = 0.6\)</span>. During the statistics class the teacher asked the students about their car ownership and birthday preference. Therefore, they know that <span class="math inline">\(P(A) =\)</span> likes wine <span class="math inline">\(= 0.68\)</span> and <span class="math inline">\(P(B) =\)</span> no car <span class="math inline">\(= 0.68\)</span>. With these information, they can obtain the probability that a student likes wine given it has no car, even if not all students without cars went to the bar: <span class="math inline">\(P(A|B) = \frac{0.6*0.68}{0.68} = 0.6\)</span>.</p>
</div>
<div id="bayes-theorem-for-continuous-parameters" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Bayes theorem for continuous parameters<a href="basics.html#bayes-theorem-for-continuous-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we use the Bayes theorem for analyzing data, then the aim is to make probability statements for parameters. Because most parameters are measured at a continuous scale we use probability density functions to describe what we know about them. The Bayes theorem can be formulated for probability density functions denoted with <span class="math inline">\(p(\theta)\)</span>, e.g. for a parameter <span class="math inline">\(\theta\)</span> (for example probability density functions see Chapter <a href="distributions.html#distributions">4</a>).
What we are interested in is the probability of the parameter <span class="math inline">\(\theta\)</span> given the data, i.e., <span class="math inline">\(p(\theta|y)\)</span>. This probability density function is called the posterior distribution of the parameter <span class="math inline">\(\theta\)</span>. Here is the Bayes theorem formulated for obtaining the posterior distribution of a parameter from the data <span class="math inline">\(y\)</span>, the prior distribution of the parameter <span class="math inline">\(p(\theta)\)</span> and assuming a model for the data generating process. The data model is defined by the likelihood that specifies how the data <span class="math inline">\(y\)</span> is distributed given the parameters <span class="math inline">\(p(y|\theta)\)</span>:</p>
<p><span class="math inline">\(p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} = \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}\)</span></p>
<p>The probability of the data <span class="math inline">\(p(y)\)</span> is also called the scaling constant, because it is a constant. It is the integral of the likelihood over all possible values of the parameter(s) of the model.</p>
</div>
<div id="estimating-a-mean-assuming-that-the-variance-is-known" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Estimating a mean assuming that the variance is known<a href="basics.html#estimating-a-mean-assuming-that-the-variance-is-known" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For illustration, we first describe a simple (unrealistic) example for which it is almost possible to follow the mathematical steps for solving the Bayes theorem even for non-mathematicians. Even if we cannot follow all steps, this example will illustrate the principle how the Bayesian theorem works for continuous parameters. The example is unrealistic because we assume that the variance <span class="math inline">\(\sigma^2\)</span> in the data <span class="math inline">\(y\)</span> is known.
We construct a data model by assuming that <span class="math inline">\(y\)</span> is normally distributed:</p>
<p><span class="math inline">\(p(y|\theta) = normal(\theta, \sigma)\)</span>, with <span class="math inline">\(\sigma\)</span> known. The function <span class="math inline">\(normal\)</span> defines the probability density function of the normal distribution (Chapter <a href="distributions.html#distributions">4</a>).</p>
<p>The parameter, for which we would like to get the posterior distribution is <span class="math inline">\(\theta\)</span>, the mean. We specify a prior distribution for <span class="math inline">\(\theta\)</span>. Because the normal distribution is a conjugate prior for a normal data model with known variance, we use the normal distribution. Conjugate priors have nice mathematical properties (see Chapter <a href="priors.html#priors">10</a>) and are therefore preferred when the posterior distribution is obtained algebraically.
That is the prior:
<span class="math inline">\(p(\theta) = normal(\mu_0, \tau_0)\)</span></p>
<p>With the above data, data model and prior, the posterior distribution of the mean <span class="math inline">\(\theta\)</span> is defined by:
<span class="math inline">\(p(\theta|y) = normal(\mu_n, \tau_n)\)</span>, where
<span class="math inline">\(\mu_n= \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau_0^2}+\frac{n}{\sigma^2}}\)</span> and
<span class="math inline">\(\frac{1}{\tau_n^2} = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\)</span></p>
<p><span class="math inline">\(\bar{y}\)</span> is the arithmetic mean of the data. Because only this value is needed in order to obtain the posterior distribution, this value is called the sufficient statistics.</p>
<p>From the mathematical formulas above and also from Fig. <a href="basics.html#fig:triplot">2.10</a> we see that the mean of the posterior distribution is a weighted average between the prior mean and <span class="math inline">\(\bar{y}\)</span> with weights equal to the precisions (<span class="math inline">\(\frac{1}{\tau_0^2}\)</span> and <span class="math inline">\(\frac{n}{\sigma^2}\)</span>).</p>
<div class="figure"><span style="display:block;" id="fig:triplot"></span>
<img src="1.1-prerequisites_files/figure-html/triplot-1.png" alt="Hypothetical example showing the result of applying the Bayes theorem for obtaining a posterior distribution of a continuous parameter. The likelhood is defined by the data and the model, the prior is expressing the knowledge about the parameter before looking at the data. Combining the two distributions using the Bayes theorem results in the posterior distribution." width="4900" />
<p class="caption">
Figure 2.10: Hypothetical example showing the result of applying the Bayes theorem for obtaining a posterior distribution of a continuous parameter. The likelhood is defined by the data and the model, the prior is expressing the knowledge about the parameter before looking at the data. Combining the two distributions using the Bayes theorem results in the posterior distribution.
</p>
</div>
</div>
<div id="estimating-the-mean-and-the-variance" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Estimating the mean and the variance<a href="basics.html#estimating-the-mean-and-the-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now move to a more realistic example, which is estimating the mean and the variance of a sample of weights of Snowfinches <em>Montifringilla nivalis</em> (Fig. <a href="basics.html#fig:ssp">2.11</a>). To analyze those data, a model with two parameters (the mean and the variance or standard deviation) is needed. The data model (or likelihood) is specified as <span class="math inline">\(p(y|\theta, \sigma) = normal(\theta, \sigma)\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ssp"></span>
<img src="images/snowfinch2.JPG" alt="Snowfinches stay above the treeline for winter. They come to feeders." width="949" />
<p class="caption">
Figure 2.11: Snowfinches stay above the treeline for winter. They come to feeders.
</p>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="basics.html#cb6-1" tabindex="-1"></a><span class="co"># weight (g)</span></span>
<span id="cb6-2"><a href="basics.html#cb6-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">47.5</span>, <span class="dv">43</span>, <span class="dv">43</span>, <span class="dv">44</span>, <span class="fl">48.5</span>, <span class="fl">37.5</span>, <span class="fl">41.5</span>, <span class="fl">45.5</span>)</span>
<span id="cb6-3"><a href="basics.html#cb6-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span></code></pre></div>
<p>Because there are two parameters, we need to specify a two-dimensional prior distribution. We looked up in <span class="citation">A. Gelman et al. (<a href="referenzen.html#ref-Gelman.2014">2014</a>)</span> that the conjugate prior distribution in our case is an Normal-Inverse-Chisquare distribution:</p>
<p><span class="math inline">\(p(\theta, \sigma) = N-Inv-\chi^2(\mu_0, \sigma_0^2/\kappa_0; v_0, \sigma_0^2)\)</span></p>
<p>From the same reference we looked up how the posterior distribution looks like in our case:</p>
<p><span class="math inline">\(p(\theta,\sigma|y) = \frac{p(y|\theta, \sigma)p(\theta, \sigma)}{p(y)} = N-Inv-\chi^2(\mu_n, \sigma_n^2/\kappa_n; v_n, \sigma_n^2)\)</span>, with</p>
<p><span class="math inline">\(\mu_n= \frac{\kappa_0}{\kappa_0+n}\mu_0 + \frac{n}{\kappa_0+n}\bar{y}\)</span>,</p>
<p><span class="math inline">\(\kappa_n = \kappa_0+n\)</span>,
<span class="math inline">\(v_n = v_0 +n\)</span>,</p>
<p><span class="math inline">\(v_n\sigma_n^2=v_0\sigma_0^2+(n-1)s^2+\frac{\kappa_0n}{\kappa_0+n}(\bar{y}-\mu_0)^2\)</span></p>
<p>For this example, we need the arithmetic mean <span class="math inline">\(\bar{y}\)</span> and standard deviation <span class="math inline">\(s^2\)</span> from the sample for obtaining the posterior distribution. Therefore, these two statistics are the sufficient statistics.</p>
<p>The above formula look intimidating, but we never really do that calculations. We let <code>R</code> doing that for us in most cases by simulating many numbers from the posterior distribution, e.g., using the function <code>sim</code> from the package arm <span class="citation">(<a href="referenzen.html#ref-Gelman.2007">Andrew Gelman and Hill 2007</a>)</span>. In the end, we can visualize the distribution of these many numbers to have a look at the posterior distribution.</p>
<p>In Fig. <a href="basics.html#fig:jointpdist">2.12</a> the two-dimensional <span class="math inline">\((\theta, \sigma)\)</span> posterior distribution is visualized by using simulated values. The two dimensional distribution is called the joint posterior distribution. The mountain of dots in Fig. <a href="basics.html#fig:jointpdist">2.12</a> visualize the Normal-Inverse-Chisquare that we calculated above. When all values of one parameter is displayed in a histogram ignoring the values of the other parameter, it is called the marginal posterior distribution. Algebraically, the marginal distribution is obtained by integrating one of the two parameters out over the joint posterior distribution. This step is definitively way easier when simulated values from the posterior distribution are available!</p>
<div class="figure"><span style="display:block;" id="fig:jointpdist"></span>
<img src="1.1-prerequisites_files/figure-html/jointpdist-1.png" alt="Visualization of the joint posterior distribution for the mean and standard deviation of Snowfinch weights. The lower left panel shows the two-dimensional joint posterior distribution, whereas the upper and right panel show the marginal posterior distributions of each parameter separately." width="672" />
<p class="caption">
Figure 2.12: Visualization of the joint posterior distribution for the mean and standard deviation of Snowfinch weights. The lower left panel shows the two-dimensional joint posterior distribution, whereas the upper and right panel show the marginal posterior distributions of each parameter separately.
</p>
</div>
<p>The marginal posterior distributions of every parameter is what we normally report in a paper to report statistical uncertainty.</p>
<p>In our example, the marginal distribution of the mean is a t-distribution (Chapter <a href="distributions.html#distributions">4</a>). Frequentist statistical methods also use a t-distribution to describe the uncertainty of an estimated mean for the case when the variance is not known. Thus, frequentist methods came to the same solution using a completely different approach and different techniques. Doesn’t that increase dramatically our trust in statistical methods?</p>
</div>
</div>
<div id="classical-frequentist-tests-and-alternatives" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Classical frequentist tests and alternatives<a href="basics.html#classical-frequentist-tests-and-alternatives" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="nullhypothesis-testing" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Nullhypothesis testing<a href="basics.html#nullhypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Null hypothesis testing is constructing a model that describes how the data would look like in case of what we expect to be would not be. Then, the data is compared to how the model thinks the data should look like. If the data does not look like the model thinks they should, we reject that model and accept that our expectation may be true.</p>
<p>To decide whether the data looks like the null-model thinks the data should look like the p-value is used. The p-value is the probability of observing the data or more extreme data given the null hypothesis is true.</p>
<p>Small p-values indicate that it is rather unlikely to observe the data or more extreme data given the null hypothesis <span class="math inline">\(H_0\)</span> is true.</p>
<p>Null hypothesis testing is problematic. We discuss some of the problems after having introduces the most commonly used classical tests.</p>
</div>
<div id="comparison-of-a-sample-with-a-fixed-value-one-sample-t-test" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Comparison of a sample with a fixed value (one-sample t-test)<a href="basics.html#comparison-of-a-sample-with-a-fixed-value-one-sample-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some studies, we would like to compare the data to a theoretical value. The theoretical value is a fixed value, e.g. calculated based on physical, biochemical, ecological or any other theory. The statistical task is then to compare the mean of the data including its uncertainty with the theoretical value. The result of such a comparison may be an estimate of the mean of the data with its uncertainty or an estimate of the difference of the mean of the data to the theoretical value with the uncertainty of this difference.</p>
<!-- fk: HIER WEITER: include a figure that shows the data in a box and the fixed value and the mean with CI, and one only showing data with stars-->
<p>For example, a null hypothesis could be <span class="math inline">\(H_0:\)</span>“The mean of Snowfinch weights is exactly 40g.”
A normal distribution with a mean of <span class="math inline">\(\mu_0=40\)</span> and a variance equal to the estimated variance in the data <span class="math inline">\(s^2\)</span> is then assumed to describe how we would expect the data to look like given the null hypothesis was true. From that model it is possible to calculate the distribution of hypothetical means of many different hypothetical samples of sample size <span class="math inline">\(n\)</span>. The result is a t-distribution (Fig. <a href="basics.html#fig:nht">2.13</a>). In classical tests, the distribution is standardized so that its variance was one. Then the sample mean, or in classical tests a standardized difference between the mean and the hypothetical mean of the null hypothesis (here 40g), called test statistics <span class="math inline">\(t = \frac{\bar{y}-\mu_0}{\frac{s}{\sqrt{n}}}\)</span>, is compared to that (standardized) t-distribution. If the test statistics falls well within the expected distribution the null hypothesis is accepted. Then, the data is well compatible with the null hypothesis. However, if the test statistics falls in the tails or outside the distribution, then the null hypothesis is rejected and we could write that the mean weight of Snowfinches is statistically significantly different from 40g. Unfortunately, we cannot infer about the probability of the null hypothesis and how relevant the result is.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nht"></span>
<img src="1.1-prerequisites_files/figure-html/nht-1.png" alt="Illustration of a one-sample t-test. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the t-distribution that shows how hypothetical sample means are expected to be distributed if the big population of Snowfinches has a mean weight of 40g (i.e., if the null hypothesis was true). Orange area shows the area of the t-distribution that lays equal or farther away from 40g than the sample mean. The orange area is the p-value." width="768" />
<p class="caption">
Figure 2.13: Illustration of a one-sample t-test. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the t-distribution that shows how hypothetical sample means are expected to be distributed if the big population of Snowfinches has a mean weight of 40g (i.e., if the null hypothesis was true). Orange area shows the area of the t-distribution that lays equal or farther away from 40g than the sample mean. The orange area is the p-value.
</p>
</div>
<p>We can use the r-function <code>t.test</code> to calculate the p-value of a one sample t-test.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="basics.html#cb7-1" tabindex="-1"></a><span class="fu">t.test</span>(y, <span class="at">mu=</span><span class="dv">40</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  y
## t = 3.0951, df = 7, p-value = 0.01744
## alternative hypothesis: true mean is not equal to 40
## 95 percent confidence interval:
##  40.89979 46.72521
## sample estimates:
## mean of x 
##   43.8125</code></pre>
<p>The output of the r-function <code>t.test</code> also includes the mean and the 95% confidence interval (or compatibility or uncertainty interval) of the mean. The CI could, alternatively, be obtained as the 2.5% and 97.5% quantiles of a t-distribution with a mean equal to the sample mean, degrees of freedom equal to the sample size minus one and a standard deviation equal to the standard error of the mean.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="basics.html#cb9-1" tabindex="-1"></a><span class="co"># lower limit of 95% CI</span></span>
<span id="cb9-2"><a href="basics.html#cb9-2" tabindex="-1"></a><span class="fu">mean</span>(y) <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.025</span>, <span class="at">df=</span><span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span><span class="fu">sd</span>(y)<span class="sc">/</span><span class="fu">sqrt</span>(n) </span></code></pre></div>
<pre><code>## [1] 40.89979</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="basics.html#cb11-1" tabindex="-1"></a><span class="co"># upper limit of 95% CI</span></span>
<span id="cb11-2"><a href="basics.html#cb11-2" tabindex="-1"></a><span class="fu">mean</span>(y) <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df=</span><span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span><span class="fu">sd</span>(y)<span class="sc">/</span><span class="fu">sqrt</span>(n) </span></code></pre></div>
<pre><code>## [1] 46.72521</code></pre>
<p>When applying the Bayes theorem for obtaining the posterior distribution of the mean we end up with the same t-distribution as described above, in case we use flat prior distributions for the mean and the standard deviation. Thus, the two different approaches end up with the same result!</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="basics.html#cb13-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.5</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb13-2"><a href="basics.html#cb13-2" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">30</span>,<span class="dv">52</span>), <span class="at">las=</span><span class="dv">1</span>, <span class="at">freq=</span><span class="cn">FALSE</span>, <span class="at">main=</span><span class="cn">NA</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.3</span>))</span>
<span id="cb13-3"><a href="basics.html#cb13-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(y), <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;lightblue&quot;</span>)</span>
<span id="cb13-4"><a href="basics.html#cb13-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">40</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb13-5"><a href="basics.html#cb13-5" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(bsim<span class="sc">@</span>coef))</span>
<span id="cb13-6"><a href="basics.html#cb13-6" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">45</span>, <span class="fl">0.3</span>, <span class="st">&quot;posterior distribution</span><span class="sc">\n</span><span class="st">of the mean of y&quot;</span>, <span class="at">cex=</span><span class="fl">0.8</span>, <span class="at">adj=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">xpd=</span><span class="cn">NA</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="1.1-prerequisites_files/figure-html/unnamed-chunk-8-1.png" alt="Illustration of the posterior distribution of the mean. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the posterior distribution that shows what we know about the mean after having looked at the data. The area under the posterior density function that is larger than 40 is the posterior probability of the hypothesis that the true mean Snwofinch weight is larger than 40g." width="672" />
<p class="caption">
Figure 2.14: Illustration of the posterior distribution of the mean. The blue histogram shows the distribution of the measured weights with the sample mean (lightblue) indicated as a vertical line. The black line is the posterior distribution that shows what we know about the mean after having looked at the data. The area under the posterior density function that is larger than 40 is the posterior probability of the hypothesis that the true mean Snwofinch weight is larger than 40g.
</p>
</div>
<p>The posterior probability of the hypothesis that the true mean Snowfinch weight is larger than 40g, <span class="math inline">\(P(H:\mu&gt;40) =\)</span>, is equal to the proportion of simulated random values from the posterior distribution, saved in the vector <code>bsim@coef</code>, that are larger than 40.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="basics.html#cb14-1" tabindex="-1"></a><span class="co"># Two ways of calculating the proportion of values </span></span>
<span id="cb14-2"><a href="basics.html#cb14-2" tabindex="-1"></a><span class="co"># larger than a specific value within a vector of values</span></span>
<span id="cb14-3"><a href="basics.html#cb14-3" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sum</span>(bsim<span class="sc">@</span>coef[,<span class="dv">1</span>]<span class="sc">&gt;</span><span class="dv">40</span>)<span class="sc">/</span><span class="fu">nrow</span>(bsim<span class="sc">@</span>coef),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.99</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="basics.html#cb16-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>(bsim<span class="sc">@</span>coef[,<span class="dv">1</span>]<span class="sc">&gt;</span><span class="dv">40</span>),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.99</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="basics.html#cb18-1" tabindex="-1"></a><span class="co"># Note: logical values TRUE and FALSE become </span></span>
<span id="cb18-2"><a href="basics.html#cb18-2" tabindex="-1"></a><span class="co"># the numeric values 1 and 0 within the functions sum() and mean()</span></span></code></pre></div>
<p>We, thus, can be pretty sure that the mean Snowfinch weight (in the big world population) is larger than 40g. Such a conclusion is not very informative, because it does not tell us how much larger we can expect the mean Snowfinch weight to be. Therefore, we prefer reporting a credible interval (or compatibility interval or uncertainty interval) that tells us what values for the mean Snowfinch weight are compatible with the data (given the data model we used realistically reflects the data generating process). Based on such an interval, we can conclude that we are pretty sure that the mean Snowfinch weight is between 40 and 48g.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="basics.html#cb19-1" tabindex="-1"></a><span class="co"># 80% credible interval, compatibility interval, uncertainty interval</span></span>
<span id="cb19-2"><a href="basics.html#cb19-2" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>coef[,<span class="dv">1</span>], <span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>))</span></code></pre></div>
<pre><code>##      10%      90% 
## 42.08254 45.56718</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="basics.html#cb21-1" tabindex="-1"></a><span class="co"># 95% credible interval, compatibility interval, uncertainty interval</span></span>
<span id="cb21-2"><a href="basics.html#cb21-2" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>coef[,<span class="dv">1</span>], <span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##     2.5%    97.5% 
## 40.86945 46.76137</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="basics.html#cb23-1" tabindex="-1"></a><span class="co"># 99% credible interval, compatibility interval, uncertainty interval</span></span>
<span id="cb23-2"><a href="basics.html#cb23-2" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>coef[,<span class="dv">1</span>], <span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.005</span>, <span class="fl">0.995</span>))</span></code></pre></div>
<pre><code>##     0.5%    99.5% 
## 39.41786 48.10440</code></pre>
</div>
<div id="comparison-of-the-locations-between-two-groups-two-sample-t-test" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Comparison of the locations between two groups (two-sample t-test)<a href="basics.html#comparison-of-the-locations-between-two-groups-two-sample-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many research questions aim at measuring differences between groups. For example, we could be curious to know how different in size car owner are from people not owning a car.
A boxplot is a nice possibility to visualize the ell length measurements of two (or more) groups (Fig. <a href="basics.html#fig:boxplt">2.15</a>). From the boxplot, we do not see how many observations are in the two samples. We can add that information to the plot. The boxplot visualizes the samples but it does not show what we know about the big (unmeasured) population mean. To show that, we need to add a compatibility interval (or uncertainty interval, credible interval, confidence interval, in brown in Fig. <a href="basics.html#fig:boxplt">2.15</a>).</p>
<div class="figure"><span style="display:block;" id="fig:boxplt"></span>
<img src="1.1-prerequisites_files/figure-html/boxplt-1.png" alt="Ell length of car owners (Y) and people not owning a car (N). Horizontal bar = median, box = interquartile range, whiskers = extremest observation within 1.5 times the interquartile range from the quartile, circles=observations farther than 1.5 times the interquartile range from the quartile. Filled brown circles = means, vertical brown bars = 95% compatibility interval." width="672" />
<p class="caption">
Figure 2.15: Ell length of car owners (Y) and people not owning a car (N). Horizontal bar = median, box = interquartile range, whiskers = extremest observation within 1.5 times the interquartile range from the quartile, circles=observations farther than 1.5 times the interquartile range from the quartile. Filled brown circles = means, vertical brown bars = 95% compatibility interval.
</p>
</div>
<p>When we added the two means with a compatibility interval, we see what we know about the two means, but we do still not see what we know about the difference between the two means. The uncertainties of the means do not show the uncertainty of the difference between the means. To do so, we need to extract the difference between the two means from a model that describes (abstractly) how the data has been generated. Such a model is a linear model that we will introduce in Chapter <a href="lm.html#lm">11</a>. The second parameter measures the differences in the means of the two groups. And from the simulated posterior distribution we can extract a 95% compatibility interval.
Thus, we can conclude that the average ell length of car owners is with high probability between 0.5 cm smaller and 2.5 cm larger than the averag ell of people not having a car.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="basics.html#cb25-1" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(ell<span class="sc">~</span>car, <span class="at">data=</span>dat)</span>
<span id="cb25-2"><a href="basics.html#cb25-2" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ell ~ car, data = dat)
## 
## Coefficients:
## (Intercept)         carY  
##      43.267        1.019</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="basics.html#cb27-1" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span>
<span id="cb27-2"><a href="basics.html#cb27-2" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>coef[,<span class="st">&quot;carY&quot;</span>], <span class="at">prob=</span><span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##      2.5%       50%     97.5% 
## -0.479615  1.006546  2.507964</code></pre>
<p>The corresponding two-sample t-test gives a p-value for the null hypothesis: “The difference between the two means equals zero.”, a confidence interval for the difference and the two means. While the function <code>lm</code>gives the difference Y minus N, the function <code>t.test</code>gives the difference N minus Y. Luckily the two means are also given in the output, so we know which group mean is the larger one.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="basics.html#cb29-1" tabindex="-1"></a><span class="fu">t.test</span>(ell<span class="sc">~</span>car, <span class="at">data=</span>dat, <span class="at">var.equal=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  ell by car
## t = -1.4317, df = 20, p-value = 0.1677
## alternative hypothesis: true difference in means between group N and group Y is not equal to 0
## 95 percent confidence interval:
##  -2.5038207  0.4657255
## sample estimates:
## mean in group N mean in group Y 
##        43.26667        44.28571</code></pre>
<p>In both possibilities, we used to compare the to means, the Bayesian posterior distribution of the difference and the t-test or the confidence interval of the difference, we used a data model. We thus assumed that the observations are normally distributed. In some cases, such an assumption is not a reasonable assumption. Then the result is not reliable. In such cases, we can either search for a more realistic model or use non-parametric (also called distribution free) methods. Nowadays, we have almost infinite possibilities to construct data models (e.g. generalized linear models and beyond). Therefore, we normally start looking for a model that fits the data better. However, in former days, all these possiblities did not exist (or were not easily available for non-mathematicians). Therefore, we here introduce two of such non-parametric methods, the Wilcoxon-test (or Mann-Whitney-U-test) and the randomisation test.</p>
<p>Some of the distribution free statistical methods are based on the rank instead of the value of the observations. The principle of the Wilcoxon-test is to rank the observations and sum the ranks per group. It is not completely true that the non-parametric methods do not have a model. The model of the Wilcoxon-test “knows” how the difference in the sum of the ranks between two groups is distributed given the mean of the two groups do not differ (null hypothesis). Therefore, it is possible to get a p-value, e.g. by the function <code>wilcox.test</code>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="basics.html#cb31-1" tabindex="-1"></a><span class="fu">wilcox.test</span>(ell<span class="sc">~</span>car, <span class="at">data=</span>dat)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  ell by car
## W = 34.5, p-value = 0.2075
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>The note in the output tells us that ranking is ambiguous, when some values are equal. Equal values are called ties when they should be ranked.</p>
<p>The result of the Wilcoxon-test tells us how probable it is to observe the difference in the rank sum between the two sample or a more extreme difference given the means of the two groups are equal. That is at least something.</p>
<p>A similar result is obtained by using a randomisation test. This test is not based on ranks but on the original values. The aim of the randomisation is to simulate a distribution of the difference in the arithmetic mean between the two groups assuming this difference would be zero. To do so, the observed values are randomly distributed among the two groups. Because of the random distribution among the two groups, we expect that, if we repeat that virtual experiment many times, the average difference between the group means would be zero (both virtual samples are drawn from the same big population).</p>
<p>We can use a loop in R for repeating the random re-assignement to the two groups and, each time, extracting the difference between the group means. As a result, we have a vector of many (<code>nsim</code>) values that all are possible differences between group means given the two samples were drawn from the same population. The proportion of these values that have an equal or larger absolute value give the probability that the observed or a larger difference between the group means is observed given the null hypothesis would be true, thus that is a p-value.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="basics.html#cb33-1" tabindex="-1"></a>diffH0 <span class="ot">&lt;-</span> <span class="fu">numeric</span>(nsim)</span>
<span id="cb33-2"><a href="basics.html#cb33-2" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim){</span>
<span id="cb33-3"><a href="basics.html#cb33-3" tabindex="-1"></a>  randomcars <span class="ot">&lt;-</span> <span class="fu">sample</span>(dat<span class="sc">$</span>car)</span>
<span id="cb33-4"><a href="basics.html#cb33-4" tabindex="-1"></a>  rmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(ell<span class="sc">~</span>randomcars, <span class="at">data=</span>dat)</span>
<span id="cb33-5"><a href="basics.html#cb33-5" tabindex="-1"></a>  diffH0[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(rmod)[<span class="st">&quot;randomcarsY&quot;</span>]</span>
<span id="cb33-6"><a href="basics.html#cb33-6" tabindex="-1"></a>}</span>
<span id="cb33-7"><a href="basics.html#cb33-7" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(diffH0)<span class="sc">&gt;</span><span class="fu">abs</span>(<span class="fu">coef</span>(mod)[<span class="st">&quot;carY&quot;</span>])) <span class="co"># p-value</span></span></code></pre></div>
<pre><code>## [1] 0.1942</code></pre>
<p>Visualizing the possible differences between the group means given the null hypothesis was true shows that the observed difference is well within what is expected if the two groups would not differ in their means (Fig. <a href="basics.html#fig:ranhist">2.16</a>).</p>
<div class="figure"><span style="display:block;" id="fig:ranhist"></span>
<img src="1.1-prerequisites_files/figure-html/ranhist-1.png" alt="Histogram if differences between the means of randomly assigned groups (grey) and the difference between the means of the two observed groups (red)" width="672" />
<p class="caption">
Figure 2.16: Histogram if differences between the means of randomly assigned groups (grey) and the difference between the means of the two observed groups (red)
</p>
</div>
<p>The randomization test results in a p-value and, we could also report the observed difference between the group means. However, it does not tell us, what values of the difference all would be compatible with the data. We do not get an uncertainty measurement for the difference.</p>
<p>In order to get a compatibility interval without assuming a distribution for the data (thus non-parametric) we could bootstrap the samples.</p>
<p>Bootstrapping is sampling observations from the data with replacement. For example, if we have a sample of 8 observations, we draw 8 times a random observation from the 8 observation. Each time, we assume that all 8 observations are available. Thus a bootstrapped sample could include some observations several times, whereas others are missing. In this way, we simulate the variance in the data that is due to the fact that our data do not contain the whole big population.</p>
<p>Also bootstrapping can be programmed in R using a loop.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="basics.html#cb35-1" tabindex="-1"></a>diffboot <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">1000</span>)</span>
<span id="cb35-2"><a href="basics.html#cb35-2" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim){</span>
<span id="cb35-3"><a href="basics.html#cb35-3" tabindex="-1"></a>  ngroups <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb35-4"><a href="basics.html#cb35-4" tabindex="-1"></a>  <span class="cf">while</span>(ngroups<span class="sc">==</span><span class="dv">1</span>){</span>
<span id="cb35-5"><a href="basics.html#cb35-5" tabindex="-1"></a>    bootrows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dat), <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb35-6"><a href="basics.html#cb35-6" tabindex="-1"></a>    ngroups <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">unique</span>(dat<span class="sc">$</span>car[bootrows]))</span>
<span id="cb35-7"><a href="basics.html#cb35-7" tabindex="-1"></a>    }</span>
<span id="cb35-8"><a href="basics.html#cb35-8" tabindex="-1"></a>  rmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(ell<span class="sc">~</span>car, <span class="at">data=</span>dat[bootrows,])</span>
<span id="cb35-9"><a href="basics.html#cb35-9" tabindex="-1"></a>  diffboot[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(rmod)[<span class="dv">2</span>]</span>
<span id="cb35-10"><a href="basics.html#cb35-10" tabindex="-1"></a>}</span>
<span id="cb35-11"><a href="basics.html#cb35-11" tabindex="-1"></a><span class="fu">quantile</span>(diffboot, <span class="at">prob=</span><span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##       2.5%      97.5% 
## -0.3541667  2.3888889</code></pre>
<p>The resulting values for the difference between the two group means can be interpreted as the distribution of those differences, if we had repeated the study many times (Fig. <a href="basics.html#fig:histboot">2.17</a>).
A 95% interval of the distribution corresponds to a 95% compatibility interval (or confidence interval or uncertainty interval).</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="basics.html#cb37-1" tabindex="-1"></a><span class="fu">hist</span>(diffboot); <span class="fu">abline</span>(<span class="at">v=</span><span class="fu">coef</span>(mod)[<span class="dv">2</span>], <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:histboot"></span>
<img src="1.1-prerequisites_files/figure-html/histboot-1.png" alt="Histogram of the boostrapped differences between the group means (grey) and the observed difference." width="672" />
<p class="caption">
Figure 2.17: Histogram of the boostrapped differences between the group means (grey) and the observed difference.
</p>
</div>
<p>For both methods, randomisation test and bootstrapping, we have to assume that all observations are independent. Randomization and bootstrapping becomes complicated or even unfeasible when data are structured.</p>
</div>
</div>
<div id="whyBayes" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Comparing frequentist and Bayesian approach - an why we use Bayes<a href="basics.html#whyBayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An important difference between the Bayesian and frequentist ways of analyzing data is how conclusions are drawn (Table <a href="basics.html#tab:TabBayesFreq">2.3</a>). Bayesians are interested in the probability of a (meaningful) hypothesis (e.g., that an effect is larger than a relevant size, say, <span class="math inline">\(H: \theta &gt; e\)</span>) given the data, <span class="math inline">\(p(H|y)\)</span>, or they describe what we know about a parameter after having looked at the data by the posterior distribution <span class="math inline">\(p(\theta|y)\)</span>. Frequentist methods assess the probability of the data given a null hypothesis, <span class="math inline">\(p(y|H_0)\)</span>, or they give estimates with a standard error for specific model parameters.</p>
<p>The second important difference is that Bayesians combine prior information with information that is obtained from data, whereas frequentists solely infer from the data at hand. Because prior information may influence inference, some people have been resistant to using Bayesian statistics. However, if prior influence is reported transparently, the degree of information in the data becomes even clearer than without any prior influence. Bayesian statistics provides the theory to combine information from different sources and, therefore, meta-analyses become relatively simple using Bayesian analysis.</p>
<table>
<caption><span id="tab:TabBayesFreq">Table 2.3: </span> Characteristics of Bayesian and frequentist statistics.</caption>
<colgroup>
<col width="26%" />
<col width="36%" />
<col width="36%" />
</colgroup>
<thead>
<tr>
<th align="left"></th>
<th align="left">Bayesian statistics</th>
<th align="left">Frequentist statistics</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Interpretation of probability</td>
<td align="left">Description of what we know about a parameter</td>
<td align="left">Relative frequency of an event</td>
</tr>
<tr>
<td align="left">Information used</td>
<td align="left">Prior distribution <span class="math inline">\(p(\theta)\)</span> and likelihood <span class="math inline">\(p(y|\theta)\)</span>, that is, data</td>
<td align="left">Data only</td>
</tr>
<tr>
<td align="left">Uncertainty measurement</td>
<td align="left">Compatibility interval (or credible interval)</td>
<td align="left">Confidence interval</td>
</tr>
<tr>
<td align="left">Hypotheses</td>
<td align="left">Probability of meaningful hypotheses <span class="math inline">\(p(H|y)\)</span></td>
<td align="left">Null hypothesis tests <span class="math inline">\(p(y|H_0)\)</span></td>
</tr>
</tbody>
</table>
<p>Doing science consists of updating prior knowledge with information from experiments or observations to obtain new (posterior) knowledge about a parameter. The Bayesian way of analyzing data formalizes the learning process of science. One of the reasons for the development of frequentist statistics was that it was not feasible to do the simulation part of Bayesian statistics before the computer age (because of the earlier application of frequentist statistical methods, they are sometimes called the “classical” statistical methods, even though the idea of Bayes is older). Nowadays, everybody can use his or her own computer to draw values from posterior distributions very easily.</p>
<p>Some scientists even reject Bayesian statistics mainly because they argue that using prior information makes a study subjective (see e.g., discussion in the Forum of Ecological Applications, 19, 2009). Frequentist analysis is subjective too! Subjectivity enters at many levels, from deciding what questions to investigate, which data to collect, which results to publish, and so on. Using prior information is not the only subjective step during a data analysis - the key thing is to report what has been done. A famous statistician once asked us whether we had ever laughed out loud at a statistical result because it absolutely did not make any sense? If yes, this is clear evidence that we have prior knowledge; clearly it is reasonable to report this prior knowledge and use it in the analysis.</p>
<p>In this book, we primarily use Bayesian methods because they allow more meaningful inferences (i.e., probabilities of specific hypotheses, posterior distributions for derived quantities). In fact, in the case of generalized linear mixed models, they are the only exact way to draw inference <span class="citation">(<a href="referenzen.html#ref-Bolker.2009">Bolker et al. 2009</a>)</span>. Bayesian methods make the fit of more complex models (such as models that estimate true state together with an observation process) feasible. They make meta-analysis relatively simple. And error propagation to derived parameters (e.g. to create an effect plot) is ridiculously easy: As a result from a Bayesian analyses, we get many, e.g. 4000, draws from the joint posterior distribution, i.e. 4000 set of parameters - whatever derived quantity we are interested in, we simply calculate it using each draw in turn, giving us 4000 values for the derived parameter, which represents the uncertainty of that derived parameter (the 2.5% and 97.5% quantiles of these 4000 values can be used as a 95% compatibility interval).</p>
<p>While our book clearly focuses on Bayesian analyses, we also briefly introduce the most important classical frequentist tests associated with linear models because they are still widely used, and because almost 100 years of scientific literature has used these techniques that we, as readers, would like to understand.</p>
</div>
<div id="summary" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Summary<a href="basics.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian data analysis is applying the Bayes theorem for summarizing knowledge based on data, priors and the model assumptions.</p>
<p>Frequentist statistics is quantifying uncertainty by hypothetical repetitions.</p>
<p>The Bayesian approach to statisticla modelling offers the following advantages over the frequentist approach:</p>
<ul>
<li>Unbiased parameter estimates in the context of mixed models</li>
<li>Specific hypotheses can be tested</li>
<li>Prior knowledge, if available, can be incorporated</li>
<li>Meta-analyses are relatively simple</li>
<li>Error propagation to derived parameters is ridiculously simple</li>
<li>More complex models can be fitted, with more modelling flexibility</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="PART-I.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="analyses_steps.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/TobiasRoth/BDAEcology/edit/master/1.1-prerequisites.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
