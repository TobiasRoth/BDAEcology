<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan</title>
  <meta name="description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/cover.jpg" />
  <meta property="og:description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="github-repo" content="TobiasRoth/BDAEcology" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Normal Linear Models | Bayesian Data Analysis in Ecology with R and Stan" />
  
  <meta name="twitter:description" content="This GitHub-book is collection of updates and additional material to the book Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN." />
  <meta name="twitter:image" content="/images/cover.jpg" />

<meta name="author" content="Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jerôme Guélat, Bettina Almasi, Pius Korner-Nievergelt" />


<meta name="date" content="2023-01-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="priors.html"/>
<link rel="next" href="residualanalysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="settings/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book"><i class="fa fa-check"></i>Why this book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-contribute"><i class="fa fa-check"></i>How to contribute?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="part"><span><b>I BASIC STATISTICS FOR ECOLOGISTS</b></span></li>
<li class="chapter" data-level="1" data-path="PART-I.html"><a href="PART-I.html"><i class="fa fa-check"></i><b>1</b> Introduction to PART I</a>
<ul>
<li class="chapter" data-level="1.1" data-path="PART-I.html"><a href="PART-I.html#further-reading"><i class="fa fa-check"></i><b>1.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>2</b> Prerequisits: Basic statistical terms</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basics.html"><a href="basics.html#variables-and-observations"><i class="fa fa-check"></i><b>2.1</b> Variables and observations</a></li>
<li class="chapter" data-level="2.2" data-path="basics.html"><a href="basics.html#displaying-and-summarizing-variables"><i class="fa fa-check"></i><b>2.2</b> Displaying and summarizing variables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="basics.html"><a href="basics.html#correlations"><i class="fa fa-check"></i><b>2.2.1</b> Correlations</a></li>
<li class="chapter" data-level="2.2.2" data-path="basics.html"><a href="basics.html#principal-components-analyses-pca"><i class="fa fa-check"></i><b>2.2.2</b> Principal components analyses PCA</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="basics.html"><a href="basics.html#inferential-statistics"><i class="fa fa-check"></i><b>2.3</b> Inferential statistics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="basics.html"><a href="basics.html#uncertainty"><i class="fa fa-check"></i><b>2.3.1</b> Uncertainty</a></li>
<li class="chapter" data-level="2.3.2" data-path="basics.html"><a href="basics.html#standard-error"><i class="fa fa-check"></i><b>2.3.2</b> Standard error</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="basics.html"><a href="basics.html#bayes-theorem-and-the-common-aim-of-frequentist-and-bayesian-methods"><i class="fa fa-check"></i><b>2.4</b> Bayes theorem and the common aim of frequentist and Bayesian methods</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="basics.html"><a href="basics.html#bayes-theorem-for-discrete-events"><i class="fa fa-check"></i><b>2.4.1</b> Bayes theorem for discrete events</a></li>
<li class="chapter" data-level="2.4.2" data-path="basics.html"><a href="basics.html#bayes-theorem-for-continuous-parameters"><i class="fa fa-check"></i><b>2.4.2</b> Bayes theorem for continuous parameters</a></li>
<li class="chapter" data-level="2.4.3" data-path="basics.html"><a href="basics.html#estimating-a-mean-assuming-that-the-variance-is-known"><i class="fa fa-check"></i><b>2.4.3</b> Estimating a mean assuming that the variance is known</a></li>
<li class="chapter" data-level="2.4.4" data-path="basics.html"><a href="basics.html#estimating-the-mean-and-the-variance"><i class="fa fa-check"></i><b>2.4.4</b> Estimating the mean and the variance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basics.html"><a href="basics.html#classical-frequentist-tests-and-alternatives"><i class="fa fa-check"></i><b>2.5</b> Classical frequentist tests and alternatives</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="basics.html"><a href="basics.html#nullhypothesis-testing"><i class="fa fa-check"></i><b>2.5.1</b> Nullhypothesis testing</a></li>
<li class="chapter" data-level="2.5.2" data-path="basics.html"><a href="basics.html#comparison-of-a-sample-with-a-fixed-values-one-sample-t-test"><i class="fa fa-check"></i><b>2.5.2</b> Comparison of a sample with a fixed values (one-sample t-test)</a></li>
<li class="chapter" data-level="2.5.3" data-path="basics.html"><a href="basics.html#comparison-of-the-locations-between-two-groups-two-sample-t-test"><i class="fa fa-check"></i><b>2.5.3</b> Comparison of the locations between two groups (two-sample t-test)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="basics.html"><a href="basics.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyses_steps.html"><a href="analyses_steps.html"><i class="fa fa-check"></i><b>3</b> Data analysis step by step</a>
<ul>
<li class="chapter" data-level="3.1" data-path="analyses_steps.html"><a href="analyses_steps.html#step1"><i class="fa fa-check"></i><b>3.1</b> Plausibility of Data</a></li>
<li class="chapter" data-level="3.2" data-path="analyses_steps.html"><a href="analyses_steps.html#step2"><i class="fa fa-check"></i><b>3.2</b> Relationships</a></li>
<li class="chapter" data-level="3.3" data-path="analyses_steps.html"><a href="analyses_steps.html#step3"><i class="fa fa-check"></i><b>3.3</b> Data Distribution</a></li>
<li class="chapter" data-level="3.4" data-path="analyses_steps.html"><a href="analyses_steps.html#step4"><i class="fa fa-check"></i><b>3.4</b> Preparation of Explanatory Variables</a></li>
<li class="chapter" data-level="3.5" data-path="analyses_steps.html"><a href="analyses_steps.html#step5"><i class="fa fa-check"></i><b>3.5</b> Data Structure</a></li>
<li class="chapter" data-level="3.6" data-path="analyses_steps.html"><a href="analyses_steps.html#step6"><i class="fa fa-check"></i><b>3.6</b> Define Prior Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="analyses_steps.html"><a href="analyses_steps.html#step7"><i class="fa fa-check"></i><b>3.7</b> Fit the Model</a></li>
<li class="chapter" data-level="3.8" data-path="analyses_steps.html"><a href="analyses_steps.html#step8"><i class="fa fa-check"></i><b>3.8</b> Check Model</a></li>
<li class="chapter" data-level="3.9" data-path="analyses_steps.html"><a href="analyses_steps.html#step9"><i class="fa fa-check"></i><b>3.9</b> Model Uncertainty</a></li>
<li class="chapter" data-level="3.10" data-path="analyses_steps.html"><a href="analyses_steps.html#step10"><i class="fa fa-check"></i><b>3.10</b> Draw Conclusions</a></li>
<li class="chapter" data-level="" data-path="analyses_steps.html"><a href="analyses_steps.html#further-reading-1"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>4</b> Probability distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distributions.html"><a href="distributions.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="distributions.html"><a href="distributions.html#discrete-distributions"><i class="fa fa-check"></i><b>4.2</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distributions.html"><a href="distributions.html#bernoulli-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="4.2.2" data-path="distributions.html"><a href="distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="4.2.3" data-path="distributions.html"><a href="distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>4.2.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="4.2.4" data-path="distributions.html"><a href="distributions.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.2.4</b> Negative-binomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="distributions.html"><a href="distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>4.3</b> Continuous distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="distributions.html"><a href="distributions.html#beta-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Normal distribution</a></li>
<li class="chapter" data-level="4.3.3" data-path="distributions.html"><a href="distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.3</b> Gamma distribution</a></li>
<li class="chapter" data-level="4.3.4" data-path="distributions.html"><a href="distributions.html#cauchydistri"><i class="fa fa-check"></i><b>4.3.4</b> Cauchy distribution</a></li>
<li class="chapter" data-level="4.3.5" data-path="distributions.html"><a href="distributions.html#t-distribution"><i class="fa fa-check"></i><b>4.3.5</b> t-distribution</a></li>
<li class="chapter" data-level="4.3.6" data-path="distributions.html"><a href="distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.3.6</b> F-distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="rfunctions.html"><a href="rfunctions.html"><i class="fa fa-check"></i><b>5</b> Important R-functions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="rfunctions.html"><a href="rfunctions.html#data-preparation"><i class="fa fa-check"></i><b>5.1</b> Data preparation</a></li>
<li class="chapter" data-level="5.2" data-path="rfunctions.html"><a href="rfunctions.html#figures"><i class="fa fa-check"></i><b>5.2</b> Figures</a></li>
<li class="chapter" data-level="5.3" data-path="rfunctions.html"><a href="rfunctions.html#summary-1"><i class="fa fa-check"></i><b>5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html"><i class="fa fa-check"></i><b>6</b> Reproducible research</a>
<ul>
<li class="chapter" data-level="6.1" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html#summary-2"><i class="fa fa-check"></i><b>6.1</b> Summary</a></li>
<li class="chapter" data-level="6.2" data-path="reproducibleresearch.html"><a href="reproducibleresearch.html#further-reading-2"><i class="fa fa-check"></i><b>6.2</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="furthertopics.html"><a href="furthertopics.html"><i class="fa fa-check"></i><b>7</b> Further topics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="furthertopics.html"><a href="furthertopics.html#bioacoustic-analyse"><i class="fa fa-check"></i><b>7.1</b> Bioacoustic analyse</a></li>
<li class="chapter" data-level="7.2" data-path="furthertopics.html"><a href="furthertopics.html#python"><i class="fa fa-check"></i><b>7.2</b> Python</a></li>
</ul></li>
<li class="part"><span><b>II BAYESIAN DATA ANALYSIS</b></span></li>
<li class="chapter" data-level="8" data-path="PART-II.html"><a href="PART-II.html"><i class="fa fa-check"></i><b>8</b> Introduction to PART II</a>
<ul>
<li class="chapter" data-level="" data-path="PART-II.html"><a href="PART-II.html#further-reading-3"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html"><i class="fa fa-check"></i><b>9</b> The Bayesian paradigm</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian_paradigm.html"><a href="bayesian_paradigm.html#summary-3"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="priors.html"><a href="priors.html"><i class="fa fa-check"></i><b>10</b> Prior distributions</a>
<ul>
<li class="chapter" data-level="10.1" data-path="priors.html"><a href="priors.html#introduction-2"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="priors.html"><a href="priors.html#choosepriors"><i class="fa fa-check"></i><b>10.2</b> How to choose a prior</a></li>
<li class="chapter" data-level="10.3" data-path="priors.html"><a href="priors.html#prior-sensitivity"><i class="fa fa-check"></i><b>10.3</b> Prior sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>11</b> Normal Linear Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lm.html"><a href="lm.html#linear-regression"><i class="fa fa-check"></i><b>11.1</b> Linear regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="lm.html"><a href="lm.html#background"><i class="fa fa-check"></i><b>11.1.1</b> Background</a></li>
<li class="chapter" data-level="11.1.2" data-path="lm.html"><a href="lm.html#fitting-a-linear-regression-in-r"><i class="fa fa-check"></i><b>11.1.2</b> Fitting a Linear Regression in R</a></li>
<li class="chapter" data-level="11.1.3" data-path="lm.html"><a href="lm.html#drawing-conclusions"><i class="fa fa-check"></i><b>11.1.3</b> Drawing Conclusions</a></li>
<li class="chapter" data-level="11.1.4" data-path="lm.html"><a href="lm.html#interpretation-of-the-r-summary-output"><i class="fa fa-check"></i><b>11.1.4</b> Interpretation of the R summary output</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="lm.html"><a href="lm.html#linear-model-with-one-categorical-predictor-one-way-anova"><i class="fa fa-check"></i><b>11.2</b> Linear model with one categorical predictor (one-way ANOVA)</a></li>
<li class="chapter" data-level="11.3" data-path="lm.html"><a href="lm.html#other-variants-of-normal-linear-models-two-way-anova-analysis-of-covariance-and-multiple-regression"><i class="fa fa-check"></i><b>11.3</b> Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="lm.html"><a href="lm.html#linear-model-with-two-categorical-predictors-two-way-anova"><i class="fa fa-check"></i><b>11.3.1</b> Linear model with two categorical predictors (two-way ANOVA)</a></li>
<li class="chapter" data-level="11.3.2" data-path="lm.html"><a href="lm.html#a-linear-model-with-a-categorical-and-a-numeric-predictor-ancova"><i class="fa fa-check"></i><b>11.3.2</b> A linear model with a categorical and a numeric predictor (ANCOVA)</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="lm.html"><a href="lm.html#partial-coefficients-and-some-comments-on-collinearity"><i class="fa fa-check"></i><b>11.4</b> Partial coefficients and some comments on collinearity</a></li>
<li class="chapter" data-level="11.5" data-path="lm.html"><a href="lm.html#orderedfactors"><i class="fa fa-check"></i><b>11.5</b> Ordered Factors and Contrasts</a></li>
<li class="chapter" data-level="11.6" data-path="lm.html"><a href="lm.html#quadratic-and-higher-polynomial-terms"><i class="fa fa-check"></i><b>11.6</b> Quadratic and Higher Polynomial Terms</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="residualanalysis.html"><a href="residualanalysis.html"><i class="fa fa-check"></i><b>12</b> Assessing Model Assumptions</a>
<ul>
<li class="chapter" data-level="12.1" data-path="residualanalysis.html"><a href="residualanalysis.html#model-assumptions"><i class="fa fa-check"></i><b>12.1</b> Model Assumptions</a></li>
<li class="chapter" data-level="12.2" data-path="residualanalysis.html"><a href="residualanalysis.html#independent-and-identically-distributed"><i class="fa fa-check"></i><b>12.2</b> Independent and Identically Distributed</a></li>
<li class="chapter" data-level="12.3" data-path="residualanalysis.html"><a href="residualanalysis.html#qqplot"><i class="fa fa-check"></i><b>12.3</b> The QQ-Plot</a></li>
<li class="chapter" data-level="12.4" data-path="residualanalysis.html"><a href="residualanalysis.html#tempautocorrelation"><i class="fa fa-check"></i><b>12.4</b> Temporal Autocorrelation</a></li>
<li class="chapter" data-level="12.5" data-path="residualanalysis.html"><a href="residualanalysis.html#spatialautocorrelation"><i class="fa fa-check"></i><b>12.5</b> Spatial Autocorrelation</a></li>
<li class="chapter" data-level="12.6" data-path="residualanalysis.html"><a href="residualanalysis.html#Heteroscedasticity"><i class="fa fa-check"></i><b>12.6</b> Heteroscedasticity</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lmer.html"><a href="lmer.html"><i class="fa fa-check"></i><b>13</b> Linear Mixed Effect Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="lmer.html"><a href="lmer.html#background-1"><i class="fa fa-check"></i><b>13.1</b> Background</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="lmer.html"><a href="lmer.html#why-mixed-effects-models"><i class="fa fa-check"></i><b>13.1.1</b> Why Mixed Effects Models?</a></li>
<li class="chapter" data-level="13.1.2" data-path="lmer.html"><a href="lmer.html#random-factors-and-partial-pooling"><i class="fa fa-check"></i><b>13.1.2</b> Random Factors and Partial Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>14</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="glm.html"><a href="glm.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="glm.html"><a href="glm.html#summary-4"><i class="fa fa-check"></i><b>14.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="glmm.html"><a href="glmm.html"><i class="fa fa-check"></i><b>15</b> Generalized linear mixed models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="glmm.html"><a href="glmm.html#introduction-4"><i class="fa fa-check"></i><b>15.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="glmm.html"><a href="glmm.html#binomial-mixed-model"><i class="fa fa-check"></i><b>15.1.1</b> Binomial Mixed Model</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="glmm.html"><a href="glmm.html#summary-5"><i class="fa fa-check"></i><b>15.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modelchecking.html"><a href="modelchecking.html"><i class="fa fa-check"></i><b>16</b> Posterior predictive model checking</a>
<ul>
<li class="chapter" data-level="16.1" data-path="modelchecking.html"><a href="modelchecking.html#introduction-5"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="modelchecking.html"><a href="modelchecking.html#summary-6"><i class="fa fa-check"></i><b>16.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="model_comparison.html"><a href="model_comparison.html"><i class="fa fa-check"></i><b>17</b> Model comparison and multimodel inference</a>
<ul>
<li class="chapter" data-level="17.1" data-path="model_comparison.html"><a href="model_comparison.html#introduction-6"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="model_comparison.html"><a href="model_comparison.html#summary-7"><i class="fa fa-check"></i><b>17.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="stan.html"><a href="stan.html"><i class="fa fa-check"></i><b>18</b> MCMC using Stan</a>
<ul>
<li class="chapter" data-level="18.1" data-path="stan.html"><a href="stan.html#background-3"><i class="fa fa-check"></i><b>18.1</b> Background</a></li>
<li class="chapter" data-level="18.2" data-path="stan.html"><a href="stan.html#install-rstan"><i class="fa fa-check"></i><b>18.2</b> Install <code>rstan</code></a></li>
<li class="chapter" data-level="18.3" data-path="stan.html"><a href="stan.html#firststanmod"><i class="fa fa-check"></i><b>18.3</b> Writing a Stan model</a></li>
<li class="chapter" data-level="18.4" data-path="stan.html"><a href="stan.html#run-stan-from-r"><i class="fa fa-check"></i><b>18.4</b> Run Stan from R</a></li>
<li class="chapter" data-level="" data-path="stan.html"><a href="stan.html#further-reading-4"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ridge_regression.html"><a href="ridge_regression.html"><i class="fa fa-check"></i><b>19</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ridge_regression.html"><a href="ridge_regression.html#introduction-7"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="SEM.html"><a href="SEM.html"><i class="fa fa-check"></i><b>20</b> Structural equation models</a>
<ul>
<li class="chapter" data-level="20.1" data-path="SEM.html"><a href="SEM.html#introduction-8"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="spatial_glmm.html"><a href="spatial_glmm.html"><i class="fa fa-check"></i><b>21</b> Modeling spatial data using GLMM</a>
<ul>
<li class="chapter" data-level="21.1" data-path="spatial_glmm.html"><a href="spatial_glmm.html#introduction-9"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="spatial_glmm.html"><a href="spatial_glmm.html#summary-8"><i class="fa fa-check"></i><b>21.2</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>III ECOLOGICAL MODELS</b></span></li>
<li class="chapter" data-level="22" data-path="PART-III.html"><a href="PART-III.html"><i class="fa fa-check"></i><b>22</b> Introduction to PART III</a>
<ul>
<li class="chapter" data-level="22.1" data-path="PART-III.html"><a href="PART-III.html#model-notations"><i class="fa fa-check"></i><b>22.1</b> Model notations</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html"><i class="fa fa-check"></i><b>23</b> Zero-inflated Poisson Mixed Model</a>
<ul>
<li class="chapter" data-level="23.1" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#introduction-10"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#example-data"><i class="fa fa-check"></i><b>23.2</b> Example data</a></li>
<li class="chapter" data-level="23.3" data-path="zeroinflated-poisson-lmm.html"><a href="zeroinflated-poisson-lmm.html#model"><i class="fa fa-check"></i><b>23.3</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="dailynestsurv.html"><a href="dailynestsurv.html"><i class="fa fa-check"></i><b>24</b> Daily nest survival</a>
<ul>
<li class="chapter" data-level="24.1" data-path="dailynestsurv.html"><a href="dailynestsurv.html#background-4"><i class="fa fa-check"></i><b>24.1</b> Background</a></li>
<li class="chapter" data-level="24.2" data-path="dailynestsurv.html"><a href="dailynestsurv.html#models-for-estimating-daily-nest-survival"><i class="fa fa-check"></i><b>24.2</b> Models for estimating daily nest survival</a></li>
<li class="chapter" data-level="24.3" data-path="dailynestsurv.html"><a href="dailynestsurv.html#known-fate-model"><i class="fa fa-check"></i><b>24.3</b> Known fate model</a></li>
<li class="chapter" data-level="24.4" data-path="dailynestsurv.html"><a href="dailynestsurv.html#dailynestsurvstan"><i class="fa fa-check"></i><b>24.4</b> The Stan model</a></li>
<li class="chapter" data-level="24.5" data-path="dailynestsurv.html"><a href="dailynestsurv.html#prepare-data-and-run-stan"><i class="fa fa-check"></i><b>24.5</b> Prepare data and run Stan</a></li>
<li class="chapter" data-level="24.6" data-path="dailynestsurv.html"><a href="dailynestsurv.html#check-convergence"><i class="fa fa-check"></i><b>24.6</b> Check convergence</a></li>
<li class="chapter" data-level="24.7" data-path="dailynestsurv.html"><a href="dailynestsurv.html#look-at-results"><i class="fa fa-check"></i><b>24.7</b> Look at results</a></li>
<li class="chapter" data-level="24.8" data-path="dailynestsurv.html"><a href="dailynestsurv.html#known-fate-model-for-irregular-nest-controls"><i class="fa fa-check"></i><b>24.8</b> Known fate model for irregular nest controls</a></li>
<li class="chapter" data-level="" data-path="dailynestsurv.html"><a href="dailynestsurv.html#further-reading-5"><i class="fa fa-check"></i>Further reading</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html"><i class="fa fa-check"></i><b>25</b> Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals</a>
<ul>
<li class="chapter" data-level="25.1" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#introduction-11"><i class="fa fa-check"></i><b>25.1</b> Introduction</a></li>
<li class="chapter" data-level="25.2" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#data-description"><i class="fa fa-check"></i><b>25.2</b> Data description</a></li>
<li class="chapter" data-level="25.3" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#model-description"><i class="fa fa-check"></i><b>25.3</b> Model description</a></li>
<li class="chapter" data-level="25.4" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#the-stan-code"><i class="fa fa-check"></i><b>25.4</b> The Stan code</a></li>
<li class="chapter" data-level="25.5" data-path="cjs_with_mix.html"><a href="cjs_with_mix.html#call-stan-from-r-check-convergence-and-look-at-results"><i class="fa fa-check"></i><b>25.5</b> Call Stan from R, check convergence and look at results</a></li>
</ul></li>
<li class="part"><span><b>IV APPENDICES</b></span></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis in Ecology with R and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lm" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">11</span> Normal Linear Models<a href="lm.html#lm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="linear-regression" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Linear regression<a href="lm.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="" target="_blank"><img src="images/snowfinch3.jpg" style="display: block; margin: auto;" /></a></p>
<hr />
<div id="background" class="section level3 hasAnchor" number="11.1.1">
<h3><span class="header-section-number">11.1.1</span> Background<a href="lm.html#background" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression.</p>
<p>Typical questions that can be answered using linear regression are: How does <span class="math inline">\(y\)</span> change with changes in <span class="math inline">\(x\)</span>? How is y predicted from <span class="math inline">\(x\)</span>? An ordinary linear regression (i.e., one numeric <span class="math inline">\(x\)</span> and one numeric <span class="math inline">\(y\)</span> variable) can be represented by a scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. We search for the line that ﬁts best and describe how the observations scatter around this regression line (see Fig. <a href="lm.html#fig:figlm">11.2</a> for an example). The model formula of a simple linear regression with one continuous predictor variable <span class="math inline">\(x_i\)</span> (the subscript <span class="math inline">\(i\)</span> denotes the <span class="math inline">\(i=1,\dots,n\)</span> data points) is:</p>
<p><span class="math display" id="eq:lm">\[\begin{align}
  \mu_i &amp;=\beta_0 + \beta_1 x_i \\
  y_i &amp;\sim normal(\mu_i, \sigma^2)
  \tag{11.1}
\end{align}\]</span></p>
<p>While the first part of Equation <a href="lm.html#eq:lm">(11.1)</a> describes the regression line, the second part describes how the data points, also called observations, are distributed around the regression line (Figure <a href="lm.html#fig:illlm">11.1</a>). In other words: the observation <span class="math inline">\(y_i\)</span> stems from a normal distribution with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The mean of the normal distribution, <span class="math inline">\(\mu_i\)</span> , equals the sum of the intercept (<span class="math inline">\(b_0\)</span> ) and the product of the slope (<span class="math inline">\(b_1\)</span>) and the continuous predictor value, <span class="math inline">\(x_i\)</span>.</p>
<p>Equation <a href="lm.html#eq:lm">(11.1)</a> is called the data model, because it describes mathematically the process that has (or, better, that we think has) produced the data. This nomenclature also helps to distinguish data models from models for parameters such as prior or posterior distributions.</p>
<p>The differences between observation <span class="math inline">\(y_i\)</span> and the predicted values <span class="math inline">\(\mu_i\)</span> are the residuals (i.e., <span class="math inline">\(\epsilon_i=y_i-\mu_i\)</span>). Equivalently to Equation <a href="lm.html#eq:lm">(11.1)</a>, the regression could thus be written as:</p>
<p><span class="math display" id="eq:lmalternativ">\[\begin{align}
  y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i\\
  \epsilon_i &amp;\sim normal(0, \sigma^2)
  \tag{11.2}
\end{align}\]</span></p>
<p>We prefer the notation in Equation <a href="lm.html#eq:lm">(11.1)</a> because, in this formula, the stochastic part (second row) is nicely separated from the deterministic part (first row) of the model, whereas, in the second notation <a href="lm.html#eq:lmalternativ">(11.2)</a> the ﬁrst row contains both stochastic and deterministic parts.</p>
<p>For illustration, we here simulate a data set and below ﬁt a linear regression to these simulated data. The advantage of simulating data is that the following analyses can be reproduced without having to read data into R. Further, for simulating data, we need to translate the algebraic model formula into R language which helps us understanding the model structure.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="lm.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">34</span>)            <span class="co"># set a seed for the random number generator</span></span>
<span id="cb40-2"><a href="lm.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># define the data structure</span></span>
<span id="cb40-3"><a href="lm.html#cb40-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span>                 <span class="co"># sample size</span></span>
<span id="cb40-4"><a href="lm.html#cb40-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">10</span>, <span class="dv">30</span>)   <span class="co"># sample values of the predictor variable</span></span>
<span id="cb40-5"><a href="lm.html#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="lm.html#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># define values for each model parameter</span></span>
<span id="cb40-7"><a href="lm.html#cb40-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">5</span>              <span class="co"># standard deviation of the residuals</span></span>
<span id="cb40-8"><a href="lm.html#cb40-8" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">2</span>                 <span class="co"># intercept</span></span>
<span id="cb40-9"><a href="lm.html#cb40-9" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fl">0.7</span>               <span class="co"># slope</span></span>
<span id="cb40-10"><a href="lm.html#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="lm.html#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate y-values from the model</span></span>
<span id="cb40-12"><a href="lm.html#cb40-12" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x       <span class="co"># define the regression line (deterministic part)</span></span>
<span id="cb40-13"><a href="lm.html#cb40-13" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, mu, <span class="at">sd =</span> sigma) <span class="co"># simulate y-values</span></span>
<span id="cb40-14"><a href="lm.html#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="lm.html#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co"># save data in a data.frame</span></span>
<span id="cb40-16"><a href="lm.html#cb40-16" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:illlm"></span>
<img src="2.03-lm_files/figure-html/illlm-1.png" alt="Illustration of a linear regression. The blue line represents the deterministic part of the model, i.e., here regression line. The stochastic part is represented by a probability distribution, here the normal distribution. The normal distribution changes its mean but not the variance along the x-axis, and it describes how the data are distributed. The blue line and the orange distribution together are a statistical model, i.e., an abstract representation of the data which is given in black." width="672" />
<p class="caption">
Figure 11.1: Illustration of a linear regression. The blue line represents the deterministic part of the model, i.e., here regression line. The stochastic part is represented by a probability distribution, here the normal distribution. The normal distribution changes its mean but not the variance along the x-axis, and it describes how the data are distributed. The blue line and the orange distribution together are a statistical model, i.e., an abstract representation of the data which is given in black.
</p>
</div>
<p>Using matrix notation equation <a href="lm.html#eq:lm">(11.1)</a> can also be written in one row:</p>
<p><span class="math display">\[\boldsymbol{y} \sim
  Norm(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2\boldsymbol{I})\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{ I}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix (it transforms the variance parameter to a <span class="math inline">\(n \times n\)</span> matrix with its diagonal elements equal <span class="math inline">\(\sigma^2\)</span> ; <span class="math inline">\(n\)</span> is the sample size). The multiplication by <span class="math inline">\(\boldsymbol{ I}\)</span> is necessary because we use vector notation, <span class="math inline">\(\boldsymbol{y}\)</span> instead of <span class="math inline">\(y_{i}\)</span> . Here, <span class="math inline">\(\boldsymbol{y}\)</span> is the vector of all observations, whereas <span class="math inline">\(y_{i}\)</span> is a single observation, <span class="math inline">\(i\)</span>. When using vector notation, we can write the linear predictor of the model, <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> , as a multiplication of the vector of the model coefﬁcients</p>
<p><span class="math display">\[\boldsymbol{\beta} =
  \begin{pmatrix}
    \beta_0 \\
    \beta_1
  \end{pmatrix}\]</span></p>
<p>times the model matrix</p>
<p><span class="math display">\[\boldsymbol{X} =
  \begin{pmatrix}
      1     &amp; x_1   \\
      \dots &amp; \dots \\
      1     &amp; x_n
  \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(x_1 , \dots, x_n\)</span> are the observed values for the predictor variable, <span class="math inline">\(x\)</span>. The ﬁrst column of <span class="math inline">\(\boldsymbol{X}\)</span> contains only ones because the values in this column are multiplied with the intercept, <span class="math inline">\(\beta_0\)</span> . To the intercept, the product of the second element of <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\beta_1\)</span> , with each element in the second column of <span class="math inline">\(\boldsymbol{X}\)</span> is added to obtain the predicted value for each observation, <span class="math inline">\(\boldsymbol{\mu}\)</span>:</p>
<p><span class="math display" id="eq:lmmatrix">\[\begin{align}
\boldsymbol{X \beta}=
\begin{pmatrix}
      1     &amp; x_1   \\
      \dots &amp; \dots \\
      1     &amp; x_n
  \end{pmatrix}
  \times
  \begin{pmatrix}
    \beta_0 \\
    \beta_1
\end{pmatrix} =
  \begin{pmatrix}
    \beta_0 + \beta_1x_1 \\
    \dots \\
    \beta_0 + \beta_1x_n
\end{pmatrix}=
\begin{pmatrix}
    \hat{y}_1 \\
  \dots \\
    \hat{y}_n
\end{pmatrix} =
\boldsymbol{\mu}
\tag{11.3}
\end{align}\]</span></p>
</div>
<div id="fitting-a-linear-regression-in-r" class="section level3 hasAnchor" number="11.1.2">
<h3><span class="header-section-number">11.1.2</span> Fitting a Linear Regression in R<a href="lm.html#fitting-a-linear-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Equation <a href="lm.html#eq:lm">(11.1)</a>, the fitted values <span class="math inline">\(\mu_i\)</span> are directly deﬁned by the model coefﬁcients, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> . Therefore, when we can estimate <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span> , and <span class="math inline">\(\sigma^2\)</span>, the model is fully deﬁned. The last parameter <span class="math inline">\(\sigma^2\)</span> describes how the observations scatter around the regression line and relies on the assumption that the residuals are normally distributed. The estimates for the model parameters of a linear regression are obtained by searching for the best ﬁtting regression line. To do so, we search for the regression line that minimizes the sum of the squared residuals. This model ﬁtting method is called the least-squares method, abbreviated as LS. It has a very simple solution using matrix algebra <span class="citation">(see e.g., <a href="referenzen.html#ref-Aitkin.2009" role="doc-biblioref">Aitkin et al. 2009</a>)</span>.</p>
<p>The least-squares estimates for the model parameters of a linear regression are obtained in R using the function <code>lm</code>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="lm.html#cb41-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span>  <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dat)</span>
<span id="cb41-2"><a href="lm.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span></code></pre></div>
<pre><code>## (Intercept)           x 
##   2.0049517   0.6880415</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="lm.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 5.04918</code></pre>
<p>The object “mod” produced by <code>lm</code> contains the estimates for the intercept, <span class="math inline">\(\beta_0\)</span> , and the slope, <span class="math inline">\(\beta_1\)</span>. The residual standard deviation <span class="math inline">\(\sigma^2\)</span> is extracted using the function <code>summary</code>. We can show the result of the linear regression as a line in a scatter plot with the covariate (<code>x</code>) on the x-axis and the observations (<code>y</code>) on the y-axis (Fig. <a href="lm.html#fig:figlm">11.2</a>).</p>
<div class="figure"><span style="display:block;" id="fig:figlm"></span>
<img src="2.03-lm_files/figure-html/figlm-1.png" alt="Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The ﬁtted values lie where the orange dotted lines touch the blue regression line." width="672" />
<p class="caption">
Figure 11.2: Linear regression. Black dots = observations, blue solid line = regression line, orange dotted lines = residuals. The ﬁtted values lie where the orange dotted lines touch the blue regression line.
</p>
</div>
<p>Conclusions drawn from a model depend on the model assumptions. When model assumptions are violated, estimates usually are biased and inappropriate conclusions can be drawn. We devote Chapter <a href="residualanalysis.html#residualanalysis">12</a> to the assessment of model assumptions, given its importance.</p>
</div>
<div id="drawing-conclusions" class="section level3 hasAnchor" number="11.1.3">
<h3><span class="header-section-number">11.1.3</span> Drawing Conclusions<a href="lm.html#drawing-conclusions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To answer the question about how strongly <span class="math inline">\(y\)</span> is related to <span class="math inline">\(x\)</span> taking into account statistical uncertainty we look at the joint posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> (vector that contains <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> ) and <span class="math inline">\(\sigma^2\)</span> , the residual variance. The function <code>sim</code> calculates the joint posterior distribution and renders a simulated values from this distribution.</p>
<font size="1">
<div style="border: 2px solid grey;">
<p>What does <code>sim</code> do?<br />
It simulates parameter values from the joint posterior distribution of a model assuming flat prior distributions. For a normal linear regression, it ﬁrst draws a random value, <span class="math inline">\(\sigma^*\)</span> from the marginal posterior distribution of <span class="math inline">\(\sigma\)</span>, and then draws random values from the conditional posterior distribution for <span class="math inline">\(\boldsymbol{\beta}\)</span> given <span class="math inline">\(\sigma^*\)</span> <span class="citation">(<a href="referenzen.html#ref-Gelman.2014" role="doc-biblioref">A. Gelman et al. 2014a</a>)</span>.</p>
<p>The conditional posterior distribution of the parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^*,\boldsymbol{y,X})\)</span> can be analytically derived. With ﬂat prior distributions, it is a uni- or multivariate normal distribution <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^*,\boldsymbol{y,X})=normal(\boldsymbol{\hat{\beta}},V_\beta,(\sigma^*)^2)\)</span> with:</p>
<p><span class="math display" id="eq:sim">\[\begin{align}
  \boldsymbol{\hat{\beta}=(\boldsymbol{X^TX})^{-1}X^Ty}
  \tag{11.4}
\end{align}\]</span></p>
<p>and <span class="math inline">\(V_\beta = (\boldsymbol{X^T X})^{-1}\)</span>.</p>
<p>The marginal posterior distribution of <span class="math inline">\(\sigma^2\)</span> is independent of speciﬁc values of <span class="math inline">\(\boldsymbol{\beta}\)</span>. It is, for ﬂat prior distributions, an inverse chi-square distribution <span class="math inline">\(p(\sigma^2|\boldsymbol{y,X})=Inv-\chi^2(n-k,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2 = \frac{1}{n-k}(\boldsymbol{y}-\boldsymbol{X,\hat{\beta}})^T(\boldsymbol{y}-\boldsymbol{X,\hat{\beta}})\)</span>, and <span class="math inline">\(k\)</span> is the number of parameters. The marginal posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> can be obtained by integrating the conditional posterior distribution <span class="math inline">\(p(\boldsymbol{\beta}|\sigma^2,\boldsymbol{y,X})=normal(\boldsymbol{\hat{\beta}},V_\beta\sigma^2)\)</span> over the distribution of <span class="math inline">\(\sigma^2\)</span> . This results in a uni- or multivariate <span class="math inline">\(t\)</span>-distribution.</p>
<p>Because <code>sim</code> simulates values <span class="math inline">\(\beta_0^*\)</span> and <span class="math inline">\(\beta_1^*\)</span> always conditional on <span class="math inline">\(\sigma^*\)</span>, a triplet of values (<span class="math inline">\(\beta_0^*\)</span>, <span class="math inline">\(\beta_1^*\)</span>, <span class="math inline">\(\sigma^*\)</span>) is one draw of the joint posterior distribution. When we visualize the distribution of the simulated values for one parameter only, ignoring the values for the other, we display the marginal posterior distribution of that parameter. Thus, the distribution of all simulated values for the parameter <span class="math inline">\(\beta_0\)</span> is a <span class="math inline">\(t\)</span>-distribution even if a normal distribution has been used for simulating the values. The <span class="math inline">\(t\)</span>-distribution is a consequence of using a different <span class="math inline">\(\sigma^2\)</span>-value for every draw of <span class="math inline">\(\beta_0\)</span>.</p>
</div>
<p></font></p>
<p>Using the function <code>sim</code> from the package, we can draw values from the joint posterior distribution of the model parameters and describe the marginal posterior distribution of each model parameter using these simulated values.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="lm.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb45-2"><a href="lm.html#cb45-2" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb45-3"><a href="lm.html#cb45-3" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim =</span> nsim)</span></code></pre></div>
<p>The function <code>sim</code> simulates (in our example) 1000 values from the joint posterior distribution of the three model parameters <span class="math inline">\(\beta_0\)</span> , <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span>. These simulated values are shown in Figure <a href="lm.html#fig:simfirstexample">11.3</a>.</p>
<div class="figure"><span style="display:block;" id="fig:simfirstexample"></span>
<img src="2.03-lm_files/figure-html/simfirstexample-1.png" alt="Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters." width="768" />
<p class="caption">
Figure 11.3: Joint (scatterplots) and marginal (histograms) posterior distribution of the model parameters. The six scatterplots show, using different axes, the three-dimensional cloud of 1000 simulations from the joint posterior distribution of the three parameters.
</p>
</div>
<p>The posterior distribution describes, given the data and the model, which values relative to each other are more likely to correspond to the parameter value we aim at measuring. It expresses the uncertainty of the parameter estimate. It shows what we know about the model parameter after having looked at the data and given the model is realistic.</p>
<p>The 2.5% and 97.5% quantiles of the marginal posterior distributions can be used as 95% uncertainty intervals of the model parameters. The function <code>coef</code> extracts the simulated values for the beta coefﬁcients, returning a matrix with <em>nsim</em> rows and the number of columns corresponding to the number of parameters. In our example, the ﬁrst column contains the simulated values from the posterior distribution of the intercept and the second column contains values from the posterior distribution of the slope. The “2” in the second argument of the apply-function (see Chapter <a href="#rmisc"><strong>??</strong></a>) indicates that the <code>quantile</code> function is applied columnwise.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="lm.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(<span class="at">X =</span> <span class="fu">coef</span>(bsim), <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> quantile, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb46-2"><a href="lm.html#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       (Intercept)    x
## 2.5%        -2.95 0.44
## 97.5%        7.17 0.92</code></pre>
<p>We also can calculate an uncertainty interval of the estimated residual standard deviation, <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="lm.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(bsim<span class="sc">@</span>sigma, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb48-2"><a href="lm.html#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   4.2   6.3</code></pre>
<p>We can further get a posterior probability for speciﬁc hypotheses, such as “The slope parameter is larger than 1” or “The slope parameter is larger than 0.5”. These probabilities are the proportion of simulated values from the posterior distribution that are larger than 1 and 0.5, respectively.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="lm.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="dv">1</span>) <span class="sc">/</span> nsim     <span class="co"># alternatively: mean(coef(bsim)[,2]&gt;1)</span></span></code></pre></div>
<pre><code>## [1] 0.008</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="lm.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fl">0.5</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.936</code></pre>
<p>From this, there is very little evidence in the data that the slope is larger than 1, but we are quite conﬁdent that the slope is larger than 0.5 (assuming that our model is realistic).</p>
<p>We often want to show the effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> graphically, with information about the uncertainty of the parameter estimates included in the graph. To draw such effect plots, we use the simulated values from the posterior distribution of the model parameters. From the deterministic part of the model, we know the regression line <span class="math inline">\(\mu = \beta_0 + \beta_1 x_i\)</span>. The simulation from the joint posterior distribution of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We can draw these regression lines in an x-y plot (scatter plot) to show the uncertainty in the regression line estimation (Fig. <a href="lm.html#fig:figlmer1">11.4</a>, left). Note, that in this case it is not advisable to use <code>ggplot</code> because we draw many lines in one plot, which makes <code>ggplot</code> rather slow.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="lm.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb54-2"><a href="lm.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">las =</span> <span class="dv">1</span>, </span>
<span id="cb54-3"><a href="lm.html#cb54-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Outcome (y)&quot;</span>)</span>
<span id="cb54-4"><a href="lm.html#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {</span>
<span id="cb54-5"><a href="lm.html#cb54-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="fu">coef</span>(bsim)[i,<span class="dv">1</span>], <span class="fu">coef</span>(bsim)[i,<span class="dv">2</span>], <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.05</span>))</span>
<span id="cb54-6"><a href="lm.html#cb54-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:figlmer1"></span>
<img src="2.03-lm_files/figure-html/figlmer1-1.png" alt="Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line." width="672" />
<p class="caption">
Figure 11.4: Regression with 1000 lines based on draws form the joint posterior distribution for the intercept and slope parameters to visualize the uncertainty of the estimated regression line.
</p>
</div>
<p>A more convenient way to show uncertainty is to draw the 95% uncertainty interval, CrI, of the regression line. To this end, we ﬁrst deﬁne new x-values for which we would like to have the ﬁtted values (about 100 points across the range of x will produce smooth-looking lines when connected by line segments). We save these new x-values within the new tibble <code>newdat</code>. Then, we create a new model matrix that contains these new x-values (<code>newmodmat</code>) using the function <code>model.matrix</code>. We then calculate the 1000 ﬁtted values for each element of the new x (one value for each of the 1000 simulated regressions, Fig. <a href="lm.html#fig:figlmer1">11.4</a>), using matrix multiplication (%*%). We save these values in the matrix “ﬁtmat”. Finally, we extract the 2.5% and 97.5% quantiles for each x-value from ﬁtmat, and draw the lines for the lower and upper limits of the credible interval (Fig. <a href="lm.html#fig:figlmer2">11.5</a>).</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="lm.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate 95% credible interval</span></span>
<span id="cb55-2"><a href="lm.html#cb55-2" aria-hidden="true" tabindex="-1"></a>newdat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="dv">30</span>, <span class="at">by =</span> <span class="fl">0.1</span>))</span>
<span id="cb55-3"><a href="lm.html#cb55-3" aria-hidden="true" tabindex="-1"></a>newmodmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>( <span class="sc">~</span> x, <span class="at">data =</span> newdat)</span>
<span id="cb55-4"><a href="lm.html#cb55-4" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol =</span> nsim, <span class="at">nrow =</span> <span class="fu">nrow</span>(newdat))</span>
<span id="cb55-5"><a href="lm.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {fitmat[,i] <span class="ot">&lt;-</span> newmodmat <span class="sc">%*%</span> <span class="fu">coef</span>(bsim)[i,]}</span>
<span id="cb55-6"><a href="lm.html#cb55-6" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>CrI_lo <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.025</span>)</span>
<span id="cb55-7"><a href="lm.html#cb55-7" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>CrI_up <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.975</span>)</span>
<span id="cb55-8"><a href="lm.html#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="lm.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make plot</span></span>
<span id="cb55-10"><a href="lm.html#cb55-10" aria-hidden="true" tabindex="-1"></a>regplot <span class="ot">&lt;-</span> </span>
<span id="cb55-11"><a href="lm.html#cb55-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb55-12"><a href="lm.html#cb55-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb55-13"><a href="lm.html#cb55-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb55-14"><a href="lm.html#cb55-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> CrI_lo), <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb55-15"><a href="lm.html#cb55-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> CrI_up), <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb55-16"><a href="lm.html#cb55-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Predictor (x)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Outcome (y)&quot;</span>)</span>
<span id="cb55-17"><a href="lm.html#cb55-17" aria-hidden="true" tabindex="-1"></a>regplot</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:figlmer2"></span>
<img src="2.03-lm_files/figure-html/figlmer2-1.png" alt="Regression with 95% credible interval of the posterior distribution of the ﬁtted values." width="672" />
<p class="caption">
Figure 11.5: Regression with 95% credible interval of the posterior distribution of the ﬁtted values.
</p>
</div>
<p>The interpretation of the 95% uncertainty interval is straightforward: We are 95% sure that the true regression line is within the credible interval (given the data and the model). As with all statistical results, this interpretation is only valid in the model world (if the world would look like the model). The larger the sample size, the narrower the interval, because each additional data point increases information about the true regression line.</p>
<p>The uncertainty interval measures statistical uncertainty of the regression line, but it does not describe how new observations would scatter around the regression line. If we want to describe where future observations will be, we have to report the posterior predictive distribution. We can get a sample of random draws from the posterior predictive distribution <span class="math inline">\(\hat{y}|\boldsymbol{\beta},\sigma^2,\boldsymbol{X}\sim normal( \boldsymbol{X \beta, \sigma^2})\)</span> using the simulated joint posterior distributions of the model parameters, thus taking the uncertainty of the parameter estimates into account. We draw a new <span class="math inline">\(\hat{y}\)</span>-value from <span class="math inline">\(normal( \boldsymbol{X \beta, \sigma^2})\)</span> for each simulated set of model parameters. Then, we can visualize the 2.5% and 97.5% quantiles of this distribution for each new x-value.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="lm.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># increase number of simulation to produce smooth lines of the posterior</span></span>
<span id="cb56-2"><a href="lm.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictive distribution</span></span>
<span id="cb56-3"><a href="lm.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">34</span>)</span>
<span id="cb56-4"><a href="lm.html#cb56-4" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb56-5"><a href="lm.html#cb56-5" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span>
<span id="cb56-6"><a href="lm.html#cb56-6" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat))</span>
<span id="cb56-7"><a href="lm.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) fitmat[,i] <span class="ot">&lt;-</span> newmodmat<span class="sc">%*%</span><span class="fu">coef</span>(bsim)[i,]</span>
<span id="cb56-8"><a href="lm.html#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="lm.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare matrix for simulated new data</span></span>
<span id="cb56-10"><a href="lm.html#cb56-10" aria-hidden="true" tabindex="-1"></a>newy <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat)) </span>
<span id="cb56-11"><a href="lm.html#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="lm.html#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for each simulated ﬁtted value, simulate one new y-value</span></span>
<span id="cb56-13"><a href="lm.html#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) {</span>
<span id="cb56-14"><a href="lm.html#cb56-14" aria-hidden="true" tabindex="-1"></a>  newy[,i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">nrow</span>(newdat), <span class="at">mean =</span> fitmat[,i], <span class="at">sd =</span> bsim<span class="sc">@</span>sigma[i])</span>
<span id="cb56-15"><a href="lm.html#cb56-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-16"><a href="lm.html#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="lm.html#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate 2.5% and 97.5% quantiles</span></span>
<span id="cb56-18"><a href="lm.html#cb56-18" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>pred_lo <span class="ot">&lt;-</span> <span class="fu">apply</span>(newy, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.025</span>)</span>
<span id="cb56-19"><a href="lm.html#cb56-19" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>pred_up <span class="ot">&lt;-</span> <span class="fu">apply</span>(newy, <span class="dv">1</span>, quantile, <span class="at">probs =</span> <span class="fl">0.975</span>)</span>
<span id="cb56-20"><a href="lm.html#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="lm.html#cb56-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the posterior predictive distribution to plot</span></span>
<span id="cb56-22"><a href="lm.html#cb56-22" aria-hidden="true" tabindex="-1"></a>regplot <span class="sc">+</span></span>
<span id="cb56-23"><a href="lm.html#cb56-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred_lo), <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb56-24"><a href="lm.html#cb56-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> newdat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> pred_up), <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:figlmer3"></span>
<img src="2.03-lm_files/figure-html/figlmer3-1.png" alt="Regression line with 95% uncertainty interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines." width="672" />
<p class="caption">
Figure 11.6: Regression line with 95% uncertainty interval (dotted lines) and the 95% interval of the simulated predictive distribution (broken lines). Note that we increased the number of simulations to 50,000 to produce smooth lines.
</p>
</div>
<p>Of future observations, 95% are expected to be within the interval deﬁned by the broken lines in Fig. <a href="lm.html#fig:figlmer3">11.6</a>. Increasing sample size will not give a narrower predictive distribution because the predictive distribution primarily depends on the residual variance <span class="math inline">\(\sigma^2\)</span> which is a property of the data that is independent of sample size.</p>
<p>The way we produced Fig. <a href="lm.html#fig:figlmer3">11.6</a> is somewhat tedious compared to how easy we could have obtained the same ﬁgure using frequentist methods: <code>predict(mod, newdata = newdat, interval = "prediction")</code> would have produced the y-values for the lower and upper lines in Fig. <a href="lm.html#fig:figlmer3">11.6</a> in one R-code line. However, once we have a simulated sample of the posterior predictive distribution, we have much more information than is contained in the frequentist prediction interval. For example, we could give an estimate for the proportion of observations greater than 20, given <span class="math inline">\(x = 25\)</span>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="lm.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(newy[newdat<span class="sc">$</span>x <span class="sc">==</span> <span class="dv">25</span>, ] <span class="sc">&gt;</span> <span class="dv">20</span>) <span class="sc">/</span> nsim</span></code></pre></div>
<pre><code>## [1] 0.44504</code></pre>
<p>Thus, we expect 44% of future observations with <span class="math inline">\(x = 25\)</span> to be higher than 20. We can extract similar information for any relevant threshold value.</p>
<p>Another reason to learn the more complicated R code we presented here, compared to the frequentist methods, is that, for more complicated models such as mixed models, the frequentist methods to obtain conﬁdence intervals of ﬁtted values are much more complicated than the Bayesian method just presented. The latter can be used with only slight adaptations for mixed models and also for generalized linear mixed models.</p>
</div>
<div id="interpretation-of-the-r-summary-output" class="section level3 hasAnchor" number="11.1.4">
<h3><span class="header-section-number">11.1.4</span> Interpretation of the R summary output<a href="lm.html#interpretation-of-the-r-summary-output" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The solution for <span class="math inline">\(\boldsymbol{\beta}\)</span> is the Equation <a href="lm.html#eq:lmmatrix">(11.3)</a>. Most statistical software, including R, return an estimated frequentist standard error for each <span class="math inline">\(\beta_k\)</span>. We extract these standard errors together with the estimates for the model parameters using the <code>summary</code> function.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="lm.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.5777  -3.6280  -0.0532   3.9873  12.1374 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   2.0050     2.5349   0.791       0.433    
## x             0.6880     0.1186   5.800 0.000000507 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.049 on 48 degrees of freedom
## Multiple R-squared:  0.412,  Adjusted R-squared:  0.3998 
## F-statistic: 33.63 on 1 and 48 DF,  p-value: 0.0000005067</code></pre>
<p>The summary output ﬁrst gives a rough summary of the residual distribution. However, we will do more rigorous residual analyses in Chapter <a href="residualanalysis.html#residualanalysis">12</a>. The estimates of the model coefﬁcients follow. The column “Estimate” contains the estimates for the intercept <span class="math inline">\(\beta_0\)</span> and the slope <span class="math inline">\(\beta_1\)</span> . The column “Std. Error” contains the estimated (frequentist) standard errors of the estimates. The last two columns contain the t-value and the p-value of the classical t-test for the null hypothesis that the coefﬁcient equals zero. The last part of the summary output gives the parameter <span class="math inline">\(\sigma\)</span> of the model, named “residual standard error” and the residual degrees of freedom.</p>
<p>We think the name “residual standard error” for “sigma” is confusing, because <span class="math inline">\(\sigma\)</span> is not a measurement of uncertainty of a parameter estimate like the standard errors of the model coefﬁcients are. <span class="math inline">\(\sigma\)</span> is a model parameter that describes how the observations scatter around the ﬁtted values, that is, it is a standard deviation. It is independent of sample size, whereas the standard errors of the estimates for the model parameters will decrease with increasing sample size. Such a standard error of the estimate of <span class="math inline">\(\sigma\)</span>, however, is not given in the summary output. Note that, by using Bayesian methods, we could easily obtain the standard error of the estimated <span class="math inline">\(\sigma\)</span> by calculating the standard deviation of the posterior distribution of <span class="math inline">\(\sigma\)</span>.</p>
<p>The <span class="math inline">\(R^2\)</span> and the adjusted <span class="math inline">\(R^2\)</span> measure the proportion of variance in the outcome variable <span class="math inline">\(y\)</span> that is explained by the predictors in the model. <span class="math inline">\(R^2\)</span> is calculated from the sum of squared residuals, <span class="math inline">\(SSR = \sum_{i=1}^{n}(y_i - \hat{y})\)</span>, and the “total sum of squares”, <span class="math inline">\(SST = \sum_{i=1}^{n}(y_i - \bar{y})\)</span>, where <span class="math inline">\(\bar{y})\)</span> is the mean of <span class="math inline">\(y\)</span>. <span class="math inline">\(SST\)</span> is a measure of total variance in <span class="math inline">\(y\)</span> and <span class="math inline">\(SSR\)</span>
is a measure of variance that cannot be explained by the model, thus <span class="math inline">\(R^2 = 1- \frac{SSR}{SST}\)</span> is a measure of variance that can be explained by the model. If <span class="math inline">\(SSR\)</span> is close to <span class="math inline">\(SST\)</span>, <span class="math inline">\(R^2\)</span> is close to zero and the model cannot explain a lot of variance. The smaller <span class="math inline">\(SSR\)</span>, the closer <span class="math inline">\(R^2\)</span> is to 1. This version of <span class="math inline">\(R2\)</span> approximates 1 if the number of model parameters approximates sample size even if none of the predictor variables correlates with the outcome. It is exactly 1 when the number of model parameters equals sample size, because <span class="math inline">\(n\)</span> measurements can be exactly described by <span class="math inline">\(n\)</span> parameters. The adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R^2 = \frac{var(y)-\hat\sigma^2}{var(y)}\)</span> takes sample size <span class="math inline">\(n\)</span> and the number of model parameters <span class="math inline">\(k\)</span> into account (see explanation to variance in chapter <a href="basics.html#basics">2</a>). Therefore, the adjusted <span class="math inline">\(R^2\)</span> is recommended as a measurement of the proportion of explained variance.</p>
</div>
</div>
<div id="linear-model-with-one-categorical-predictor-one-way-anova" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Linear model with one categorical predictor (one-way ANOVA)<a href="lm.html#linear-model-with-one-categorical-predictor-one-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The aim of analysis of variance (ANOVA) is to compare means of an outcome variable <span class="math inline">\(y\)</span> between different groups. To do so in the frequentist’s framework, variances between and within the groups are compared using F-tests (hence the name “analysis of variance”). When doing an ANOVA in a Bayesian way, inference is based on the posterior distributions of the group means and the differences between the group means.</p>
<p>One-way ANOVA means that we only have one predictor variable, specifically a categorical predictor variable (in R defined as a “factor”). We illustrate the one-way ANOVA based on an example of simulated data (Fig. <a href="lm.html#fig:figanova">11.7</a>). We have measured weights of 30 virtual individuals for each of 3 groups. Possible research questions could be: How big are the differences between the group means? Are individuals from group 2 heavier than the ones from group 1? Which group mean is higher than 7.5 g?</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="lm.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># settings for the simulation</span></span>
<span id="cb61-2"><a href="lm.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">626436</span>)</span>
<span id="cb61-3"><a href="lm.html#cb61-3" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">12</span>        <span class="co"># mean of group 1 (reference group)</span></span>
<span id="cb61-4"><a href="lm.html#cb61-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">2</span>      <span class="co"># residual standard deviation</span></span>
<span id="cb61-5"><a href="lm.html#cb61-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="dv">3</span>         <span class="co"># difference between group 1 and group 2</span></span>
<span id="cb61-6"><a href="lm.html#cb61-6" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">5</span>        <span class="co"># difference between group 1 and group 3</span></span>
<span id="cb61-7"><a href="lm.html#cb61-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">90</span>         <span class="co"># sample size</span></span>
<span id="cb61-8"><a href="lm.html#cb61-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-9"><a href="lm.html#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co"># generate data</span></span>
<span id="cb61-10"><a href="lm.html#cb61-10" aria-hidden="true" tabindex="-1"></a>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;group 1&quot;</span>,<span class="st">&quot;group 2&quot;</span>, <span class="st">&quot;group 3&quot;</span>), <span class="at">each=</span><span class="dv">30</span>))</span>
<span id="cb61-11"><a href="lm.html#cb61-11" aria-hidden="true" tabindex="-1"></a>simresid <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span>sigma)  <span class="co"># simulate residuals</span></span>
<span id="cb61-12"><a href="lm.html#cb61-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> b0 <span class="sc">+</span> </span>
<span id="cb61-13"><a href="lm.html#cb61-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(group<span class="sc">==</span><span class="st">&quot;group 2&quot;</span>) <span class="sc">*</span> b1 <span class="sc">+</span> </span>
<span id="cb61-14"><a href="lm.html#cb61-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(group<span class="sc">==</span><span class="st">&quot;group 3&quot;</span>) <span class="sc">*</span> b2 <span class="sc">+</span> </span>
<span id="cb61-15"><a href="lm.html#cb61-15" aria-hidden="true" tabindex="-1"></a>  simresid</span>
<span id="cb61-16"><a href="lm.html#cb61-16" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(y, group)</span>
<span id="cb61-17"><a href="lm.html#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="lm.html#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co"># make figure</span></span>
<span id="cb61-19"><a href="lm.html#cb61-19" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span> </span>
<span id="cb61-20"><a href="lm.html#cb61-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> group, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb61-21"><a href="lm.html#cb61-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-22"><a href="lm.html#cb61-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Weight (g)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-23"><a href="lm.html#cb61-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:figanova"></span>
<img src="2.03-lm_files/figure-html/figanova-1.png" alt="Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box." width="672" />
<p class="caption">
Figure 11.7: Weights (g) of the 30 individuals in each group. The dark horizontal line is the median, the box contains 50% of the observations (i.e., the interquartile range), the whiskers mark the range of all observations that are less than 1.5 times the interquartile range away from the edge of the box.
</p>
</div>
<p>An ANOVA is a linear regression with a categorical predictor variable instead of a continuous one. The categorical predictor variable with <span class="math inline">\(k\)</span> levels is (as a default in R) transformed to <span class="math inline">\(k-1\)</span> indicator variables. An indicator variable is a binary variable containing 0 and 1 where 1 indicates a speciﬁc level (a category of the predictor variable). Often, one indicator variable is constructed for every level except for the reference level. In our example, the categorical variable is “group” with the three levels “group 1”, “group 2”, and “group 3” (<span class="math inline">\(k = 3\)</span>). Group 1 is taken as the reference level (default in R is the first in the alphabeth), and for each of the other two groups an indicator variable is constructed, <span class="math inline">\(I(group_i = 2)\)</span> and <span class="math inline">\(I(group_i = 3)\)</span>. The function <span class="math inline">\(I()\)</span> gives out 1, if the expression is true and 0 otherwise. We can write the model as a formula:</p>
<p><span class="math display" id="eq:anova">\[\begin{align}
  \mu_i &amp;=\beta_0 + \beta_1 I(group_i=2) + \beta_1 I(group_i=3) \\
  y_i &amp;\sim normal(\mu_i, \sigma^2)
  \tag{11.5}
\end{align}\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the <span class="math inline">\(i\)</span>-th observation (weight measurement for individual <span class="math inline">\(i\)</span> in our example), and <span class="math inline">\(\beta_{0,1,2}\)</span> are the model coefﬁcients. The residual variance is <span class="math inline">\(\sigma^2\)</span>. The model coefﬁcients <span class="math inline">\(\beta_{0,1,2}\)</span> constitute the deterministic part of the model. From the model formula it follows that the group means, <span class="math inline">\(m_g\)</span>, are:</p>
<p><span class="math display" id="eq:anovamw">\[\begin{align}
  m_1 &amp;=\beta_0 \\
  m_2 &amp;=\beta_0 + \beta_1 \\
  m_3 &amp;=\beta_0 + \beta_2 \\
  \tag{11.6}
\end{align}\]</span></p>
<p>There are other possibilities to describe three group means with three parameters, for example:</p>
<p><span class="math display" id="eq:anovamwalt">\[\begin{align}
  m_1 &amp;=\beta_1 \\
  m_2 &amp;=\beta_2 \\
  m_3 &amp;=\beta_3 \\
  \tag{11.7}
\end{align}\]</span></p>
<p>In this case, the model formula would be:</p>
<p><span class="math display" id="eq:anovaalt">\[\begin{align}
  \mu_i &amp;= \beta_1 I(group_i=1) + \beta_2 I(group_i=2) + \beta_3 I(group_i=3) \\
  y_i &amp;\sim Norm(\mu_i, \sigma^2)
  \tag{11.8}
\end{align}\]</span></p>
<p>The way the group means are calculated within a model is called the parameterization of the model. Different statistical software use different parameterizations. The parameterization used by R by default is the one shown in Equation <a href="lm.html#eq:anova">(11.5)</a>. R automatically takes the ﬁrst level as the reference (the ﬁrst level is the ﬁrst one alphabetically unless the user deﬁnes a different order for the levels). The mean of the ﬁrst group (i.e., of the ﬁrst factor level) is the intercept, <span class="math inline">\(b_0\)</span> , of the model. The mean of another factor level is obtained by adding, to the intercept, the estimate of the corresponding parameter (which is the difference from the reference group mean).</p>
<p>The parameterization of the model is deﬁned by the model matrix. In the case of a one-way ANOVA, there are as many columns in the model matrix as there are factor levels (i.e., groups); thus there are k factor levels and k model coefﬁcients. Recall from Equation <a href="lm.html#eq:lmmatrix">(11.3)</a> that for each observation, the entry in the <span class="math inline">\(j\)</span>-th column of the model matrix is multiplied by the <span class="math inline">\(j\)</span>-th element of the model coefﬁcients and the <span class="math inline">\(k\)</span> products are summed to obtain the ﬁtted values. For a data set with <span class="math inline">\(n = 5\)</span> observations of which the ﬁrst two are from group 1, the third from group 2, and the last two from group 3, the model matrix used for the parameterization described in Equation <a href="lm.html#eq:anovamw">(11.6)</a> and defined in R by the formula <code>~ group</code> is</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{X}=
\begin{pmatrix}
      1 &amp; 0 &amp; 0 \\
      1 &amp; 0 &amp; 0 \\
      1 &amp; 1 &amp; 0 \\
      1 &amp; 0 &amp; 1 \\
      1 &amp; 0 &amp; 1 \\
  \end{pmatrix}
\end{align}\]</span></p>
<p>If parameterization of Equation <a href="lm.html#eq:anovamwalt">(11.7)</a> (corresponding R formula: <code>~ group - 1</code>) were used,</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{X}=
\begin{pmatrix}
      1 &amp; 0 &amp; 0 \\
      1 &amp; 0 &amp; 0 \\
      0 &amp; 1 &amp; 0 \\
      0 &amp; 0 &amp; 1 \\
      0 &amp; 0 &amp; 1 \\
  \end{pmatrix}
\end{align}\]</span></p>
<p>To obtain the parameter estimates for model parameterized according to Equation <a href="lm.html#eq:anovamw">(11.6)</a> we ﬁt the model in R:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="lm.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb62-2"><a href="lm.html#cb62-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>group, <span class="at">data=</span>dat)  </span>
<span id="cb62-3"><a href="lm.html#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="lm.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># parameter estimates </span></span>
<span id="cb62-5"><a href="lm.html#cb62-5" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ group, data = dat)
## 
## Coefficients:
##  (Intercept)  groupgroup 2  groupgroup 3  
##       12.367         2.215        -5.430</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="lm.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 1.684949</code></pre>
<p>The “Intercept” is <span class="math inline">\(\beta_0\)</span>. The other coefﬁcients are named with the factor name (“group”) and the factor level (either “group 2” or “group 3”). These are <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> , respectively. Before drawing conclusions from an R output we need to examine whether the model assumptions are met, that is, we need to do a residual analysis as described in Chapter <a href="residualanalysis.html#residualanalysis">12</a>.</p>
<p>Different questions can be answered using the above ANOVA: What are the group means? What is the difference in the means between group 1 and group 2? What is the difference between the means of the heaviest and lightest group? In a Bayesian framework we can directly assess how strongly the data support the hypothesis that the mean of the group 2 is larger than the mean of group 1. We ﬁrst simulate from the posterior distribution of the model parameters.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="lm.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb66-2"><a href="lm.html#cb66-2" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb66-3"><a href="lm.html#cb66-3" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span></code></pre></div>
<p>Then we obtain the posterior distributions for the group means according to the parameterization of the model formula (Equation <a href="lm.html#eq:anovamw">(11.6)</a>).</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="lm.html#cb67-1" aria-hidden="true" tabindex="-1"></a>m.g1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>]  </span>
<span id="cb67-2"><a href="lm.html#cb67-2" aria-hidden="true" tabindex="-1"></a>m.g2 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(bsim)[,<span class="dv">2</span>] </span>
<span id="cb67-3"><a href="lm.html#cb67-3" aria-hidden="true" tabindex="-1"></a>m.g3 <span class="ot">&lt;-</span> <span class="fu">coef</span>(bsim)[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(bsim)[,<span class="dv">3</span>] </span></code></pre></div>
<p>The histograms of the simulated values from the posterior distributions of the three means are given in Fig. <a href="lm.html#fig:figanovares">11.8</a>. The three means are well separated and, based on our data, we are conﬁdent that the group means differ. From these simulated posterior distributions we obtain the means and use the 2.5% and 97.5% quantiles as limits of the 95% uncertainty intervals (Fig. <a href="lm.html#fig:figanovares">11.8</a>, right).</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="lm.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save simulated values from posterior distribution in  tibble</span></span>
<span id="cb68-2"><a href="lm.html#cb68-2" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> </span>
<span id="cb68-3"><a href="lm.html#cb68-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="st">`</span><span class="at">group 1</span><span class="st">`</span> <span class="ot">=</span> m.g1, <span class="st">`</span><span class="at">group 2</span><span class="st">`</span> <span class="ot">=</span> m.g2, <span class="st">`</span><span class="at">group 3</span><span class="st">`</span> <span class="ot">=</span> m.g3) <span class="sc">%&gt;%</span> </span>
<span id="cb68-4"><a href="lm.html#cb68-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="st">&quot;groups&quot;</span>, <span class="st">&quot;Group means&quot;</span>) </span>
<span id="cb68-5"><a href="lm.html#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="lm.html#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="co"># histograms per group</span></span>
<span id="cb68-7"><a href="lm.html#cb68-7" aria-hidden="true" tabindex="-1"></a>leftplot <span class="ot">&lt;-</span> </span>
<span id="cb68-8"><a href="lm.html#cb68-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(post, <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">fill =</span> groups)) <span class="sc">+</span></span>
<span id="cb68-9"><a href="lm.html#cb68-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..), <span class="at">binwidth =</span> <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb68-10"><a href="lm.html#cb68-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb68-11"><a href="lm.html#cb68-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;top&quot;</span>, <span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb68-12"><a href="lm.html#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="lm.html#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="co"># plot mean and 95%-CrI</span></span>
<span id="cb68-14"><a href="lm.html#cb68-14" aria-hidden="true" tabindex="-1"></a>rightplot <span class="ot">&lt;-</span> </span>
<span id="cb68-15"><a href="lm.html#cb68-15" aria-hidden="true" tabindex="-1"></a>  post <span class="sc">%&gt;%</span> </span>
<span id="cb68-16"><a href="lm.html#cb68-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(groups) <span class="sc">%&gt;%</span> </span>
<span id="cb68-17"><a href="lm.html#cb68-17" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(</span>
<span id="cb68-18"><a href="lm.html#cb68-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">mean</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>),</span>
<span id="cb68-19"><a href="lm.html#cb68-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">CrI_lo =</span> <span class="fu">quantile</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">probs =</span> <span class="fl">0.025</span>),</span>
<span id="cb68-20"><a href="lm.html#cb68-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">CrI_up =</span> <span class="fu">quantile</span>(<span class="st">`</span><span class="at">Group means</span><span class="st">`</span>, <span class="at">probs =</span> <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb68-21"><a href="lm.html#cb68-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> groups, <span class="at">y =</span> mean)) <span class="sc">+</span></span>
<span id="cb68-22"><a href="lm.html#cb68-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb68-23"><a href="lm.html#cb68-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> CrI_lo, <span class="at">ymax =</span> CrI_up), <span class="at">width =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb68-24"><a href="lm.html#cb68-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>) <span class="sc">+</span></span>
<span id="cb68-25"><a href="lm.html#cb68-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Weight (g)&quot;</span>, <span class="at">x =</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb68-26"><a href="lm.html#cb68-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-27"><a href="lm.html#cb68-27" aria-hidden="true" tabindex="-1"></a><span class="fu">multiplot</span>(leftplot, rightplot, <span class="at">cols =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:figanovares"></span>
<img src="2.03-lm_files/figure-html/figanovares-1.png" alt="Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% uncertainty intervals obtained from the simulated distributions (right)." width="672" />
<p class="caption">
Figure 11.8: Distribution of the simulated values from the posterior distributions of the group means (left); group means with 95% uncertainty intervals obtained from the simulated distributions (right).
</p>
</div>
<p>To obtain the posterior distribution of the difference between the means of group 1 and group 2, we simply calculate this difference for each draw from the joint posterior distribution of the group means.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="lm.html#cb69-1" aria-hidden="true" tabindex="-1"></a>d.g1<span class="fl">.2</span> <span class="ot">&lt;-</span> m.g1 <span class="sc">-</span> m.g2</span>
<span id="cb69-2"><a href="lm.html#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(d.g1<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## [1] -2.209551</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="lm.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(d.g1<span class="fl">.2</span>, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## -3.128721 -1.342693</code></pre>
<p>The estimated difference is -2.2095511. In the small model world, we are 95% sure that the difference between the means of group 1 and 2 is between -3.1287208 and -1.3426929.</p>
<p>How strongly do the data support the hypothesis that the mean of group 2 is larger than the mean of group 1? To answer this question we calculate the proportion of the draws from the joint posterior distribution for which the mean of group 2 is larger than the mean of group 1.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="lm.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(m.g2 <span class="sc">&gt;</span> m.g1) <span class="sc">/</span> nsim </span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>This means that in all of the 1000 simulations from the joint posterior distribution, the mean of group 2 was larger than the mean of group 1. Therefore, there is a very high probability (i.e., it is close to 1; because probabilities are never exactly 1, we write &gt;0.999) that the mean of group 2 is larger than the mean of group 1.</p>
</div>
<div id="other-variants-of-normal-linear-models-two-way-anova-analysis-of-covariance-and-multiple-regression" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Other variants of normal linear models: Two-way anova, analysis of covariance and multiple regression<a href="lm.html#other-variants-of-normal-linear-models-two-way-anova-analysis-of-covariance-and-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Up to now, we introduced normal linear models with one predictor only. We can add more predictors to the model and these can be numerical or categorical ones. Traditionally, models with 2 or 3 categorical predictors are called two-way or three-way ANOVA, respectively. Models with a mixture of categorical and numerical predictors are called ANCOVA. And, models containing only numerical predictors are called multiple regressions. Nowadays, we only use the term “normal linear model” as an umbrella term for all these types of models.
While it is easy to add additional predictors in the R formula of the model, it becomes more difficult to interpret the coefficients of such multi-dimensional models. Two important topics arise with multi-dimensional models, <em>interactions</em> and <em>partial effects</em>. We dedicate partial effects the full next chapter and introduce interactions in this chapter using two examples. The first, is a model including two categorical predictors and the second is a model with one categorical and one numeric predictor.</p>
<div id="linear-model-with-two-categorical-predictors-two-way-anova" class="section level3 hasAnchor" number="11.3.1">
<h3><span class="header-section-number">11.3.1</span> Linear model with two categorical predictors (two-way ANOVA)<a href="lm.html#linear-model-with-two-categorical-predictors-two-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the first example, we ask how large are the differences in wing length between age and sex classes of the Coal tit <em>Periparus ater</em>. Wing lengths were measured on 19 coal tit museum skins with known sex and age class.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="lm.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(periparusater)</span>
<span id="cb75-2"><a href="lm.html#cb75-2" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(periparusater) <span class="co"># give the data a short handy name</span></span>
<span id="cb75-3"><a href="lm.html#cb75-3" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>age <span class="ot">&lt;-</span> <span class="fu">recode_factor</span>(dat<span class="sc">$</span>age, <span class="st">&quot;4&quot;</span><span class="ot">=</span><span class="st">&quot;adult&quot;</span>, <span class="st">&quot;3&quot;</span><span class="ot">=</span><span class="st">&quot;juvenile&quot;</span>) <span class="co"># replace EURING code</span></span>
<span id="cb75-4"><a href="lm.html#cb75-4" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>sex <span class="ot">&lt;-</span> <span class="fu">recode_factor</span>(dat<span class="sc">$</span>sex, <span class="st">&quot;2&quot;</span><span class="ot">=</span><span class="st">&quot;female&quot;</span>, <span class="st">&quot;1&quot;</span><span class="ot">=</span><span class="st">&quot;male&quot;</span>)    <span class="co"># replace EURING code</span></span></code></pre></div>
<p>To describe differences in wing length between the age classes or between the sexes a normal linear model with two categorical predictors is fitted to the data. The two predictors are specified on the right side of the model formula separated by the “+” sign, which means that the model is an additive combination of the two effects (as opposed to an interaction, see following).</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="lm.html#cb76-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(wing <span class="sc">~</span> sex <span class="sc">+</span> age, <span class="at">data=</span>dat)</span></code></pre></div>
<p>After having seen that the residual distribution does not appear to violate the model assumptions (as assessed with diagnostic residual plots, see Chapter <a href="residualanalysis.html#residualanalysis">12</a>), we can draw inferences. We first have a look at the model parameter estimates:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="lm.html#cb77-1" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wing ~ sex + age, data = dat)
## 
## Coefficients:
## (Intercept)      sexmale  agejuvenile  
##     61.3784       3.3423      -0.8829</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="lm.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 2.134682</code></pre>
<p>R has taken the first level of the factors age and sex (as defined in the data.frame dat) as the reference levels. The intercept is the expected wing length for individuals having the reference level in age and sex, thus adult female. The other two parameters provide estimates of what is to be added to the intercept to get the expected wing length for the other levels. The parameter <code>sexmale</code> is the average difference between females and males. We can conclude that in males have in average a 3.3 mm longer wing than females. Similarly, the parameter <code>agejuvenile</code> measures the differences between the age classes and we can conclude that, in average, juveniles have a 0.9 shorter wing than adults. When we insert the parameter estimates into the model formula, we get the receipt to calculate expected values for each age and sex combination:</p>
<p><span class="math inline">\(\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}I(sex=male) + \hat{\beta_2}I(age=juvenile)\)</span> which yields<br />
<span class="math inline">\(\hat{y_i}\)</span> = 61.4 <span class="math inline">\(+\)</span> 3.3 <span class="math inline">\(I(sex=male) +\)</span> -0.9 <span class="math inline">\(I(age=juvenile)\)</span>.</p>
<p>Alternatively, we could use matrix notation. We construct a new data set that contains one virtual individual for each age and sex class.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="lm.html#cb81-1" aria-hidden="true" tabindex="-1"></a>newdat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="fu">expand.grid</span>(<span class="at">sex=</span><span class="fu">factor</span>(<span class="fu">levels</span>(dat<span class="sc">$</span>sex)),</span>
<span id="cb81-2"><a href="lm.html#cb81-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">age=</span><span class="fu">factor</span>(<span class="fu">levels</span>(dat<span class="sc">$</span>age))))</span>
<span id="cb81-3"><a href="lm.html#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="co"># expand.grid creates a data frame with all combination of values given</span></span>
<span id="cb81-4"><a href="lm.html#cb81-4" aria-hidden="true" tabindex="-1"></a>newdat</span></code></pre></div>
<pre><code>## # A tibble: 4 × 2
##   sex    age     
##   &lt;fct&gt;  &lt;fct&gt;   
## 1 female adult   
## 2 male   adult   
## 3 female juvenile
## 4 male   juvenile</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="lm.html#cb83-1" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod, <span class="at">newdata=</span>newdat)  <span class="co"># fast way of getting fitted values</span></span>
<span id="cb83-2"><a href="lm.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="co"># or</span></span>
<span id="cb83-3"><a href="lm.html#cb83-3" aria-hidden="true" tabindex="-1"></a>Xmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>sex<span class="sc">+</span>age, <span class="at">data=</span>newdat) <span class="co"># creates a model matrix</span></span>
<span id="cb83-4"><a href="lm.html#cb83-4" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>fit <span class="ot">&lt;-</span> Xmat <span class="sc">%*%</span> <span class="fu">coef</span>(mod)</span></code></pre></div>
<p>For this new data set the model matrix contains four rows (one for each combination of age class and sex) and three columns. The first column contains only ones because the values of this column are multiplied by the intercept (<span class="math inline">\(\beta_0\)</span>) in the matrix multiplication. The second column contains an indicator variable for males (so only the rows corresponding to males contain a one) and the third column has ones for juveniles.</p>
<p><span class="math display" id="eq:lmmatrix">\[\begin{align}
\hat{y} =
\boldsymbol{X \hat{\beta}} =
\begin{pmatrix}
      1 &amp; 0 &amp; 0 \\
      1 &amp; 1 &amp; 0 \\
      1 &amp; 0 &amp; 1 \\
      1 &amp; 1 &amp; 1 \\
  \end{pmatrix}
  \times
  \begin{pmatrix}
    61.4 \\
    3.3 \\
    -0.9
\end{pmatrix} =
  \begin{pmatrix}
    61.4 \\
    64.7 \\
    60.5 \\
    63.8
\end{pmatrix} =
\boldsymbol{\mu}
\tag{11.3}
\end{align}\]</span></p>
<p>The result of the matrix multiplication is a vector containing the expected wing length for adult and juvenile females and adult and juvenile males.</p>
<p>When creating the model matrix with <code>model.matrix</code> care has to be taken that the columns in the model matrix match the parameters in the vector of model coefficients. To achieve that, it is required that the model formula is identical to the model formula of the model (same order of terms!), and that the factors in newdat are identical in their levels and their order as in the data the model was fitted to.</p>
<p>To describe the uncertainty of the fitted values, we use 2000 sets of parameter values of the joint posterior distribution to obtain 2000 values for each of the four fitted values. These are stored in the object “fitmat”. In the end, we extract for every fitted value, i.e., for every row in fitmat, the 2.5% and 97.5% quantiles as the lower and upper limits of the 95% uncertainty interval.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="lm.html#cb84-1" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb84-2"><a href="lm.html#cb84-2" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">n.sim=</span>nsim)</span>
<span id="cb84-3"><a href="lm.html#cb84-3" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat))</span>
<span id="cb84-4"><a href="lm.html#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) fitmat[,i] <span class="ot">&lt;-</span> Xmat <span class="sc">%*%</span> <span class="fu">coef</span>(bsim)[i,] </span>
<span id="cb84-5"><a href="lm.html#cb84-5" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>lwr <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs=</span><span class="fl">0.025</span>)</span>
<span id="cb84-6"><a href="lm.html#cb84-6" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>upr <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs=</span><span class="fl">0.975</span>)</span></code></pre></div>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="lm.html#cb85-1" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>sexage <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">paste</span>(dat<span class="sc">$</span>sex, dat<span class="sc">$</span>age))</span>
<span id="cb85-2"><a href="lm.html#cb85-2" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>sexage <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">paste</span>(newdat<span class="sc">$</span>sex, newdat<span class="sc">$</span>age))</span>
<span id="cb85-3"><a href="lm.html#cb85-3" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>pch <span class="ot">&lt;-</span> <span class="dv">21</span></span>
<span id="cb85-4"><a href="lm.html#cb85-4" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>pch[dat<span class="sc">$</span>sex<span class="sc">==</span><span class="st">&quot;male&quot;</span>] <span class="ot">&lt;-</span> <span class="dv">22</span></span>
<span id="cb85-5"><a href="lm.html#cb85-5" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>col<span class="ot">=</span><span class="st">&quot;blue&quot;</span></span>
<span id="cb85-6"><a href="lm.html#cb85-6" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>col[dat<span class="sc">$</span>age<span class="sc">==</span><span class="st">&quot;adult&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;orange&quot;</span></span>
<span id="cb85-7"><a href="lm.html#cb85-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-8"><a href="lm.html#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>))</span>
<span id="cb85-9"><a href="lm.html#cb85-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wing<span class="sc">~</span><span class="fu">jitter</span>(<span class="fu">as.numeric</span>(sexage), <span class="at">amount=</span><span class="fl">0.05</span>), <span class="at">data=</span>dat, <span class="at">las=</span><span class="dv">1</span>, </span>
<span id="cb85-10"><a href="lm.html#cb85-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Wing length (mm)&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Sex and age&quot;</span>, <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">pch=</span>dat<span class="sc">$</span>pch, </span>
<span id="cb85-11"><a href="lm.html#cb85-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">bg=</span>dat<span class="sc">$</span>col, <span class="at">cex.lab=</span><span class="fl">1.2</span>, <span class="at">cex=</span><span class="dv">1</span>, <span class="at">cex.axis=</span><span class="dv">1</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">4.5</span>))</span>
<span id="cb85-12"><a href="lm.html#cb85-12" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>), <span class="at">labels=</span><span class="fu">levels</span>(dat<span class="sc">$</span>sexage), <span class="at">cex.axis=</span><span class="dv">1</span>)</span>
<span id="cb85-13"><a href="lm.html#cb85-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-14"><a href="lm.html#cb85-14" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="fu">as.numeric</span>(newdat<span class="sc">$</span>sexage), newdat<span class="sc">$</span>lwr, <span class="fu">as.numeric</span>(newdat<span class="sc">$</span>sexage), newdat<span class="sc">$</span>upr, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb85-15"><a href="lm.html#cb85-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">lend=</span><span class="st">&quot;butt&quot;</span>)</span>
<span id="cb85-16"><a href="lm.html#cb85-16" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">as.numeric</span>(newdat<span class="sc">$</span>sexage), newdat<span class="sc">$</span>fit, <span class="at">pch=</span><span class="dv">17</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:fgwingpa"></span>
<img src="2.03-lm_files/figure-html/fgwingpa-1.png" alt="Wing length measurements on 19 museumm skins of coal tits per age class and sex. Fitted values are from the additive model (black triangles) and from the model including an interaction (black dots). Vertical bars = 95% uncertainty intervals." width="672" />
<p class="caption">
Figure 11.9: Wing length measurements on 19 museumm skins of coal tits per age class and sex. Fitted values are from the additive model (black triangles) and from the model including an interaction (black dots). Vertical bars = 95% uncertainty intervals.
</p>
</div>
<p>We can see that the fitted values are not equal to the arithmetic means of the groups; this is especially clear for juvenile males. The fitted values are constrained because only three parameters were used to estimate four means. In other words, this model assumes that the age difference is equal in both sexes and, vice versa, that the difference between the sexes does not change with age. If the effect of sex changes with age, we would include an <em>interaction</em> between sex and age in the model. Including an interaction adds a fourth parameter enabling us to estimate the group means exactly. In R, an interaction is indicated with the <code>:</code> sign.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="lm.html#cb86-1" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(wing <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> sex<span class="sc">:</span>age, <span class="at">data=</span>dat)</span>
<span id="cb86-2"><a href="lm.html#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="lm.html#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="co"># alternative formulations of the same model:</span></span>
<span id="cb86-4"><a href="lm.html#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="co"># mod2 &lt;- lm(wing ~ sex * age, data=dat)</span></span>
<span id="cb86-5"><a href="lm.html#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="co"># mod2 &lt;- lm(wing ~ (sex + age)^2, data=dat)</span></span></code></pre></div>
<p>The formula for this model is <span class="math inline">\(\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}I(sex=male) + \hat{\beta_2}I(age=juvenile) + \hat{\beta_3}I(age=juvenile)I(sex=male)\)</span>. From this formula we get the following expected values for the sexes and age classes:</p>
<p>for adult females: <span class="math inline">\(\hat{y} = \beta_0\)</span><br />
for adult males: <span class="math inline">\(\hat{y} = \beta_0 + \beta_1\)</span><br />
for juveniles females: <span class="math inline">\(\hat{y} = \beta_0 + \beta_2\)</span><br />
for juveniles males: <span class="math inline">\(\hat{y} = \beta_0 + \beta_1 + \beta_2 + \beta_3\)</span></p>
<p>The interaction parameter measures how much different between age classes is the difference between the sexes.</p>
<p>To obtain the fitted values the R-code above can be recycled with two adaptations. First, the model name needs to be changed to “mod2”. Second, importantly, the model matrix needs to be adapted to the new model formula.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="lm.html#cb87-1" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata=</span>newdat)</span>
<span id="cb87-2"><a href="lm.html#cb87-2" aria-hidden="true" tabindex="-1"></a>bsim <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod2, <span class="at">n.sim=</span>nsim)</span>
<span id="cb87-3"><a href="lm.html#cb87-3" aria-hidden="true" tabindex="-1"></a>Xmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> sex<span class="sc">:</span>age, <span class="at">data=</span>newdat)</span>
<span id="cb87-4"><a href="lm.html#cb87-4" aria-hidden="true" tabindex="-1"></a>fitmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">ncol=</span>nsim, <span class="at">nrow=</span><span class="fu">nrow</span>(newdat))</span>
<span id="cb87-5"><a href="lm.html#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim) fitmat[,i] <span class="ot">&lt;-</span> Xmat <span class="sc">%*%</span> <span class="fu">coef</span>(bsim)[i,] </span>
<span id="cb87-6"><a href="lm.html#cb87-6" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>lwr2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs=</span><span class="fl">0.025</span>)</span>
<span id="cb87-7"><a href="lm.html#cb87-7" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>upr2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(fitmat, <span class="dv">1</span>, quantile, <span class="at">probs=</span><span class="fl">0.975</span>)</span>
<span id="cb87-8"><a href="lm.html#cb87-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(newdat[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,<span class="dv">7</span><span class="sc">:</span><span class="dv">9</span>)], <span class="at">digits=</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 8
##   sex    age      fit[,1]   lwr   upr  fit2  lwr2  upr2
##   &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 female adult       61.4  59.3  63.3  61.1  58.8  63.5
## 2 male   adult       64.7  63.3  66.2  64.8  63.3  66.4
## 3 female juvenile    60.5  58.4  62.6  60.8  58.2  63.4
## 4 male   juvenile    63.8  61.7  66.0  63.5  60.7  66.2</code></pre>
<p>These fitted values are now exactly equal to the arithmetic means of each groups.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="lm.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tapply</span>(dat<span class="sc">$</span>wing, <span class="fu">list</span>(dat<span class="sc">$</span>age, dat<span class="sc">$</span>sex), mean) <span class="co"># arithmetic mean per group</span></span></code></pre></div>
<pre><code>##            female     male
## adult    61.12500 64.83333
## juvenile 60.83333 63.50000</code></pre>
<p>We can also see that the uncertainty of the fitted values is larger for the model with an interaction than for the additive model. This is because, in the model including the interaction, an additional parameter has to be estimated based on the same amount of data. Therefore, the information available per parameter is smaller than in the additive model. In the additive model, some information is pooled between the groups by making the assumption that the difference between the sexes does not depend on age.</p>
<p>The degree to which a difference in wing length is ‘important’ depends on the context of the study. Here, for example, we could consider effects of wing length on flight energetics and maneuverability or methodological aspects like measurement error. Mean between-observer difference in wing length measurement is around 0.3 mm <span class="citation">(<a href="referenzen.html#ref-Jenni.1989" role="doc-biblioref">Jenni and Winkler 1989</a>)</span>. Therefore, we may consider that the interaction is important because its point estimate is larger than 0.3 mm.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="lm.html#cb91-1" aria-hidden="true" tabindex="-1"></a>mod2</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wing ~ sex + age + sex:age, data = dat)
## 
## Coefficients:
##         (Intercept)              sexmale          agejuvenile  
##             61.1250               3.7083              -0.2917  
## sexmale:agejuvenile  
##             -1.0417</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="lm.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 2.18867</code></pre>
<p>Further, we think a difference of 1 mm in wing length may be relevant compared to the among-individual variation of which the standard deviation is around 2 mm. Therefore, we report the parameter estimates of the model including the interaction together with their uncertainty intervals.</p>
<table>
<caption><span id="tab:sumtabpa">Table 11.1: </span>Parameter estimates of the model for wing length of Coal tits with 95% uncertainty interval.</caption>
<thead>
<tr class="header">
<th align="left">Parameter</th>
<th align="right">Estimate</th>
<th align="right">lwr</th>
<th align="right">upr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">61.12</td>
<td align="right">58.85</td>
<td align="right">63.53</td>
</tr>
<tr class="even">
<td align="left">sexmale</td>
<td align="right">3.71</td>
<td align="right">0.93</td>
<td align="right">6.59</td>
</tr>
<tr class="odd">
<td align="left">agejuvenile</td>
<td align="right">-0.29</td>
<td align="right">-3.93</td>
<td align="right">3.36</td>
</tr>
<tr class="even">
<td align="left">sexmale:agejuvenile</td>
<td align="right">-1.04</td>
<td align="right">-5.96</td>
<td align="right">3.90</td>
</tr>
</tbody>
</table>
<p>From these parameters we obtain the estimated differences in wing length between the sexes for adults of 3.7mm and the posterior probability of the hypotheses that males have an average wing length that is at least 1mm larger compared to females is <code>mean(bsim@coef[,2]&gt;1)</code> which is 0.97. Thus, there is some evidence that adult Coal tit males have substantially larger wings than adult females in these data. However, we do not draw further conclusions on other differences from these data because statistical uncertainty is large due to the low sample size.</p>
</div>
<div id="a-linear-model-with-a-categorical-and-a-numeric-predictor-ancova" class="section level3 hasAnchor" number="11.3.2">
<h3><span class="header-section-number">11.3.2</span> A linear model with a categorical and a numeric predictor (ANCOVA)<a href="lm.html#a-linear-model-with-a-categorical-and-a-numeric-predictor-ancova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An analysis of covariance, ANCOVA, is a normal linear model that contains at least one factor and one continuous variable as predictor variables. The continuous variable is also called a covariate, hence the name analysis of covariance. An ANCOVA can be used, for example, when we are interested in how the biomass of grass depends on the distance from the surface of the soil to the ground water in two different species (<em>Alopecurus pratensis</em>, <em>Dactylis glomerata</em>). The two species were grown by <span class="citation">Ellenberg (<a href="referenzen.html#ref-Ellenberg1953" role="doc-biblioref">1953</a>)</span> in tanks that showed a gradient in distance from the soil surface to the ground water. The distance from the soil surface to the ground water is used as a covariate (‘water’). We further assume that the species react differently to the water conditions. Therefore, we include an interaction between species and water. The model formula is then
<span class="math inline">\(\hat{y_i} = \beta_0 + \beta_1I(species=Dg) + \beta_2water_i + \beta_3I(species=Dg)water_i\)</span><br />
<span class="math inline">\(y_i \sim normal(\hat{y_i}, \sigma^2)\)</span></p>
<p>To fit the model, it is important to first check whether the factor is indeed defined as a factor and the continuous variable contains numbers (i.e., numeric or integer values) in the data frame.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="lm.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ellenberg)</span>
<span id="cb95-2"><a href="lm.html#cb95-2" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">is.element</span>(ellenberg<span class="sc">$</span>Species, <span class="fu">c</span>(<span class="st">&quot;Ap&quot;</span>, <span class="st">&quot;Dg&quot;</span>)) <span class="sc">&amp;</span> <span class="fu">complete.cases</span>(ellenberg<span class="sc">$</span>Yi.g)</span>
<span id="cb95-3"><a href="lm.html#cb95-3" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> ellenberg[index,<span class="fu">c</span>(<span class="st">&quot;Water&quot;</span>, <span class="st">&quot;Species&quot;</span>, <span class="st">&quot;Yi.g&quot;</span>)] <span class="co"># select two species</span></span>
<span id="cb95-4"><a href="lm.html#cb95-4" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">droplevels</span>(dat)</span>
<span id="cb95-5"><a href="lm.html#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(dat)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    84 obs. of  3 variables:
##  $ Water  : int  5 20 35 50 65 80 95 110 125 140 ...
##  $ Species: Factor w/ 2 levels &quot;Ap&quot;,&quot;Dg&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Yi.g   : num  34.8 28 44.5 24.8 37.5 ...</code></pre>
<p>Species is a factor with two levels and Water is an integer variable, so we are fine and we can fit the model</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="lm.html#cb97-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(Yi.g) <span class="sc">~</span> Species <span class="sc">+</span> Water <span class="sc">+</span> Species<span class="sc">:</span>Water, <span class="at">data=</span>dat)</span>
<span id="cb97-2"><a href="lm.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(mod)  # 4 standard residual plots</span></span></code></pre></div>
<p>We log-transform the biomass to make the residuals closer to normally distributed. So, the normal distribution assumption is met well. However, a slight banana shaped relationship exists between the residuals and the fitted values indicating a slight non-linear relationship between biomass and water. Further, residuals showed substantial autocorrelation because the grass biomass was measured in different tanks. Measurements from the same tank were more similar than measurements from different tanks after correcting for the distance to water. Thus, the analysis we have done here suffers from pseudoreplication. We will re-analyze the example data in a more appropriate way in Chapter <a href="lmer.html#lmer">13</a>.</p>
<p>Let’s have a look at the model matrix (first and last six rows only).</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="lm.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">model.matrix</span>(mod))  <span class="co"># print the first 6 rows of the matrix</span></span></code></pre></div>
<pre><code>##    (Intercept) SpeciesDg Water SpeciesDg:Water
## 24           1         0     5               0
## 25           1         0    20               0
## 26           1         0    35               0
## 27           1         0    50               0
## 28           1         0    65               0
## 29           1         0    80               0</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="lm.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(<span class="fu">model.matrix</span>(mod))  <span class="co"># print the last 6 rows of the matrix</span></span></code></pre></div>
<pre><code>##     (Intercept) SpeciesDg Water SpeciesDg:Water
## 193           1         1    65              65
## 194           1         1    80              80
## 195           1         1    95              95
## 196           1         1   110             110
## 197           1         1   125             125
## 198           1         1   140             140</code></pre>
<p>The first column of the model matrix contains only 1s. These are multiplied by the intercept in the matrix multiplication that yields the fitted values. The second column contains the indicator variable for species <em>Dactylis glomerata</em> (Dg). Species <em>Alopecurus pratensis</em> (Ap) is the reference level. The third column contains the values for the covariate. The last column contains the product of the indicator for species Dg and water. This column specifies the interaction between species and water.</p>
<p>The parameters are the intercept, the difference between the species, a slope for water and the interaction parameter.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="lm.html#cb102-1" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(Yi.g) ~ Species + Water + Species:Water, data = dat)
## 
## Coefficients:
##     (Intercept)        SpeciesDg            Water  SpeciesDg:Water  
##         4.33041         -0.23700         -0.01791          0.01894</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="lm.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 0.9001547</code></pre>
<p>These four parameters define two regression lines, one for each species (Figure <a href="lm.html#fig:fgbiom">11.10</a> Left). For Ap, it is <span class="math inline">\(\hat{y_i} = \beta_0 + \beta_2water_i\)</span>, and for Dg it is <span class="math inline">\(\hat{y_i} = (\beta_0 + \beta_1) + (\beta_2 + \beta_3)water_i\)</span>. Thus, <span class="math inline">\(\beta_1\)</span> is the difference in the intercept between the species and <span class="math inline">\(\beta_3\)</span> is the difference in the slope.</p>
<div class="figure"><span style="display:block;" id="fig:fgbiom"></span>
<img src="2.03-lm_files/figure-html/fgbiom-1.png" alt="Aboveground biomass (g, log-transformed) in relation to distance to ground water and species (two grass species). Fitted values from a model including an interaction species x water (left) and a model without interaction (right) are added. The dotted line indicates water=0." width="672" />
<p class="caption">
Figure 11.10: Aboveground biomass (g, log-transformed) in relation to distance to ground water and species (two grass species). Fitted values from a model including an interaction species x water (left) and a model without interaction (right) are added. The dotted line indicates water=0.
</p>
</div>
<p>As a consequence of including an interaction in the model, the interpretation of the main effects become difficult. From the above model output, we read that the intercept of the species Dg is lower than the intercept of the species Ap. However, from a graphical inspection of the data, we would expect that the average biomass of species Dg is higher than the one of species Ap. The estimated main effect of species is counter-intuitive because it is measured where water is zero (i.e, it is the difference in the intercepts and not between the mean biomasses of the species). Therefore, the main effect of species in the above model does not have a biologically meaningful interpretation. We have two possibilities to get a meaningful species effect. First, we could delete the interaction from the model (Figure <a href="lm.html#fig:fgbiom">11.10</a> Right). Then the difference in the intercept reflects an average difference between the species. However, the fit for such an additive model is much worth compared to the model with interaction, and an average difference between the species may not make much sense because this difference so much depends on water. Therefore, we prefer to use a model including the interaction and may opt for th second possibility. Second, we could move the location where water equals 0 to the center of the data by transforming, specifically centering, the variable water: <span class="math inline">\(water.c = water - mean(water)\)</span>. When the predictor variable (water) is centered, then the intercept corresponds to the difference in fitted values measured in the center of the data.</p>
<p>For drawing biological conclusions from these data, we refer to Chapter <a href="lmer.html#lmer">13</a>, where we use a more appropriate model.</p>
</div>
</div>
<div id="partial-coefficients-and-some-comments-on-collinearity" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Partial coefficients and some comments on collinearity<a href="lm.html#partial-coefficients-and-some-comments-on-collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many biologists think that it is forbidden to include correlated predictor variables in a model. They use variance inflating factors (VIF) to omit some of the variables. However, omitting important variables from the model just because a correlation coefficient exceeds a threshold value can have undesirable effects. Here, we explain why and we present the usefulness and limits of partial coefficients (also called partial correlation or partial effects). We start with an example illustrating the usefulness of partial coefficients and then, give some guidelines on how to deal with collinearity.</p>
<p>As an example, we look at hatching dates of Snowfinches and how these dates relate to the date when snow melt started (first date in the season when a minimum of 5% ground is snow free). A thorough analyses of the data is presented by <span class="citation">Schano et al. (<a href="referenzen.html#ref-Schano.2021" role="doc-biblioref">2021</a>)</span>. An important question is how well can Snowfinches adjust their hatching dates to the snow conditions. For Snowfinches, it is important to raise their nestlings during snow melt. Their nestlings grow faster when they are reared during the snow melt compared to after snow has completely melted, because their parents find nutrient rich insect larvae in the edges of melting snow patches.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="lm.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;RData/snowfinch_hatching_date.rda&quot;</span>)</span>
<span id="cb106-2"><a href="lm.html#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="lm.html#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pearson&#39;s correlation coefficient</span></span>
<span id="cb106-4"><a href="lm.html#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(datsf<span class="sc">$</span>elevation, datsf<span class="sc">$</span>meltstart, <span class="at">use =</span> <span class="st">&quot;pairwise.complete&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.3274635</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="lm.html#cb108-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(meltstart<span class="sc">~</span>elevation, <span class="at">data=</span>datsf)</span>
<span id="cb108-2"><a href="lm.html#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="dv">100</span><span class="sc">*</span><span class="fu">coef</span>(mod)[<span class="dv">2</span>] <span class="co"># change in meltstart with 100m change in elevation</span></span></code></pre></div>
<pre><code>## elevation 
##   2.97768</code></pre>
<p>Hatching dates of Snowfinch broods were inferred from citizen science data from the Alps, where snow melt starts later at higher elevations compared to lower elevations. Thus, the start of snow melt is correlated with elevation (Pearson’s correlation coefficient 0.33). In average, snow starts melting 3 days later with every 100m increase in elevation.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="lm.html#cb110-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(hatchday.mean<span class="sc">~</span>meltstart, <span class="at">data=</span>datsf)</span>
<span id="cb110-2"><a href="lm.html#cb110-2" aria-hidden="true" tabindex="-1"></a>mod1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hatchday.mean ~ meltstart, data = datsf)
## 
## Coefficients:
## (Intercept)    meltstart  
##   167.99457      0.06325</code></pre>
<p>From a a normal linear regression of hatching date on the snow melt date, we obtain an estimate of 0.06 days delay in hatching date with one day later snow melt. This effect sizes describes the relationship in the data that were collected along an elevational gradient. Along the elevational gradient there are many factors that change such as average temperature, air pressure or sun radiation. All these factors may have an influence on the birds decision to start breeding. Consequentily, from the raw correlation between hatching dates and start of snow melt we cannot conclude how Snowfinches react to changes in the start of snow melt because the correlation seen in the data may be caused by other factors changing with elevation (such a correlation is called “pseudocorrelation”). However, we are interested in the correlation between hatching date and date of snow melt independent of other factors changing with elevation. In other words, we would like to measure how much in average hatching date delays when snow melt starts one day later while all other factors are kept constant. This is called the partial effect of snow melt date. Therefore, we include elevation as a covariate in the model.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="lm.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arm)</span>
<span id="cb112-2"><a href="lm.html#cb112-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(hatchday.mean<span class="sc">~</span>elevation <span class="sc">+</span> meltstart, <span class="at">data=</span>datsf)</span>
<span id="cb112-3"><a href="lm.html#cb112-3" aria-hidden="true" tabindex="-1"></a>mod</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hatchday.mean ~ elevation + meltstart, data = datsf)
## 
## Coefficients:
## (Intercept)    elevation    meltstart  
##  154.383936     0.007079     0.037757</code></pre>
<p>From this model, we obtain an estimate of 0.04 days delay in hatching date with one day later snow melt at a given elevation. That gives a difference in hatching date between early and late years (around one month difference in snow melt date) at a given elevation of 1.13 days (Figure <a href="lm.html#fig:sfexfig">11.11</a>). We further get an estimate of 0.71 days later hatching date for each 100m shift in elevation. Thus, a 18.75 days later snow melt corresponds to a similar delay in average hatching date when elevation increases by 100m.</p>
<p>When we estimate the coefficient within a constant elevation (coloured regression lines in Figure <a href="lm.html#fig:sfexfig">11.11</a>), it is lower than the raw correlation and closer to a causal relationship, because it is corrected for elevation. However, in observational studies, we never can be sure whether the partial coefficients can be interpreted as a causal relationship unless we include all factors that influence hatching date. Nevertheless, partial effects give much more insight into a system compared to univariate analyses because we can separated effects of simultaneously acting variables (that we have measured). The result indicates that Snowfinches may not react very sensibly to varying timing of snow melt, whereas at higher elevations they clearly breed later compared to lower elevations.</p>
<div class="figure"><span style="display:block;" id="fig:sfexfig"></span>
<img src="2.03-lm_files/figure-html/sfexfig-1.png" alt="Illustration of the partial coefficient of snow melt date in a  model of hatching date. Panel A shows the entire raw data together with the regression lines drawn for three different elevations. The regression lines span the range of snow melt dates occurring at the respective elevation (shown in panel C). Panel B is the same as panel A, but zoomed in to the better see the regression lines and with an additional regression line (in black) from the model that does not take elevation into account." width="672" />
<p class="caption">
Figure 11.11: Illustration of the partial coefficient of snow melt date in a model of hatching date. Panel A shows the entire raw data together with the regression lines drawn for three different elevations. The regression lines span the range of snow melt dates occurring at the respective elevation (shown in panel C). Panel B is the same as panel A, but zoomed in to the better see the regression lines and with an additional regression line (in black) from the model that does not take elevation into account.
</p>
</div>
<p>We have seen that it can be very useful to include more than one predictor variable in a model even if they are correlated with each other. In fact, there is nothing wrong with that. However, correlated predictors (collinearity) make things more complicated.</p>
<p>For example, partial regression lines should not be drawn across the whole range of values of a variable, to avoid extrapolating out of data. At 2800 m asl snow melt never starts in the beginning of March. Therefore, the blue regression line would not make sense for snow melt dates in March.</p>
<p>Further, sometimes correlations among predictors indicate that these predictors measure the same underlying aspect and we are actually interested in the effect of this underlying aspect on our response. For example, we could include also the date of the end of snow melt. Both variables, the start and the end of the snow melt measure the timing of snow melt. Including both as predictor in the model would result in partial coefficients that measure how much hatching date changes when the snow melt starts one day later, while the end date is constant. That interpretation is a mixture of the effect of timing and duration rather than of snow melt timing alone. Similarly, the coefficient of the end of snow melt measures a mixture of duration and timing. Thus, if we include two variables that are correlated because they measure the same aspect (just a little bit differently), we get coefficients that are hard to interpret and may not measure what we actually are interested in. In such a cases, we get easier to interpret model coefficients, if we include just one variable of each aspect that we are interested in, e.g. we could include one timing variable (e.g. start of snow melt) and the duration of snow melt that may or may not be correlated with the start of snow melt.<br />
To summarize, the decision of what to do with correlated predictors primarily relies on the question we are interested in, i.e., what exactly should the partial coefficients be an estimate for.</p>
<p>A further drawback of collinearity is that model fitting can become difficult. When strong correlations are present, model fitting algorithms may fail. If they do not fail, the statistical uncertainty of the estimates often becomes large. This is because the partial coefficient of one variable needs to be estimated for constant values of the other predictors in the model which means that a reduced range of values is available as illustrated in Figure <a href="lm.html#fig:sfexfig">11.11</a> C. However, if uncertainty intervals (confidence, credible or compatibility intervals) are reported alongside the estimates, then using correlated predictors in the same model is absolutely fine, if the fitting algorithm was successful.</p>
<p>The correlations per se can be interesting. Further readings on how to visualize and analyse data with complex correlation structures:</p>
<ul>
<li>principal component analysis <span class="citation">(<a href="referenzen.html#ref-Manly.1994" role="doc-biblioref">Manly 1994</a>)</span><br />
</li>
<li>path analyses, e.g. <span class="citation">Shipley (<a href="referenzen.html#ref-Shipley.2009" role="doc-biblioref">2009</a>)</span><br />
</li>
<li>structural equation models <span class="citation">(<a href="referenzen.html#ref-Hoyle2012" role="doc-biblioref">Hoyle 2012</a>)</span></li>
</ul>
<p><a href="" target="_blank"><img src="images/ruchen.jpg" style="display: block; margin: auto;" /></a></p>
</div>
<div id="orderedfactors" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Ordered Factors and Contrasts<a href="lm.html#orderedfactors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we have seen that the model matrix is an <span class="math inline">\(n \times k\)</span> matrix (with <span class="math inline">\(n\)</span> = sample size and <span class="math inline">\(k\)</span> = number of model coefficients) that is multiplied by the vector of the <span class="math inline">\(k\)</span> model coefficients to obtain the fitted values of a normal linear model. The first column of the model matrix normally contains only ones. This column is multiplied by the intercept. The other columns contain the observed values of the predictor variables if these are numeric variables, or indicator variables (= dummy variables) for factor levels if the predictors are categorical variables (= factors). For categorical variables the model matrix can be constructed in a number of ways. How it is constructed determines how the model coefficients can be interpreted. For example, coefficients could represent differences between means of specific factor levels to the mean of the reference level. That is what we have introduced above. However, they could also represent a linear, quadratic or cubic effect of an ordered factor. Here, we show how this works.</p>
<p>An ordered factor is a categorical variable with levels that have a natural order, for example, ‘low’, ‘medium’ and ‘high’. How do we tell R that a factor is ordered? The swallow data contain a factor ‘nesting_aid’ that contains the type aid provided in a barn for the nesting swallows. The natural order of the levels is none &lt; support (e.g., a wooden stick in the wall that helps support a nest built by the swallow) &lt; artificial_nest &lt; both (support and artificial nest). However, when we read in the data R orders these levels alphabetically rather than according to the logical order.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="lm.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(swallows)</span>
<span id="cb114-2"><a href="lm.html#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(swallows<span class="sc">$</span>nesting_aid) </span></code></pre></div>
<pre><code>## [1] &quot;artif_nest&quot; &quot;both&quot;       &quot;none&quot;       &quot;support&quot;</code></pre>
<p>And with the function contrasts we see how R will construct the model matrix.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="lm.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(swallows<span class="sc">$</span>nesting_aid)</span></code></pre></div>
<pre><code>##            both none support
## artif_nest    0    0       0
## both          1    0       0
## none          0    1       0
## support       0    0       1</code></pre>
<p>R will construct three dummy variables and call them ‘both’, ‘none’, and ‘support’. The variable ‘both’ will have an entry of one when the observation is ‘both’ and zero otherwise. Similarly, the other two dummy variables are indicator variables of the other two levels and ‘artif_nest’ is the reference level. The model coefficients can then be interpreted as the difference between ‘artif_nest’ and each of the other levels. The instruction how to transform a factor into columns of a model matrix is called the contrasts.
Now, let’s bring the levels into their natural order and define the factor as an ordered factor.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="lm.html#cb118-1" aria-hidden="true" tabindex="-1"></a>swallows<span class="sc">$</span>nesting_aid <span class="ot">&lt;-</span> <span class="fu">factor</span>(swallows<span class="sc">$</span>nesting_aid, <span class="at">levels=</span><span class="fu">c</span>(<span class="st">&quot;none&quot;</span>, <span class="st">&quot;support&quot;</span>, <span class="st">&quot;artif_nest&quot;</span>, <span class="st">&quot;both&quot;</span>), <span class="at">ordered=</span><span class="cn">TRUE</span>)</span>
<span id="cb118-2"><a href="lm.html#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(swallows<span class="sc">$</span>nesting_aid)</span></code></pre></div>
<pre><code>## [1] &quot;none&quot;       &quot;support&quot;    &quot;artif_nest&quot; &quot;both&quot;</code></pre>
<p>The levels are now in the natural order. R will, from now on, use this order for analyses, tables, and plots, and because we defined the factor to be an ordered factor, R will use polynomial contrasts:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="lm.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(swallows<span class="sc">$</span>nesting_aid)</span></code></pre></div>
<pre><code>##              .L   .Q         .C
## [1,] -0.6708204  0.5 -0.2236068
## [2,] -0.2236068 -0.5  0.6708204
## [3,]  0.2236068 -0.5 -0.6708204
## [4,]  0.6708204  0.5  0.2236068</code></pre>
<p>When using polynomial contrasts, R will construct three (= number of levels minus one) variables that are called ‘.L’, ‘.Q’, and ‘.C’ for linear, quadratic and cubic effects. The contrast matrix defines which numeric value will be inserted in each of the three corresponding columns in the model matrix for each observation, for example, an observation with ‘support’ in the factor ‘nesting_aid’ will get the values -0.224, -0.5 and 0.671 in the columns L, Q and C of the model matrix. These contrasts define yet another way to get 4 different group means:<br />
<span class="math inline">\(m1 = \beta_0 – 0.671* \beta_1 + 0.5*\beta_2 - 0.224* \beta_3\)</span><br />
<span class="math inline">\(m2 = \beta_0 – 0.224* \beta_1 - 0.5*\beta_2 + 0.671* \beta_3\)</span><br />
<span class="math inline">\(m3 = \beta_0 + 0.224* \beta_1 - 0.5*\beta_2 - 0.671* \beta_3\)</span><br />
<span class="math inline">\(m4 = \beta_0 + 0.671* \beta_1 + 0.5*\beta_2 + 0.224* \beta_3\)</span></p>
<p>The group means are the same, independent of whether a factor is defined as ordered or not. The ordering also has no effect on the variance that is explained by the factor ‘nesting_aid’ or the overall model fit. Only the model coefficients and their interpretation depend on whether a factor is defined as ordered or not. When we define a factor as ordered, the coefficients can be interpreted as linear, quadratic, cubic, or higher order polynomial effects. The number of the polynomials will always be the number of factor levels minus one (unless the intercept is omitted from the model in which case it is the number of factor levels). Linear, quadratic, and further polynomial effects normally are more interesting for ordered factors than single differences from a reference level because linear and polynomial trends tell us something about consistent changes in the outcome along the ordered factor levels. Therefore, an ordered factor with k levels is treated like a covariate consisting of the centered level numbers (-1.5, -0.5, 0.5, 1.5 in our case with four levels) and k-1 orthogonal polynomials of this covariate are included in the model. Thus, if we have an ordered factor A with three levels, <code>y~A</code> is equivalent to <code>y~x+I(x^2)</code>, with x=-1 for the lowest, x=0 for the middle and x=1 for the highest level.
Note that it is also possible to define own contrasts if we are interested in specific differences or trends. However, it is not trivial to find meaningful and orthogonal (= uncorrelated) contrasts.</p>
</div>
<div id="quadratic-and-higher-polynomial-terms" class="section level2 hasAnchor" number="11.6">
<h2><span class="header-section-number">11.6</span> Quadratic and Higher Polynomial Terms<a href="lm.html#quadratic-and-higher-polynomial-terms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The straight regression line for the biomass of grass species Ap <em>Alopecurus pratensis</em> dependent on the distance to the ground water does not fit well (Figure <a href="lm.html#fig:fgbiom">11.10</a>). The residuals at low and high values of water tend to be positive and intermediate water levels are associated with negative residuals. This points out a possible violation of the model assumptions.
The problem is that the relationship between distance to water and biomass of species Ap is not linear. In real life, we often find non-linear relationships, but if the shape of the relationship is quadratic (plus, potentially, a few more polynomials) we can still use ‘linear modeling’ (the term ‘linear’ refers to the linear function used to describe the relationship between the outcome and the predictor variables: <span class="math inline">\(f(x) = \beta_0 + \beta_1x + \beta_2x^2\)</span> is a linear function compared to, e.g., <span class="math inline">\(f(x) = \beta^x\)</span>, which is not a linear function). We simply add the quadratic term of the predictor variable, that is, water in our example, as a further predictor in the linear predictor:
<span class="math inline">\(\hat{y_i} = \beta_0+\beta_1water_i+\beta_2water_i^2\)</span>.
A quadratic term can be fitted in R using the function <code>I()</code> which tells R that we want the squared values of distance to water. If we do not use <code>I()</code> the <code>^2</code> indicates a two-way interaction. The model specification is then <code>lm(log(Yi.g) ~ Water + I(Water^2), data=...)</code>.
The cubic term would be added by <code>+I(Water^3)</code>.
As with interactions, a polynomial term changes the interpretation of lower level polynomials. Therefore, we normally include all polynomials up to a specific degree. Furthermore, polynomials are normally correlated (if no special transformation is used, see below) which could cause problems when fitting the model such as non-convergence. To avoid collinearity among polynomials, so called orthogonal polynomials can be used. These are polynomials that are uncorrelated. To that end, we can use the function <code>poly</code> which creates as many orthogonal polynomials of the variable as we want:
<code>poly(dat$Water, 2)</code> creates two columns, the first one can be used to model the linear effect of water, the second one to model the quadratic term of water:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="lm.html#cb122-1" aria-hidden="true" tabindex="-1"></a>t.poly <span class="ot">&lt;-</span> <span class="fu">poly</span>(dat<span class="sc">$</span>Water, <span class="dv">2</span>)</span>
<span id="cb122-2"><a href="lm.html#cb122-2" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>Water.l <span class="ot">&lt;-</span> t.poly[,<span class="dv">1</span>]   <span class="co"># linear term for water</span></span>
<span id="cb122-3"><a href="lm.html#cb122-3" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>Water.q <span class="ot">&lt;-</span> t.poly[,<span class="dv">2</span>]   <span class="co"># quadratic term for water</span></span>
<span id="cb122-4"><a href="lm.html#cb122-4" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(Yi.g) <span class="sc">~</span> Water.l <span class="sc">+</span> Water.q, <span class="at">data=</span>dat)</span></code></pre></div>
<p>When orthogonal polynomials are used, the estimated linear and quadratic effects can be interpreted as purely linear and purely quadratic influences of the predictor on the outcome. The function poly applies a specific transformation to the original variables. To reproduce the transformation (e.g. for getting the corresponding orthogonal polynomials for new data used to draw an effect plot), the function predict can be used with the poly-object created based on the original data.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="lm.html#cb123-1" aria-hidden="true" tabindex="-1"></a>newdat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Water =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">130</span>)) </span>
<span id="cb123-2"><a href="lm.html#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="co"># transformation analogous to the one used to fit the model:</span></span>
<span id="cb123-3"><a href="lm.html#cb123-3" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>Water.l <span class="ot">&lt;-</span> <span class="fu">predict</span>(t.poly, newdat<span class="sc">$</span>Water)[,<span class="dv">1</span>]</span>
<span id="cb123-4"><a href="lm.html#cb123-4" aria-hidden="true" tabindex="-1"></a>newdat<span class="sc">$</span>Water.q <span class="ot">&lt;-</span> <span class="fu">predict</span>(t.poly, newdat<span class="sc">$</span>Water)[,<span class="dv">2</span>]</span></code></pre></div>
<p>These transformed variables can then be used to calculate fitted values that correspond to the water values specified in the new data.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="priors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="residualanalysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/TobiasRoth/BDAEcology/edit/master/2.03-lm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
